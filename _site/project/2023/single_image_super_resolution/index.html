<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Single Image Super Resolution | Badal Tripathi</title> <meta name="author" content="Badal Tripathi"> <meta name="description" content="This is a test. A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://badal11.github.io/project/2023/single_image_super_resolution/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Badal </span>Tripathi</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Single Image Super Resolution</h1> <p class="post-meta">July 12, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/project"> <i class="fa-solid fa-hashtag fa-sm"></i> project</a>     ·   <a href="/blog/category/deep-learning"> <i class="fa-solid fa-tag fa-sm"></i> Deep_Learning</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p><a href="https://arxiv.org/abs/1406.2661" rel="external nofollow noopener" target="_blank">GAN</a> (Generating Adversarial Network) is about creating, like drawing but completely from scratch. It has got two models: the Generator and the Discriminator are put together into a game of adversary. Through which they learn the intricate details of the target data distribution. These two models are playing a MIN-MAX game where one tries to minimize the loss and the other tries to maximize. While doing so a global optimum is reached, where the Discriminator is no longer able to distinguish between real and generated (fake) data distribution.</p> <p>SISR(Single Image Super-Resolution) is an application of GAN. Image super-resolution is the process of enlarging small photos while maintaining a high level of quality, or of restoring high-resolution images from low-resolution photographs with rich information. Here the model’s work is to map the function from low-resolution image data to its high-resolution image. Instead of giving a random noise to the Generator, a low-resolution image is fed into it. After passing through various Convolutional Layers and Upsampling Layers, the Generator gives a high-resolution image output. Generally, there are multiple solutions to this problem, so it’s quite difficult to master the output up to original images in terms of richness and quality.</p> <p><br></p> <h2 id="data-preprocessing-and-augmentation">Data Preprocessing and Augmentation:</h2> <p>We have used the <a href="https://data.vision.ee.ethz.ch/cvl/DIV2K/" rel="external nofollow noopener" target="_blank">DIV2K</a> [Agustsson and Timofte (2017)] dataset provided by the TensorFlow library. There are altogether 800 pairs of low resolution and high-resolution images in the the training set whereas 100 pairs in the testing set. This data contains mainly people, cities, fauna, sceneries, etc. We used flipping and rotating through 90, 180, and 270 degrees randomly over the dataset. Since we had limited memory on the training computer, we had to split large images into patches of smaller size. 19 patches of size 96 ✕ 96 pixels resolution were obtained from an image randomly. Our generator is designed to upsample images by 4 times so, the output image patch will be of dimension: 384 ✕ 384 pixels.</p> <p><br></p> <h2 id="generator">Generator</h2> <p>The generator is the block in the architecture which is responsible for generating the high resolution(HR) images from low resolution(LR) images. In 2015, <a href="https://arxiv.org/abs/1609.04802" rel="external nofollow noopener" target="_blank">SRGAN</a> was published which introduced the concept of using GAN for SISR tasks which produced the state the art solution. The generator of <a href="https://arxiv.org/abs/1609.04802" rel="external nofollow noopener" target="_blank">SRGAN</a> consists of several residual blocks that facilitate the flow of the gradient during backpropagation.</p> <p><img src="/assets/img/from_other/generator.png#center" alt="Generator Architecture"></p> <p>To further enhance the quality of generator images <a href="https://arxiv.org/abs/1809.00219" rel="external nofollow noopener" target="_blank">ESRGAN</a> was released which performed some modifications in the generator of the <a href="https://arxiv.org/abs/1609.04802" rel="external nofollow noopener" target="_blank">SRGAN</a> which includes:</p> <ul> <li>Removing the batch normalized(BN) layers.</li> <li>Replacing the original residual block with the proposed Residual-in-Residual Dense Block (RRDB), which combines multi-level residual network and dense connections as in the figure below:</li> </ul> <p><img src="/assets/img/from_other/rrdb.png#center" alt="RRDB Diagram"></p> <p>Fig: Residual in Residual Dense Block(RRDB)</p> <p>Removing BN layers has proven to increase performance and reduce the computational complexity in different PSNR-oriented tasks including SR and deblurring. BN layers normalize the features using mean and variance in a batch during training and use the estimated mean and variance of the whole training dataset during testing. When the statistics of training and testing datasets differ a lot, BN layers tend to introduce unpleasant artifacts and limit the generalization ability. The researchers empirically observe that BN layers are more likely to bring artifacts when the network is deeper and trained under a GAN framework. These artifacts occasionally appear among iterations and different settings, violating the need for stable performance overtraining. Therefore, removing BN layers for stable training and consistent performance. Furthermore, removing BN layers helps to improve generalization ability and to reduce computational complexity and memory usage.</p> <p>RRDB employs a deeper and more complex structure than the original residual block in SRGAN. Specifically, as shown in the figure above, the proposed RRDB has a residual-in-residual structure, where residual learning is used at different levels. Here, the RRDB uses dense block in the main path, where the network capacity becomes higher benefiting from the dense connections. In addition to the improved architecture, it also exploits several techniques to facilitate training a very deep network such as residual scaling(beta) i.e., scaling down the residuals by multiplying a constant between 0 and 1 before adding them to the main path to prevent instability.</p> <p><br></p> <h2 id="discriminator">Discriminator</h2> <p>The task of the discriminator is to discriminate between real HR images and generated SR images. Discriminator architecture here used is similar to DC-GAN architecture with LeakyReLU activation function. The network contains eight convolutional layers with 3×3 filter kernels, increasing by a factor of 2 from 64 to 512 kernels as in the VGG network. Strided convolutions are used to reduce the image resolution each time the number of features is doubled. But to overcome the instability while training of original GAN, we use a variant of GANs named improved training of Wasserstein GANs (WGAN-GP). So the last sigmoid layer of the conventional DC-GAN discriminator is omitted. This helps in not restricting the feature maps in 0 to 1 value.</p> <p><img src="/assets/img/from_other/discriminator.png#center" alt="discriminator image"></p> <h2 id="losses">Losses:</h2> <h3 id="generator-loss">Generator Loss</h3> <p>The generator loss is the sum of MSE, perceptual loss +adversarial loss</p> <p><em>l<sub>G</sub> = MSE+Perceptual Loss +Adversarial loss</em></p> <p><em>l<sub>G</sub>= l<sub>MSE</sub>+l<sub>p</sub>+ l<sub>GA</sub></em></p> <p><br></p> <h3 id="mean-square-errormse">Mean Square Error(MSE)</h3> <p>As the most common optimization objective for SISR, the pixelwise MSE loss is calculated as:</p> <table> <tbody> <tr> <td>_l<sub>MSE</sub> =</td> <td> </td> <td>G<sub>Θ</sub>(I<sub>LR</sub>) - I<sub>HR</sub> </td> <td> </td> <td> <sub>2</sub><sup>2</sup>_,</td> </tr> </tbody> </table> <p>where the parameter of the generator is denoted by ; the generated image, namely I<sub>SR</sub>,is denoted by G<sub>Θ</sub>(I<sub>LR</sub>); and the ground truth is denoted by I<sub>HR</sub> . Although models with MSE loss favor a high PSNR value, the generated results tend to be perceptually unsatisfying with overly smooth textures. Despite the aforementioned shortcomings, this loss term is still kept because MSE has clear physical meaning and helps to maintain color stability.</p> <p><br></p> <h3 id="perceptual-loss">Perceptual Loss</h3> <p>To compensate for the shortcomings of MSE loss and allow the loss function to better measure semantic and perceptual differences between images, we define and optimize a perceptual loss based on high-level features extracted from a pretrained network. The rationality of this loss term lies in that the pretrained network for classification originally has learned to encode the semantic and perceptual information that may be measured in the loss function. To enhance the performance of the perceptual loss, a 19-layer VGG network is used. The perceptual loss is actually the Euclidean distance between feature representations, which is defined as</p> <table> <tbody> <tr> <td>_l<sub>p</sub> =</td> <td> </td> <td>𝜙(G<sub>Θ</sub>(I<sub>LR</sub>)) - 𝜙(I<sub>HR</sub>)</td> <td> </td> <td> <sub>2</sub><sup>2</sup>_,</td> </tr> </tbody> </table> <p>where 𝜙 refers to the 19-layer VGG network. With this loss term, I<sub>SR</sub> and I<sub>HR</sub> are encouraged to have similar feature representations rather than to exactly match with each other in a pixel wise manner.</p> <p><br></p> <h3 id="adversarial-losses">Adversarial Losses:</h3> <p>In <a href="https://arxiv.org/abs/1609.04802" rel="external nofollow noopener" target="_blank">SRGAN</a>, the adopted generative model is generative adversarial network (GAN) and it suffers from training instability. WGAN leverages the Wasserstein distance to produce a value function, which has better theoretical properties than the original GAN. However, WGAN requires that the discriminator must lie within the space of 1-Lipschitz through weight clipping, resulting in either vanishing or exploding gradients without careful tuning of the clipping threshold. <br> To overcome the flaw of clipping , a new approach is applied called Gradient Pelanty method. It is used to enforce the Lipschitz constraint. This way Wasserstein distance between two distributions to help decide when to stop the training but penalizes the gradient of the discriminator with respect to its input instead of weight clipping. With gradient penalty, the discriminator is encouraged to learn smoother decision boundaries.</p> <p><br></p> <h3 id="generator-loss-1">Generator Loss</h3> <p><em>l<sub>GA</sub>=-𝔼[D(G<sub>Θ</sub>(I<sub>LR</sub>)]</em></p> <p><br></p> <h3 id="discriminator-loss">Discriminator Loss</h3> <table> <tbody> <tr> <td>_l<sub>DA</sub>=𝔼[D(G<sub>Θ</sub>(I<sub>LR</sub>)]-𝔼[D(I<sub>HR</sub>)] + λ𝔼(</td> <td> </td> <td>▽<sub>hat{I}</sub>D(hat{I})-1</td> <td> </td> <td> <sub>2</sub>-1)<sup>2</sup>_</td> </tr> </tbody> </table> <p><img src="/assets/img/from_other/work_flow.png#center" alt="workflow diagram"></p> <p>Normally, the output of the classifier i.e. discriminator in this case is kept between 0-1 using a sigmoid function in the last layer, where if discriminator prediction 0 for an image then the image is SR likewise if the prediction is 1 then it is an HR image. Here the discriminator is trained using WGAN-GP approach <a href="https://sulavtimilsina.github.io/posts/wgan-gp/" rel="external nofollow noopener" target="_blank">(described here)</a>, hence the output is not bounded between 0-1 instead the discriminator will try to maximize the distance between the prediction of SR image and HR image and generator will try to minimize it. Let’s look at the loss of the generator ie. I<sub>GA</sub> and the loss of discriminator I<sub>DA</sub> .</p> <p><br></p> <h3 id="understanding-discriminator-adversarial-loss">UNDERSTANDING DISCRIMINATOR ADVERSARIAL LOSS</h3> <p>(not considering the gradient penalty term for making it easier to understand)</p> <p><em>l<sub>DA</sub>=𝔼[D(G<sub>Θ</sub>(I<sub>LR</sub>)]-𝔼[D(I<sub>HR</sub>)]</em></p> <p>(Note: <em>l<sub>DA</sub>= 𝔼[D(I<sub>HR</sub>)]-𝔼[D(G<sub>Θ</sub>(I<sub>LR</sub>)]</em> if the loss of the discriminator is in this form than the discriminator will try to maximize this equation and the generator will try to minimize the I<sub>GA</sub>).</p> <p>Considering D(G<sub>Θ</sub>(I<sub>LR</sub>))= 5 and D(I<sub>HR</sub>) = 5 initially when the discriminator doesn’t have the ability to differentiate between them.</p> <p>Therefore the loss at the very beginning: <em>l<sub>DA</sub>=5-5= 0,</em></p> <p>The discriminator wants to minimize the loss l<sub>DA</sub>, hence increasing the distance between D(G<sub>Θ</sub>(I<sub>LR</sub>))and D(I<sub>HR</sub>) . Suppose after the update of the gradient of the discriminator for the few step, the value of prediction becomes D(G<sub>Θ</sub>(I<sub>LR</sub>))=-2 and D(I<sub>HR</sub>) = 2 ,therefore discriminator is learning to know the difference between the LR image and the HR image, hence making the loss(l<sub>DA</sub>)= -4, Here the loss is minimized and the distance between the two predictions is maximized.</p> <p><img src="/assets/img/from_other/understanding_disc_adv_loss.png#center" alt="discriminator adverserial loss"></p> <p><br></p> <h3 id="understanding-generator-adversarial-loss">UNDERSTANDING GENERATOR ADVERSARIAL LOSS</h3> <p>Discriminator is trained for a few steps and then the update of the generator happens. Therefore the discriminator is kept a few steps ahead of the generator in terms of its learning. Let’s consider the discriminator has been trained for the few steps and it predicted outputs are:</p> <p><em>D(G<sub>Θ</sub>(I<sub>LR</sub>)) = -2</em> <em>D(I<sub>HR</sub>) = 2</em></p> <p>The loss of the generator is:</p> <p><em>l<sub>GA</sub> = -𝔼[D(G<sub>Θ</sub>(I<sub>LR</sub>)]</em></p> <p>Therefore, <em>l<sub>GA</sub>= -(-2) = 2</em></p> <p>Generator wants to minimize l<sub>GA</sub> , which can only we achieved by increasing the value of D(G<sub>Θ</sub>(I<sub>LR</sub>)) hence ultimately reducing the distance between D(G<sub>Θ</sub>(I<sub>LR</sub>)) and D(I<sub>HR</sub>) ,hence making the SR image and HR image identical as:</p> <p><em>l<sub>GA</sub>= -(large positive value) ≈ global minima</em></p> <p><img src="/assets/img/from_other/understanding_gen_adv_loss.png#center" alt="generator adverserial loss"></p> <p><br></p> <h3 id="result-and-conclusion">Result and Conclusion:</h3> <p>We chose Kaggle’s kernel with Tesla P100 GPU to train the model. Since it has 20 million training parameters, training it for 500 epochs is a tedious job. Until now we have trained it only up to 100 epochs. Following is the sample output of the 100th epoch. The rightmost image is Low-Resolution Patch, the Middle one is the High-Resolution Patch and the Left most one is the Generated High-Resolution Image.</p> <p><img src="/assets/img/from_other/output.png#center" alt="outputs"></p> </div> </article> </div> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>