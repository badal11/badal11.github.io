<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://badal11.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://badal11.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-11-26T10:04:14+00:00</updated><id>https://badal11.github.io/feed.xml</id><title type="html">blank</title><subtitle>This is a test. A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Stock Price forecasting On Nabil Bank</title><link href="https://badal11.github.io/project/2023/stock_price_nabil/" rel="alternate" type="text/html" title="Stock Price forecasting On Nabil Bank"/><published>2023-07-15T13:56:00+00:00</published><updated>2023-07-15T13:56:00+00:00</updated><id>https://badal11.github.io/project/2023/stock_price_nabil</id><content type="html" xml:base="https://badal11.github.io/project/2023/stock_price_nabil/"><![CDATA[ <p>In statistical terms, time series forecasting is the process of analyzing the time series data using statistics and modeling to make predictions and informed strategic decisions. It falls under Quantitative Forecasting. Examples of Time Series Forecasting are weather forecast over next week, forecasting the closing price of a stock each day etc. In this article we will see different types of models which can be used for this analysis and pick what is best for what situations.</p> <p><br/></p> <h2 id="time-series-data">Time series data</h2> <p>Time series data are simply measurements or events that are tracked, monitored, downsampled, and aggregated over time. This could be server metrics, application performance monitoring, network data, sensor data, events, clicks, trades in a market, and many other types of analytics data. We will be taking stock price data to perform our analysis.</p> <p>Nabil bank is a bank located in Nepal, it’s been trading in NEPSE(Nepal Stock Exchange ) for the past 20 years. We can easily get this data by going to the NEPSE website. This data contains four features such as the price of the stock when the market opens on a particular date, the maximum value of the stock, the minimum value, and the value of the stock at which the market close on that day.</p> <p><img src="/assets/img/from_other/stock_dataframe.png#center" alt="Image of the data" title="Dataset"/></p> <p><br/></p> <h2 id="preprocessing">Preprocessing</h2> <h3 id="1-normalization">1. Normalization</h3> <p>We need to normalize between 0-1, to remove the problem which arises if the features having in different scales. But when normalizing validate and test data don’t use the validate.max() or text.max() and validate.min() or text.min() for their respective normalization use train.max and train.min for both of them. Because we can’t look at the validate or test dataset they are unknown to us. The important thing to note here is that the normalization has been done on the input feature only not on the label, the model will predict the actual value of stock.</p> <p><br/></p> <h3 id="2-sliding-window">2. Sliding window</h3> <p>To perform Supervised learning the dataset should have inputs and its corresponding label. Data windowing is a popular technique for converting historical data like time series to data suitable for supervised learning. It works as it sounds, we select a window for inputs and feed the model the data which has been selected into that window and the model will try to predict the label for that window. The main features of the input windows are:</p> <p>• The width (number of time steps) of the input and label windows.</p> <p>• The time offset between them.</p> <p>• Which features are used as inputs, labels, or both.</p> <p>Depending on the task and type of model we may want to generate a variety of data windows. Here are some examples:</p> <ol> <li>A model that makes a prediction one hour into the future given six days of history, would need a window like this:</li> </ol> <p><img src="/assets/img/from_other/example1.png#center" alt="Example_1" title="Example 1"/></p> <ol> <li>Similarly, to make a single prediction 24 days into the future, given 24 days of history, we might define a window like this:</li> </ol> <p><img src="/assets/img/from_other/example2.png#center" alt="Example_2" title="Example 1"/></p> <p><a href="https://www.tensorflow.org/tutorials/">source</a></p> <p>Therefore, depending upon the task and model we can generate varieties of inputs which helps to reduce the redundancy of code as by defining an data window using a class.</p> <p><br/></p> <h2 id="models">Models</h2> <p>In time series forecasting depending upon the number of steps we are going to do the prediction for the models can be classified into two types:</p> <h3 id="1-single-step-model">1. Single step Model</h3> <p>In single step model, model will look one step into the future. For example given all the past one month of stock data model will predict what will be the stock value tomorrow. For this task we will be using models like:</p> <p><br/></p> <h4 id="11-dense-model">1.1 Dense model:</h4> <p>A single dense layer is a single layer of fully connected neural network. Here, we will be sending our Inputs of specific input width into multiple dense layer and finally the output of these dense layer will be send though a single neuron dense layer to produce a single step output. It is an regression problem where we take Open, Close, Low and High as input to predict the closing value of the stock.</p> <p>def dense_func(input_shape): input= tf.keras.Input(shape= tf.constant(input_shape)) x= tf.keras.layers.Flatten()(input)</p> <p>#Basically there are four dense layer each followed by an dropout layer x= tf.keras.layers.Dense(units=556, activation=’relu’)(x) x= tf.keras.layers.Dropout(0.2)(x)</p> <p>x= tf.keras.layers.Dense(units=228, activation=’relu’)(x) x= tf.keras.layers.Dropout(0.2)(x)</p> <p>x= tf.keras.layers.Dense(units=128, activation=’relu’)(x) x= tf.keras.layers.Dropout(0.2)(x)</p> <p>x= tf.keras.layers.Dense(units=64, activation=’relu’)(x) x= tf.keras.layers.Dropout(0.2)(x)</p> <p>output= tf.keras.layers.Dense(units=1)(x) model= tf.keras.Model(inputs= input,outputs= output)<br/> return model</p> <p><img src="/assets/img/from_other/desce_code.png#center," alt="Architecture" title="Architecture"/></p> <h5 id="a-hyperparameter-tuning">a. Hyperparameter tuning</h5> <p>One of the most important hyperparameter for stock price prediction is the number of days that the model sees to make future prediction ie input_width. This hyperparameter value is calculated by training the model on different number of input width and the model which produces lowest loss it is selected.</p> <p><img src="/assets/img/from_other/hyperparameter_for_dense_model.png#center" alt="Input width vs Mean square Error(MSE)" title="Input width vs Mean square Error(MSE)"/></p> <p>We trained the model on input width [3,5,8,15,18,22,25,28,31] and among them the minimum value of MSE was obtained with the 3. So, the input width for the dense model is selected as 3.</p> <p><br/></p> <h5 id="b-evaluation">b. Evaluation</h5> <p>At input width 3, the label and prediction for Close value of stock look like this: <img src="/assets/img/from_other/Dense_model_ground_truth_vs_prediction.png#center" alt="label vs prediction" title="label vs prediction"/></p> <p><br/></p> <h4 id="12-lstm-model">1.2 LSTM model</h4> <p>A Recurrent Neural Network (RNN) is a type of neural network well-suited to time series data. RNNs process a time series step-by-step, maintaining an internal state from time step to time step.Let’s see understand how RNN will process time series data:</p> <p><img src="/assets/img/from_other/RNN.png#center" alt="LSTM modelS" title="LSTM model"/></p> <p>Here the RNN/LSTM is trained on every single input step, as a result, it makes the model more robust to changing landscape which is common in the stock dataset. It will take stock prices[Open, Close, High, Low] as input for the first day and predict the Close value for the second day. Similarly, a second time stamp will take the feature vector generated from the first time stamp and second days inputs to predict the 3rd step value and so on until it predicts one step into the future.</p> <p>def lstm_model(input_shape):</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>inp= Input(shape=input_shape) #BATCH,TIMESTAMP,FEATURES
x= tf.keras.layers.LSTM(128,return_sequences=False,name= 'LSTM')(inp)#batch,timestamp,32
x= Dense(units=256, activation='relu',name= 'Dense1')(x)
x=Dense(units=64, activation='relu',name= 'Dense2')(x)

x=Dense(units=32, activation='relu',name= 'Dense3')(x)
out= Dense(units=1)(x)
model= Model(inp,out)
return model
</code></pre></div></div> <p><img src="/assets/img/from_other/lstm_code.png#center" alt="Architecture" title="Architecture"/></p> <h5 id="a-evaluation">a. Evaluation</h5> <p>The minimum value of loss was obtained at input width 11 and its MSE value is similar dense model. Let’s look it label and prediction plot: <img src="/assets/img/from_other/rnn_ground_truth_vs_prediction.png#center" alt="label vs prediction" title="label vs prediction"/></p> <p><br/></p> <h3 id="2-multi-step-model">2 Multi-step model</h3> <p>In a multi-step prediction, the model needs to learn to predict a range of future values. Thus, unlike a single-step model, where only a single future point is predicted, a multi-step model predicts a sequence of the future values. There are two rough approaches to this:</p> <p>2.1. Single-shot Model Single-shot Model makes prediction of the entire time series at once. It is a time machine that can jump to any day into the future.</p> <p>2.2. Autoregressive predictions where the model only makes single-step predictions and its output is fed back as its input. We can see it as a time machine that can’t directly jump to any future date, instead, it had to go through each of the previous dates until it reaches the required future date.</p> <p><img src="/assets/img/from_other/autoregressive.png#center" alt="Autoregressive Model" title="Autoregressive Model"/></p> <p>For example, a person is living in 2012 who wants to go to 2022, if he used its single-shot time machine he can directly go to the year 2022 but as the machine doesn’t have any information about the jumped years its prediction events may be different from the actual events. But on the other hand, if he used its autoregressive time machine, the time machine will take him to the year 2013 and then 2014 until he reaches the year 2022, therefore the machine learns information about the intermediate year also which helps to improve the prediction significantly.</p> <p>class denseLayers(tf.keras.layers.Layer): def <strong>init</strong>(self): super().<strong>init</strong>() self.dense1= layers.Dense(256) self.dense2= layers.Dense(128) self.dense3= layers.Dense(32) self.dense4= layers.Dense(1)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def call(self,inputs):
    x= self.dense1(inputs)
    x= layers.Dropout(0.1)(x)
    
    x= self.dense2(x)
    x= layers.Dropout(0.1)(x)
    
    x= self.dense3(x)
    x= layers.Dropout(0.1)(x)
    
    x= self.dense4(x)
    return x
</code></pre></div></div> <p>def AutoRegressive_func():</p> <p>class AutoRegressive(tf.keras.Model): def <strong>init</strong>(self, units,output_steps): super().<strong>init</strong>() self.unit= units self.out_step= output_steps self.lstm_cell= layers.LSTMCell(self.unit)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      self.lstm_layer= layers.LSTM(128,return_state=True)

      self.dense_layer= denseLayers()

      
  def call(self,inputs,training= True):
      '''
      input= [batch,timestamp,features]
      '''
      predictions= []
      output,state_h,state_c= self.lstm_layer(inputs) 
      #output=[batch,units], similarly output= state_h

      state= [state_h,state_c] # The state
  
      prediction= self.dense_layer(output) 
      predictions.append(prediction)
      
      #Now iterating through the every step
      for i in range(self.out_step-1):
          output,state= self.lstm_cell(output,state,training)
          prediction= self.dense_layer(output) #Prediction= [batch,1] As we are outputting "Close" value at every time stamp
          predictions.append(prediction)
          

      # predictions.shape =&gt; (time, batch, features)
      predictions = tf.stack(predictions)
      
      # predictions.shape =&gt; (batch, time, features)
      predictions = tf.transpose(predictions, [1, 0, 2])
      return predictions   return AutoRegressive
</code></pre></div></div> <p>In the code, we can see we send the input to an LSTM layer which produces an output and state of the last LSTM cell. These output and state vectors are sent to an LSTM cell to forecast the price for a single day. Similarly, the next LSTM cell executes the previous cell output as input, and the state of the previous cell gets initialized as its initial state. It goes on until we predicted the whole range of output.</p> <p><br/></p> <h4 id="a-hyperparamter-tuning">a. Hyperparamter Tuning</h4> <p>In previous single step model, the minimum value of MSE was obtain when the input width is small but interesting in autoregressive model as the input width increase the MSE reduces. Therefore, the model performs the best when it’s looking large number of previous date data.</p> <p><img src="/assets/img/from_other/hyperparameter_for_autoregressiv_model.png#center" alt="Input width vs Mean square Error(MSE)"/></p> <p>We can see the model performs the best when the input width is 31.</p> <p><br/></p> <h4 id="b-evaluation-1">b. Evaluation</h4> <p><img src="/assets/img/from_other/autoregressive_prediction.png#center" alt="Plotting Close value for consecutive days"/></p> <p>The model is taking a consecutive input of the past 31 days and it is predicting the next 3 days. The difference between the actual price and the predicted price is not much. Therefore depending upon the task we can select an appropriate model and do the task.</p>]]></content><author><name></name></author><category term="machine-learning"/><category term="ML"/><summary type="html"><![CDATA[In statistical terms, time series forecasting is the process of analyzing the time series data using statistics and modeling to make predictions and informed strategic decisions. It falls under Quantitative Forecasting. Examples of Time Series Forecasting are weather forecast over next week, forecasting the closing price of a stock each day etc. In this article we will see different types of models which can be used for this analysis and pick what is best for what situations.]]></summary></entry><entry><title type="html">Single Image Super Resolution</title><link href="https://badal11.github.io/project/2023/single_image_super_resolution/" rel="alternate" type="text/html" title="Single Image Super Resolution"/><published>2023-07-12T13:56:00+00:00</published><updated>2023-07-12T13:56:00+00:00</updated><id>https://badal11.github.io/project/2023/single_image_super_resolution</id><content type="html" xml:base="https://badal11.github.io/project/2023/single_image_super_resolution/"><![CDATA[<p><a href="https://arxiv.org/abs/1406.2661">GAN</a> (Generating Adversarial Network) is about creating, like drawing but completely from scratch. It has got two models: the Generator and the Discriminator are put together into a game of adversary. Through which they learn the intricate details of the target data distribution. These two models are playing a MIN-MAX game where one tries to minimize the loss and the other tries to maximize. While doing so a global optimum is reached, where the Discriminator is no longer able to distinguish between real and generated (fake) data distribution.</p> <p>SISR(Single Image Super-Resolution) is an application of GAN. Image super-resolution is the process of enlarging small photos while maintaining a high level of quality, or of restoring high-resolution images from low-resolution photographs with rich information. Here the model’s work is to map the function from low-resolution image data to its high-resolution image. Instead of giving a random noise to the Generator, a low-resolution image is fed into it. After passing through various Convolutional Layers and Upsampling Layers, the Generator gives a high-resolution image output. Generally, there are multiple solutions to this problem, so it’s quite difficult to master the output up to original images in terms of richness and quality.</p> <p><br/></p> <h2 id="data-preprocessing-and-augmentation">Data Preprocessing and Augmentation:</h2> <p>We have used the <a href="https://data.vision.ee.ethz.ch/cvl/DIV2K/">DIV2K</a> [Agustsson and Timofte (2017)] dataset provided by the TensorFlow library. There are altogether 800 pairs of low resolution and high-resolution images in the the training set whereas 100 pairs in the testing set. This data contains mainly people, cities, fauna, sceneries, etc. We used flipping and rotating through 90, 180, and 270 degrees randomly over the dataset. Since we had limited memory on the training computer, we had to split large images into patches of smaller size. 19 patches of size 96 ✕ 96 pixels resolution were obtained from an image randomly. Our generator is designed to upsample images by 4 times so, the output image patch will be of dimension: 384 ✕ 384 pixels.</p> <p><br/></p> <h2 id="generator">Generator</h2> <p>The generator is the block in the architecture which is responsible for generating the high resolution(HR) images from low resolution(LR) images. In 2015, <a href="https://arxiv.org/abs/1609.04802">SRGAN</a> was published which introduced the concept of using GAN for SISR tasks which produced the state the art solution. The generator of <a href="https://arxiv.org/abs/1609.04802">SRGAN</a> consists of several residual blocks that facilitate the flow of the gradient during backpropagation.</p> <p><img src="/assets/img/from_other/generator.png#center" alt="Generator Architecture"/></p> <p>To further enhance the quality of generator images <a href="https://arxiv.org/abs/1809.00219">ESRGAN</a> was released which performed some modifications in the generator of the <a href="https://arxiv.org/abs/1609.04802">SRGAN</a> which includes:</p> <ul> <li>Removing the batch normalized(BN) layers.</li> <li>Replacing the original residual block with the proposed Residual-in-Residual Dense Block (RRDB), which combines multi-level residual network and dense connections as in the figure below:</li> </ul> <p><img src="/assets/img/from_other/rrdb.png#center" alt="RRDB Diagram"/></p> <p>Fig: Residual in Residual Dense Block(RRDB)</p> <p>Removing BN layers has proven to increase performance and reduce the computational complexity in different PSNR-oriented tasks including SR and deblurring. BN layers normalize the features using mean and variance in a batch during training and use the estimated mean and variance of the whole training dataset during testing. When the statistics of training and testing datasets differ a lot, BN layers tend to introduce unpleasant artifacts and limit the generalization ability. The researchers empirically observe that BN layers are more likely to bring artifacts when the network is deeper and trained under a GAN framework. These artifacts occasionally appear among iterations and different settings, violating the need for stable performance overtraining. Therefore, removing BN layers for stable training and consistent performance. Furthermore, removing BN layers helps to improve generalization ability and to reduce computational complexity and memory usage.</p> <p>RRDB employs a deeper and more complex structure than the original residual block in SRGAN. Specifically, as shown in the figure above, the proposed RRDB has a residual-in-residual structure, where residual learning is used at different levels. Here, the RRDB uses dense block in the main path, where the network capacity becomes higher benefiting from the dense connections. In addition to the improved architecture, it also exploits several techniques to facilitate training a very deep network such as residual scaling(beta) i.e., scaling down the residuals by multiplying a constant between 0 and 1 before adding them to the main path to prevent instability.</p> <p><br/></p> <h2 id="discriminator">Discriminator</h2> <p>The task of the discriminator is to discriminate between real HR images and generated SR images. Discriminator architecture here used is similar to DC-GAN architecture with LeakyReLU activation function. The network contains eight convolutional layers with 3×3 filter kernels, increasing by a factor of 2 from 64 to 512 kernels as in the VGG network. Strided convolutions are used to reduce the image resolution each time the number of features is doubled. But to overcome the instability while training of original GAN, we use a variant of GANs named improved training of Wasserstein GANs (WGAN-GP). So the last sigmoid layer of the conventional DC-GAN discriminator is omitted. This helps in not restricting the feature maps in 0 to 1 value.</p> <p><img src="/assets/img/from_other/discriminator.png#center" alt="discriminator image"/></p> <h2 id="losses">Losses:</h2> <h3 id="generator-loss">Generator Loss</h3> <p>The generator loss is the sum of MSE, perceptual loss +adversarial loss</p> <p><em>l<sub>G</sub> = MSE+Perceptual Loss +Adversarial loss</em></p> <p><em>l<sub>G</sub>= l<sub>MSE</sub>+l<sub>p</sub>+ l<sub>GA</sub></em></p> <p><br/></p> <h3 id="mean-square-errormse">Mean Square Error(MSE)</h3> <p>As the most common optimization objective for SISR, the pixelwise MSE loss is calculated as:</p> <table> <tbody> <tr> <td>_l<sub>MSE</sub> =</td> <td> </td> <td>G<sub>Θ</sub>(I<sub>LR</sub>) - I<sub>HR</sub></td> <td> </td> <td><sub>2</sub><sup>2</sup>_,</td> </tr> </tbody> </table> <p>where the parameter of the generator is denoted by ; the generated image, namely I<sub>SR</sub>,is denoted by G<sub>Θ</sub>(I<sub>LR</sub>); and the ground truth is denoted by I<sub>HR</sub> . Although models with MSE loss favor a high PSNR value, the generated results tend to be perceptually unsatisfying with overly smooth textures. Despite the aforementioned shortcomings, this loss term is still kept because MSE has clear physical meaning and helps to maintain color stability.</p> <p><br/></p> <h3 id="perceptual-loss">Perceptual Loss</h3> <p>To compensate for the shortcomings of MSE loss and allow the loss function to better measure semantic and perceptual differences between images, we define and optimize a perceptual loss based on high-level features extracted from a pretrained network. The rationality of this loss term lies in that the pretrained network for classification originally has learned to encode the semantic and perceptual information that may be measured in the loss function. To enhance the performance of the perceptual loss, a 19-layer VGG network is used. The perceptual loss is actually the Euclidean distance between feature representations, which is defined as</p> <table> <tbody> <tr> <td>_l<sub>p</sub> =</td> <td> </td> <td>𝜙(G<sub>Θ</sub>(I<sub>LR</sub>)) - 𝜙(I<sub>HR</sub>)</td> <td> </td> <td><sub>2</sub><sup>2</sup>_,</td> </tr> </tbody> </table> <p>where 𝜙 refers to the 19-layer VGG network. With this loss term, I<sub>SR</sub> and I<sub>HR</sub> are encouraged to have similar feature representations rather than to exactly match with each other in a pixel wise manner.</p> <p><br/></p> <h3 id="adversarial-losses">Adversarial Losses:</h3> <p>In <a href="https://arxiv.org/abs/1609.04802">SRGAN</a>, the adopted generative model is generative adversarial network (GAN) and it suffers from training instability. WGAN leverages the Wasserstein distance to produce a value function, which has better theoretical properties than the original GAN. However, WGAN requires that the discriminator must lie within the space of 1-Lipschitz through weight clipping, resulting in either vanishing or exploding gradients without careful tuning of the clipping threshold. <br/> To overcome the flaw of clipping , a new approach is applied called Gradient Pelanty method. It is used to enforce the Lipschitz constraint. This way Wasserstein distance between two distributions to help decide when to stop the training but penalizes the gradient of the discriminator with respect to its input instead of weight clipping. With gradient penalty, the discriminator is encouraged to learn smoother decision boundaries.</p> <p><br/></p> <h3 id="generator-loss-1">Generator Loss</h3> <p><em>l<sub>GA</sub>=-𝔼[D(G<sub>Θ</sub>(I<sub>LR</sub>)]</em></p> <p><br/></p> <h3 id="discriminator-loss">Discriminator Loss</h3> <table> <tbody> <tr> <td>_l<sub>DA</sub>=𝔼[D(G<sub>Θ</sub>(I<sub>LR</sub>)]-𝔼[D(I<sub>HR</sub>)] + λ𝔼(</td> <td> </td> <td>▽<sub>hat{I}</sub>D(hat{I})-1</td> <td> </td> <td><sub>2</sub>-1)<sup>2</sup>_</td> </tr> </tbody> </table> <p><img src="/assets/img/from_other/work_flow.png#center" alt="workflow diagram"/></p> <p>Normally, the output of the classifier i.e. discriminator in this case is kept between 0-1 using a sigmoid function in the last layer, where if discriminator prediction 0 for an image then the image is SR likewise if the prediction is 1 then it is an HR image. Here the discriminator is trained using WGAN-GP approach <a href="https://sulavtimilsina.github.io/posts/wgan-gp/">(described here)</a>, hence the output is not bounded between 0-1 instead the discriminator will try to maximize the distance between the prediction of SR image and HR image and generator will try to minimize it. Let’s look at the loss of the generator ie. I<sub>GA</sub> and the loss of discriminator I<sub>DA</sub> .</p> <p><br/></p> <h3 id="understanding-discriminator-adversarial-loss">UNDERSTANDING DISCRIMINATOR ADVERSARIAL LOSS</h3> <p>(not considering the gradient penalty term for making it easier to understand)</p> <p><em>l<sub>DA</sub>=𝔼[D(G<sub>Θ</sub>(I<sub>LR</sub>)]-𝔼[D(I<sub>HR</sub>)]</em></p> <p>(Note: <em>l<sub>DA</sub>= 𝔼[D(I<sub>HR</sub>)]-𝔼[D(G<sub>Θ</sub>(I<sub>LR</sub>)]</em> if the loss of the discriminator is in this form than the discriminator will try to maximize this equation and the generator will try to minimize the I<sub>GA</sub>).</p> <p>Considering D(G<sub>Θ</sub>(I<sub>LR</sub>))= 5 and D(I<sub>HR</sub>) = 5 initially when the discriminator doesn’t have the ability to differentiate between them.</p> <p>Therefore the loss at the very beginning: <em>l<sub>DA</sub>=5-5= 0,</em></p> <p>The discriminator wants to minimize the loss l<sub>DA</sub>, hence increasing the distance between D(G<sub>Θ</sub>(I<sub>LR</sub>))and D(I<sub>HR</sub>) . Suppose after the update of the gradient of the discriminator for the few step, the value of prediction becomes D(G<sub>Θ</sub>(I<sub>LR</sub>))=-2 and D(I<sub>HR</sub>) = 2 ,therefore discriminator is learning to know the difference between the LR image and the HR image, hence making the loss(l<sub>DA</sub>)= -4, Here the loss is minimized and the distance between the two predictions is maximized.</p> <p><img src="/assets/img/from_other/understanding_disc_adv_loss.png#center" alt="discriminator adverserial loss"/></p> <p><br/></p> <h3 id="understanding-generator-adversarial-loss">UNDERSTANDING GENERATOR ADVERSARIAL LOSS</h3> <p>Discriminator is trained for a few steps and then the update of the generator happens. Therefore the discriminator is kept a few steps ahead of the generator in terms of its learning. Let’s consider the discriminator has been trained for the few steps and it predicted outputs are:</p> <p><em>D(G<sub>Θ</sub>(I<sub>LR</sub>)) = -2</em> <em>D(I<sub>HR</sub>) = 2</em></p> <p>The loss of the generator is:</p> <p><em>l<sub>GA</sub> = -𝔼[D(G<sub>Θ</sub>(I<sub>LR</sub>)]</em></p> <p>Therefore, <em>l<sub>GA</sub>= -(-2) = 2</em></p> <p>Generator wants to minimize l<sub>GA</sub> , which can only we achieved by increasing the value of D(G<sub>Θ</sub>(I<sub>LR</sub>)) hence ultimately reducing the distance between D(G<sub>Θ</sub>(I<sub>LR</sub>)) and D(I<sub>HR</sub>) ,hence making the SR image and HR image identical as:</p> <p><em>l<sub>GA</sub>= -(large positive value) ≈ global minima</em></p> <p><img src="/assets/img/from_other/understanding_gen_adv_loss.png#center" alt="generator adverserial loss"/></p> <p><br/></p> <h3 id="result-and-conclusion">Result and Conclusion:</h3> <p>We chose Kaggle’s kernel with Tesla P100 GPU to train the model. Since it has 20 million training parameters, training it for 500 epochs is a tedious job. Until now we have trained it only up to 100 epochs. Following is the sample output of the 100th epoch. The rightmost image is Low-Resolution Patch, the Middle one is the High-Resolution Patch and the Left most one is the Generated High-Resolution Image.</p> <p><img src="/assets/img/from_other/output.png#center" alt="outputs"/></p>]]></content><author><name></name></author><category term="deep-learning"/><category term="char"/><category term="RNN"/><summary type="html"><![CDATA[GAN (Generating Adversarial Network) is about creating, like drawing but completely from scratch. It has got two models: the Generator and the Discriminator are put together into a game of adversary. Through which they learn the intricate details of the target data distribution. These two models are playing a MIN-MAX game where one tries to minimize the loss and the other tries to maximize. While doing so a global optimum is reached, where the Discriminator is no longer able to distinguish between real and generated (fake) data distribution.]]></summary></entry><entry><title type="html">Automated Musical Tune Generation using char RNN</title><link href="https://badal11.github.io/project/2023/music_generation/" rel="alternate" type="text/html" title="Automated Musical Tune Generation using char RNN"/><published>2023-07-12T13:56:00+00:00</published><updated>2023-07-12T13:56:00+00:00</updated><id>https://badal11.github.io/project/2023/music_generation</id><content type="html" xml:base="https://badal11.github.io/project/2023/music_generation/"><![CDATA[<h2 id="demo-video"><a href="https://youtu.be/9OP5QJuB0w4">Demo video</a></h2> <h2 id="code"><a href="https://github.com/badal11/Musical_tune_composition">Code</a></h2> <h2 id="objectives">Objectives</h2> <p>-To create an automated musical tune prediction model that takes arbitary notes as input and predicts a sequence of musical notes</p> <p>-To develop an abc notation media player that can transform the converted note sequence into beautiful musical notes</p> <h2 id="music-and-its-representation">Music and it’s representation</h2> <p>A musical tune is a combination of sounds in time through the elements of melody, harmony, rhythm, and timbre. Music, like language, is a method of communication in which a series of notes can convey a variety of emotions. One important point to remember is that music must be both expressive and precise. Classical music, for example, is noted for its careful structure and emotional impact. Beginners find it difficult to compose musical tunes because they must first learn the “language” of music, such as time signatures and how to keep unit beats steady throughout a song while introducing creative uniqueness. We investigate if deep learning can be used to model this dynamic musical structure and effects. Nowadays, the music generation is quite crucial. It has a wide range of applications. Musicians and artists build on the machine’s output to create their creative work. Software-generated music or art is sometimes marketed by the companies or individuals that created it. They can be licensed to businesses and retailers for use in advertising, retail music, and so forth.</p> <h2 id="abc-notation">ABC notation</h2> <p>It uses alphanumeric Character to represent music. ABC notation is a simple but powerful ASCII musical notation format. ABC notation consists of 2 parts: first metadata and second actual music representation. Metadata: Contains the Title(T), Note length(L), Key and Clef(K) and so on. But most important as it determines the beat length of music.</p> <p>Music: Contain information about actual music.</p> <p><img src="https://user-images.githubusercontent.com/67474080/169774597-fdb9e6c5-af3f-4a40-b303-b0c0d9e532cb.png" alt="abc"/></p> <h2 id="midi-format">Midi Format</h2> <p><img src="https://user-images.githubusercontent.com/67474080/169775222-bd3d10ee-3b9c-43ab-9b84-bea3a1d99cf3.png" alt="midi format"/></p> <p>IT Uses Cord name, key and beats to represent music</p> <p>Image</p> <h2 id="sheet-music">Sheet Music</h2> <p><img src="https://user-images.githubusercontent.com/67474080/169775262-227390d2-8f12-44f3-9b71-8c4c9d7c2951.png" alt="sheet music"/></p> <p>Sheet music is a type of musical notation that is handwritten or printed and includes musical symbols to describe the pitches, rhythms, and chords of a song or instrumental musical composition.</p> <h2 id="why-abc-notation">Why abc notation?</h2> <p>Music, as we know it, is continuous in formats such as mp3, midi, wav, and so on, and for an artificially intelligent agent to learn from it, it must first master intricate polyphonic structures in music. The entire syntax, grammar, and logic of music make it difficult to build an artificial network because it has so many parameters like amplitude, pitch, timing, and so on. How does ABC notation help to solve this problem? Music is made using different notes with specific timing for each note. A beautiful melody can be made using such nodes from different instruments superimposing on themselves. Single musical instruments like piano, Guitar, Violin, and bass can be represented using sheet music. Contrary to other formats sheet music is a discrete form of musical representation. But several assumptions like common time (4/4-time signature), key of G, and so on are taken into consideration. And thus, ABC notation maps each symbol from sheet music. ABC notation has strings(A-G). and numerals to represent music. Thus, music is represented in sequential form like the sequence of DNA. This narrow band of string in sequential form is used to train the ANN.</p> <h2 id="dataset-preparation">Dataset Preparation</h2> <p>As bulk music ABC format data is not available in a single source, we’ve collected data from multiple sources using web scraping. Numbers and special characters, as well as letter notations from A to G, are utilized to represent the specified notes in the ABC notation representation of the dataset. Each letter corresponds to a note in the classical scale, such as Do, Re, or MI. Other units are utilized as extra information in addition to the notes that make up the melody, such as the reference number, composer, origin, note length, tempo, rhythm key, ornament, and so on. The characteristics of our data collection are made up of each of these values. Below is a sample example of music written in ABC notation from the dataset.</p> <p>Tools used for Web Scraping:<br/> BeautifulSoup<br/> Requests</p> <p>The websites that were used for data collections are:<br/> https://abcnotation.com/browseTunes,<br/> http://roaringjelly.org<br/> http://www.folktunefinder.com</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">requests</span>
<span class="kn">from</span> <span class="n">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>

<span class="n">f</span> <span class="o">=</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">folktune.txt</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">w</span><span class="sh">'</span><span class="p">)</span>
<span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sh">''</span><span class="p">)</span>
<span class="n">f</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>

<span class="c1">#from page1 to page10
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">11</span><span class="p">):</span>
    <span class="n">url</span> <span class="o">=</span> <span class="sh">'</span><span class="s">http://www.folktunefinder.com/tunes?features=&amp;page=</span><span class="sh">'</span><span class="o">+</span><span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="n">page</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>

    <span class="n">soup</span> <span class="o">=</span> <span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">page</span><span class="p">.</span><span class="n">text</span><span class="p">,</span> <span class="sh">'</span><span class="s">lxml</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">div</span> <span class="o">=</span> <span class="n">soup</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="sh">"</span><span class="s">div</span><span class="sh">"</span><span class="p">,</span> <span class="n">class_</span><span class="o">=</span> <span class="sh">"</span><span class="s">col-md-9</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">tunes</span> <span class="o">=</span> <span class="n">div</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="sh">'</span><span class="s">ul</span><span class="sh">'</span><span class="p">).</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">"</span><span class="s">li</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">tune</span> <span class="ow">in</span> <span class="n">tunes</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">tune</span><span class="p">.</span><span class="n">a</span><span class="p">.</span><span class="n">string</span><span class="p">)</span>

        <span class="n">site</span> <span class="o">=</span> <span class="sh">'</span><span class="s">https://www.folktunefinder.com</span><span class="sh">'</span><span class="o">+</span><span class="n">tune</span><span class="p">.</span><span class="n">a</span><span class="p">[</span><span class="sh">"</span><span class="s">href</span><span class="sh">"</span><span class="p">]</span>
        <span class="n">sitePage</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">site</span><span class="p">)</span>
        <span class="n">siteSoup</span> <span class="o">=</span> <span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">sitePage</span><span class="p">.</span><span class="n">text</span><span class="p">,</span> <span class="sh">'</span><span class="s">lxml</span><span class="sh">'</span><span class="p">)</span>

        <span class="n">f</span> <span class="o">=</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">folktune.txt</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">siteSoup</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="sh">'</span><span class="s">pre</span><span class="sh">'</span><span class="p">).</span><span class="n">string</span><span class="p">)</span>
        <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sh">'</span><span class="se">\n\n\n\n</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">f</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>
<span class="kn">import</span> <span class="n">requests</span>

<span class="n">f</span> <span class="o">=</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">roaringdata.txt</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">w</span><span class="sh">'</span><span class="p">)</span>
<span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sh">''</span><span class="p">)</span>
<span class="n">f</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>

<span class="n">url</span> <span class="o">=</span> <span class="sh">'</span><span class="s">http://roaringjelly.org/~jc/cgi/abc/find.cgi?P=.*&amp;find=FIND&amp;m=title&amp;W=wide&amp;scale=0.65&amp;limit=160&amp;thresh=5&amp;fmt=single&amp;V=1&amp;Tsel=tune&amp;Nsel=0</span><span class="sh">'</span>
<span class="n">page</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="n">soup</span> <span class="o">=</span> <span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">page</span><span class="p">.</span><span class="n">text</span><span class="p">,</span><span class="sh">'</span><span class="s">lxml</span><span class="sh">'</span><span class="p">)</span>

<span class="n">form</span> <span class="o">=</span> <span class="n">soup</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="sh">"</span><span class="s">form</span><span class="sh">"</span><span class="p">,</span> <span class="n">class_</span> <span class="o">=</span> <span class="sh">"</span><span class="s">match</span><span class="sh">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">table</span> <span class="ow">in</span> <span class="n">form</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">table</span><span class="sh">'</span><span class="p">,</span> <span class="n">class_</span> <span class="o">=</span> <span class="sh">'</span><span class="s">match</span><span class="sh">'</span><span class="p">):</span>
    <span class="n">lengthOfTable</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">table</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">tr</span><span class="sh">'</span><span class="p">))</span>

    <span class="n">checkTitle</span> <span class="o">=</span><span class="sh">''</span>
    <span class="n">title</span> <span class="o">=</span> <span class="sh">''</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">lengthOfTable</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>  <span class="c1">#final range = lengthOfTable-1
</span>        <span class="n">data</span> <span class="o">=</span> <span class="n">table</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">tr</span><span class="sh">'</span><span class="p">)[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">title</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">td</span><span class="sh">'</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">string</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">title</span>  <span class="o">+</span> <span class="sh">'</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>


        <span class="n">urlStringConcat</span> <span class="o">=</span> <span class="sh">'</span><span class="s">http://roaringjelly.org/~jc/cgi/abc/get.cgi?F=ABC&amp;U=</span><span class="sh">'</span>
        <span class="n">abcUrl</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">td</span><span class="sh">'</span><span class="p">)[</span><span class="mi">4</span><span class="p">].</span><span class="nf">find</span><span class="p">(</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">)[</span><span class="sh">"</span><span class="s">href</span><span class="sh">"</span><span class="p">]</span>
        <span class="n">abcUrl</span> <span class="o">=</span> <span class="n">abcUrl</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">U=</span><span class="sh">'</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">finalUrl</span> <span class="o">=</span> <span class="n">urlStringConcat</span> <span class="o">+</span> <span class="n">abcUrl</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">finalUrl</span> <span class="o">+</span><span class="sh">'</span><span class="se">\n\n\n\n</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">title</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">td</span><span class="sh">'</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">string</span>
        <span class="n">abcPage</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">finalUrl</span><span class="p">)</span>
        <span class="n">abcSoup</span> <span class="o">=</span> <span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">abcPage</span><span class="p">.</span><span class="n">text</span><span class="p">,</span><span class="sh">'</span><span class="s">lxml</span><span class="sh">'</span><span class="p">)</span>

        <span class="n">abc</span> <span class="o">=</span> <span class="n">abcSoup</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="sh">"</span><span class="s">pre</span><span class="sh">"</span><span class="p">).</span><span class="n">string</span>
        <span class="n">f</span> <span class="o">=</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">roaringdata.txt</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="sh">"</span><span class="s">utf-8</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">abc</span><span class="p">)</span>
        <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sh">'</span><span class="se">\n\n\n</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">f</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
        <span class="c1"># print(abc)
</span>
<span class="c1"># parsing all tables
# from each table url is extracted
# this url is used to get abc format data
# file writting
# Same title data is not implemented
</span></code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">requests</span>
<span class="kn">from</span> <span class="n">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>


<span class="n">url</span> <span class="o">=</span> <span class="sh">'</span><span class="s">https://abcnotation.com/browseTunes</span><span class="sh">'</span>
<span class="n">page</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="n">soup</span> <span class="o">=</span> <span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">page</span><span class="p">.</span><span class="n">text</span><span class="p">,</span> <span class="sh">'</span><span class="s">lxml</span><span class="sh">'</span><span class="p">)</span>

<span class="n">table</span> <span class="o">=</span> <span class="n">soup</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="sh">"</span><span class="s">table</span><span class="sh">"</span><span class="p">,</span> <span class="n">class_</span> <span class="o">=</span><span class="sh">"</span><span class="s">width100pc table-bordered</span><span class="sh">"</span><span class="p">)</span>
<span class="n">tableRows</span> <span class="o">=</span> <span class="n">table</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">tr</span><span class="sh">'</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
<span class="c1">#should be iterated to all td
</span><span class="k">for</span> <span class="n">tableData</span> <span class="ow">in</span> <span class="n">tableRows</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">td</span><span class="sh">'</span><span class="p">):</span>

    <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">firstLink</span> <span class="ow">in</span> <span class="n">tableData</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">abcWebsite</span> <span class="o">=</span> <span class="sh">'</span><span class="s">https://abcnotation.com/</span><span class="sh">'</span>
            <span class="n">firstLink</span> <span class="o">=</span>  <span class="n">abcWebsite</span> <span class="o">+</span> <span class="n">firstLink</span><span class="p">[</span><span class="sh">'</span><span class="s">href</span><span class="sh">'</span><span class="p">]</span>

            <span class="n">secondPage</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">firstLink</span><span class="p">)</span>
            <span class="n">secondSoup</span> <span class="o">=</span> <span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">secondPage</span><span class="p">.</span><span class="n">text</span><span class="p">,</span> <span class="sh">'</span><span class="s">lxml</span><span class="sh">'</span><span class="p">)</span>

            <span class="n">checkString</span> <span class="o">=</span> <span class="sh">''</span>

            <span class="n">secondSoup</span><span class="p">.</span><span class="n">pre</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">)[</span><span class="mi">1</span><span class="p">][</span><span class="sh">"</span><span class="s">href</span><span class="sh">"</span><span class="p">]</span>

            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">secondSoup</span><span class="p">.</span><span class="n">pre</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">):</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">i</span><span class="p">[</span><span class="sh">"</span><span class="s">href</span><span class="sh">"</span><span class="p">]</span>
                    <span class="nf">if</span><span class="p">(</span><span class="n">i</span><span class="p">.</span><span class="n">string</span> <span class="o">!=</span> <span class="n">checkString</span><span class="p">):</span>
                        <span class="c1"># # print(i['href'])
</span>                        <span class="c1"># print(i.string+'\n')
</span>                        <span class="n">checkString</span> <span class="o">=</span> <span class="n">i</span><span class="p">.</span><span class="n">string</span>
                        <span class="n">thirdLink</span> <span class="o">=</span> <span class="n">abcWebsite</span> <span class="o">+</span> <span class="n">i</span><span class="p">[</span><span class="sh">'</span><span class="s">href</span><span class="sh">'</span><span class="p">]</span>
                        <span class="c1">#print(thirdLink)
</span>                        <span class="nf">print</span><span class="p">(</span><span class="n">counter</span><span class="p">)</span>
                        <span class="n">counter</span> <span class="o">+=</span><span class="mi">1</span>

                        <span class="n">thirdPage</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">thirdLink</span><span class="p">)</span>
                        <span class="n">thirdSoup</span> <span class="o">=</span> <span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">thirdPage</span><span class="p">.</span><span class="n">text</span><span class="p">,</span> <span class="sh">'</span><span class="s">lxml</span><span class="sh">'</span><span class="p">)</span>
                        <span class="c1"># # print(thirdSoup.find('pre'))
</span>
                        <span class="n">f</span> <span class="o">=</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">datafromABC.txt</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">)</span>
                        <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">thirdSoup</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="sh">'</span><span class="s">pre</span><span class="sh">'</span><span class="p">).</span><span class="n">string</span><span class="p">)</span>
                        <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sh">'</span><span class="se">\n\n\n\n</span><span class="sh">'</span><span class="p">)</span>
                        <span class="n">f</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>

                <span class="k">except</span><span class="p">:</span>
                    <span class="k">pass</span>
        <span class="k">except</span><span class="p">:</span>
                <span class="k">pass</span>
</code></pre></div></div> <h2 id="data-cleaning">Data cleaning</h2> <p>Data cleaning is an important task. Uncleaned data produces more inaccuracy in the result. It helps in detecting, removing and correcting the incomplete, irrelevant, corrupt or inaccurate part of data from a dataset. Example of a cleaned sample data</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>X:111
T:11. DROPS OF BRANDY (32 bar polka)
T:Spokes's Tune/Vic's No. 2
M:2/4
L:1/8
K:D
d/2e/2|\
fd ec|dc/2B/2 AG|FA dF|EA Ad/2e/2|\
fd ec|dc/2B/2 AG|FA Bc|[1 d D D:| [2 d D D2 |
FA/2A/2 AA|GB/2B/2 BB|FA/2A/2 AF|EA A&gt;G|\
FA df|ge dc|dB AG|FE D2 :|
</code></pre></div></div> <h2 id="data-preparation">Data Preparation</h2> <p>Each data point has different characters length. But are a total of 87 unique characters in for musical representation in every data point. Each unique character has been allocated a numerical index. We’ve developed a dictionary in which the key corresponds to a character and the value is the character’s index. We’ve also made the polar opposite, where the key belongs to the index and the value is the character for ease of recognition.</p> <p><img src="https://user-images.githubusercontent.com/67474080/169769684-acd3fcfa-f3e6-4ba0-9251-b0e24beaeb16.png" alt="unique_chars"/></p> <p>Data is fed into batches. We’ll feed a batch of character sequences into our model at once. First, we must create our batches. The following parameters have been set: Batch size = 16 Sequence Length= 64, Input.txt length = 129,665, Number of unique characters = 87 Every batch has got two components, X tensor of size: 16<em>64 and Y tensor of size: 16</em>64*87.</p> <h2 id="model-building">Model Building</h2> <p>The code for model building is available in the repo above. The model buidling process is inspired form the very famous blog of Andrej Karpathy <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a>. Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems. Networks in each recurrent cell learns to control the storage of information through the use of:</p> <p><strong>Forget gate</strong><br/> The forget gate determines which data need attention and which can be overlooked. The sigmoid function passes information from the current input X(t) and the hidden state h(t-1). Sigmoid generates values ranging from 0 to 1. It determines if a portion of the previous output is required (by giving the output closer to 1). The cell will utilize this value of f(t) for point-by-point multiplication later.</p> <p><strong>Input gate</strong><br/> To update the cell status, the input gate performs the following processes. The second sigmoid function receives the current state X(t) and the previously hidden state h(t-1). The values are changed from 0 to 1 (important) (not-important). The tanh function will then be used to pass the identical information from the hidden and current states. The tanh operator will construct a vector (C(t)) with all the possible values between -1 and 1 to regulate the network. The activation functions generate output values that are ready for point-by-point multiplication.</p> <p><strong>Output gate</strong><br/> The value of the next hidden state is determined by the output gate. This state stores data from earlier inputs. The current state and previous concealed state values are first sent to the third sigmoid function. The tanh function is then used to construct a new cell state from the old cell state. These two outputs are multiplied one by one. The network selects which information the concealed state should convey based on the final value. Prediction is based on this concealed state. The new cell state and hidden state are then passed forward to the next time step.</p> <p><strong>Why LSTM?</strong><br/> Good for handling long term dependencies in data<br/> Quickly adaptable to new data<br/> Does not suffer from exploding and vanishing gradients.<br/> Language models can be operated at the character level, n-gram level, sentence level or even paragraph level.</p> <p><img src="https://user-images.githubusercontent.com/67474080/169770697-19b3705a-5d30-4074-a691-022bfb633bc5.png" alt="Underlying structure of a single LSTM cell"/></p> <p><strong>Underlying structure of a single LSTM cell</strong></p> <p><img src="https://user-images.githubusercontent.com/67474080/169770745-13b5b7de-4c4a-4a22-8c86-3ae29fbb482c.png" alt="Illustration of multiple LSTM cells to form a network"/></p> <p><strong>Illustration of multiple LSTM cells to form a network</strong></p> <h2 id="workflow">Workflow</h2> <p>The basic principle in generation of musical notes<br/> • select some seed information as the first item (e.g., the first note of a melody);<br/> • feedforward it into the recurrent network in order to produce the next item (e.g., next note);<br/> • use this next item as the next input to produce the next item; and<br/> • repeat this process iteratively until a sequence (e.g., of notes, i.e., a melody) of the desired length is produced.</p> <p>Insert Two figs- underlying stru and mutilple combns</p> <h2 id="dropout-layers">Dropout layers</h2> <p>The Dropout layer, which helps minimize overfitting, sets input units to 0 at random with a rate frequency at each step during training time. Inputs that are not set to 0 are scaled up by 1/(1 - rate) such that the total sum remains unaltered. The Dropout layer only applies when the training parameter is set to True, which means no data are dropped during inference. When using model.fit, training is automatically set to True, and in other cases, you can manually set the kwarg to True when calling the layer.</p> <h2 id="the-softmax-activation-function">The Softmax Activation function</h2> <p>In neural network models that predict a multinomial probability distribution, the softmax function is utilized as the activation function in the output layer. Softmax is utilized as the activation function for multi-class classification issues in which class membership is required to classify characters from one of the 87 mappings.</p> <h2 id="optimizer-and-hyperparameter-tuning">Optimizer and hyperparameter tuning</h2> <p>Standard gradient descent and its variants still face a lot of problems in training deep neural networks in practical solutions which gave rise to an optimizer called Adam. Adam calculates adaptive learning-rates for every parameter which makes it suitable for sparse datasets. It uses momentum which is a concept that reduces oscillations of loss function in unnecessary directions thereby accelerating the convergence towards the relevant direction. In our case the hyperparameters had values 0.9 and 0.999 and value of 0.0001 as learning rate.</p> <h2 id="model-training">Model training</h2> <p>The model was trained on a local PC with an RTX 2070 GPU and the required software. Three basic functions comprise the model training. The first function is batch data reading. The second function is to train the batches in a sequential order, and the third is to build the model in which the batches of data are trained. Our batch size is 16, and the length of the sequence is 64. The total number of batches used is 129665, with each batch containing 8104 data points. The training was initially completed over 100 epochs, with each epoch containing 126 batches. Because numerical values are easy to compute. The character to index and vice versa mapping is done here to make the computation easier since numerical values are flexible to perform complex calculations. The batch size, sequence length, and vocabulary size inputs are passed to the model construct python method. It then uses category cross entropy and the Adam optimizer to assemble the model. There are 256 LSTM layers in total, with a dropout of 0.2 and a model of sequential flow of pattern and softmax activation function. The generated train model weights are kept in a separate model weights file, and checkpointing is conducted to allow the training procedure to resume if the run time is off for various technical reasons. On a high-end system, model training took 2 hours for 100 epochs with 126 batches each. The model was also evaluated on Google Colab’s free version, which took roughly 7 hours.</p> <h2 id="next-character-probability-determination">Next character probability determination</h2> <p><img src="https://user-images.githubusercontent.com/67474080/169771366-f5dda257-23ed-4676-821a-197c27d4734a.png" alt="Next character probability determination"/></p> <p><img src="https://user-images.githubusercontent.com/67474080/169771404-3fa474d0-a21d-4c3e-9016-53067b15839b.png" alt="Next char A"/><br/></p> <p><strong>Therefore, the next character is: A</strong></p> <p><img src="https://user-images.githubusercontent.com/67474080/169771435-20459c1d-61f6-48cf-958b-ae8522a39c3c.png" alt="Next char B"/>,<br/> <strong>Therefore, the next character is: “</strong></p> <p>A limitation of the iterative feedforward strategy on an RNN, is that generation is deterministic. Indeed, a neural network is deterministic . As a consequence, feedforwarding the same input will always produce the same output. As the generation of the next note, the next note, etc., is deterministic, the same seed note will lead to the same generated series of notes .<br/> Sampling Fortunately, will solve this issue. The assumption is that the output representation of the melody is one-hot encoded. In other words, the output representation is of abc type with 87 possible characters, the output activation layer is SoftMax and generation is modeled as a classification task.</p> <h2 id="model-metrics">Model metrics</h2> <p><img src="https://user-images.githubusercontent.com/67474080/169775526-6e3e30ba-2ab7-4df2-b38b-3f5e93e0beb8.gif" alt="Epoch vs Accuracy Loss plot"/> <img src="https://user-images.githubusercontent.com/67474080/169775546-c47e9d44-ff89-4bce-a365-08025913fde7.png" alt="Testing different optimizers"/> <img src="https://user-images.githubusercontent.com/67474080/169775590-8898b3dc-5453-4614-b1f9-0739e0567541.png" alt="Testing different number of LSTM layers"/> <img src="https://user-images.githubusercontent.com/67474080/169775604-9dc148b6-150b-4620-9f4d-a33e1ddf4168.png" alt="Testing different number of dropout values"/></p> <h2 id="input">Input</h2> <p>As we used RNN technique, the first character for the output can be given. It should be noted that the first character can be random but we can always give freedom to user to choose the first musical character. Moreover, input interface can contain input field to mention the epoch of model to be used for generating the music. As the epoch goes higher the error tends to be lower but takes more computational power. Finally, the length of the piece of music can be manually set. The input interface is similar to below:</p> <p><img src="https://user-images.githubusercontent.com/67474080/169776375-3e9a598a-7a1a-42ba-a153-97a6f7343702.png" alt="Input"/></p> <p>Ouput using inbuilt package <img src="https://user-images.githubusercontent.com/67474080/169776417-c8993e45-9528-4fe3-9743-39ee81df8c43.png" alt="Output using inbuilt package"/></p> <p>Ouput using external web application abcjs.net <img src="https://user-images.githubusercontent.com/67474080/169776441-d545213f-6422-4bd7-8c5f-06374b615d62.png" alt="Output using abcjs net"/></p> <h2 id="references">References</h2> <p>[1]Andrej karpathy. “The Unreasonable Effectiveness of Recurrent Neural Networks”, May 2021<br/> [2] J. McCormack, “Grammar based music composition, Complex systems”, Chapter 96, 1996<br/> [3] K. Dongwoo, W. Christian, “Computer Assisted Composition with Recurrent Neural Network”, 2018<br/> [4] S Madjiheurem, L Qu, C Walder, “Chord2vec: Learning musical chord embeddings - Proceedings of the constructive machine learning”, 2016<br/> [5] O. D. Van, A. Dieleman, Z. Sander, S. Heiga, K. Vinyals, A. Graves, N. Kalchabrenner, A. Senior, K. Kavukcuogl, “WaveNet:A Generative Model for Raw Audio, 1609”,2016<br/> [6] Payne, Christine. MuseNet: OpenAI, Available at: openai.com/blog/musenet. 25 Apr. 2019<br/> [7] C. Dalitz, “ABC User’s Guide Book”, Apr. 2022<br/> [8] L. Richardson, Beautiful soup Documentation Release 4.4.0, 24 Dec. 2019</p>]]></content><author><name></name></author><category term="deep-learning"/><category term="char"/><category term="RNN"/><summary type="html"><![CDATA[Automated composition of abc notation using char RNN that can be played by using inbuilt music library or external library]]></summary></entry><entry><title type="html">a post with jupyter notebook</title><link href="https://badal11.github.io/project/2023/jupyter-notebook/" rel="alternate" type="text/html" title="a post with jupyter notebook"/><published>2023-07-04T12:57:00+00:00</published><updated>2023-07-04T12:57:00+00:00</updated><id>https://badal11.github.io/project/2023/jupyter-notebook</id><content type="html" xml:base="https://badal11.github.io/project/2023/jupyter-notebook/"><![CDATA[<p>To include a jupyter notebook in a post, you can use the following code:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{::nomarkdown}
{% assign jupyter_path = "assets/jupyter/blog.ipynb" | relative_url %}
{% capture notebook_exists %}{% file_exists assets/jupyter/blog.ipynb %}{% endcapture %}
{% if notebook_exists == "true" %}
    {% jupyter_notebook jupyter_path %}
{% else %}
    <span class="nt">&lt;p&gt;</span>Sorry, the notebook you are looking for does not exist.<span class="nt">&lt;/p&gt;</span>
{% endif %}
{:/nomarkdown}
</code></pre></div></div> <p>Let’s break it down: this is possible thanks to <a href="https://github.com/red-data-tools/jekyll-jupyter-notebook">Jekyll Jupyter Notebook plugin</a> that allows you to embed jupyter notebooks in your posts. It basically calls <a href="https://nbconvert.readthedocs.io/en/latest/usage.html#convert-html"><code class="language-plaintext highlighter-rouge">jupyter nbconvert --to html</code></a> to convert the notebook to an html page and then includes it in the post. Since <a href="https://jekyllrb.com/docs/configuration/markdown/">Kramdown</a> is the default Markdown renderer for Jekyll, we need to surround the call to the plugin with the <a href="https://kramdown.gettalong.org/syntax.html#extensions">::nomarkdown</a> tag so that it stops processing this part with Kramdown and outputs the content as-is.</p> <p>The plugin takes as input the path to the notebook, but it assumes the file exists. If you want to check if the file exists before calling the plugin, you can use the <code class="language-plaintext highlighter-rouge">file_exists</code> filter. This avoids getting a 404 error from the plugin and ending up displaying the main page inside of it instead. If the file does not exist, you can output a message to the user. The code displayed above outputs the following:</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/blog.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <p>Note that the jupyter notebook supports both light and dark themes.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="jupyter"/><summary type="html"><![CDATA[an example of a blog post with jupyter notebook]]></summary></entry><entry><title type="html">Student Room</title><link href="https://badal11.github.io/project/2023/student_room/" rel="alternate" type="text/html" title="Student Room"/><published>2023-07-01T13:56:00+00:00</published><updated>2023-07-01T13:56:00+00:00</updated><id>https://badal11.github.io/project/2023/student_room</id><content type="html" xml:base="https://badal11.github.io/project/2023/student_room/"><![CDATA[<h2 id="demo-video"><a href="https://youtu.be/JP1Se0RW0xU">Demo video</a></h2> <h2 id="code"><a href="https://github.com/badal11/StudentRoom11">Code</a></h2>]]></content><author><name></name></author><category term="Web"/><category term="Development"/><category term="Laravel"/><category term="VueJS"/><summary type="html"><![CDATA[Advanced Student Discussion forum made with Laravel and VueJS]]></summary></entry><entry><title type="html">a post with custom blockquotes</title><link href="https://badal11.github.io/project/2023/custom-blockquotes/" rel="alternate" type="text/html" title="a post with custom blockquotes"/><published>2023-05-12T19:53:00+00:00</published><updated>2023-05-12T19:53:00+00:00</updated><id>https://badal11.github.io/project/2023/custom-blockquotes</id><content type="html" xml:base="https://badal11.github.io/project/2023/custom-blockquotes/"><![CDATA[<p>This post shows how to add custom styles for blockquotes. Based on <a href="https://github.com/sighingnow/jekyll-gitbook">jekyll-gitbook</a> implementation.</p> <p>We decided to support the same custom blockquotes as in <a href="https://sighingnow.github.io/jekyll-gitbook/jekyll/2022-06-30-tips_warnings_dangers.html">jekyll-gitbook</a>, which are also found in a lot of other sites’ styles. The styles definitions can be found on the <a href="https://github.com/alshedivat/al-folio/blob/master/_sass/_base.scss">_base.scss</a> file, more specifically:</p> <div class="language-scss highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/* Tips, warnings, and dangers */</span>
<span class="nc">.post</span> <span class="nc">.post-content</span> <span class="nt">blockquote</span> <span class="p">{</span>
    <span class="k">&amp;</span><span class="nc">.block-tip</span> <span class="p">{</span>
    <span class="nl">border-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-tip-block</span><span class="p">);</span>
    <span class="nl">background-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-tip-block-bg</span><span class="p">);</span>

    <span class="nt">p</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-tip-block-text</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="nt">h1</span><span class="o">,</span> <span class="nt">h2</span><span class="o">,</span> <span class="nt">h3</span><span class="o">,</span> <span class="nt">h4</span><span class="o">,</span> <span class="nt">h5</span><span class="o">,</span> <span class="nt">h6</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-tip-block-title</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="k">&amp;</span><span class="nc">.block-warning</span> <span class="p">{</span>
    <span class="nl">border-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-warning-block</span><span class="p">);</span>
    <span class="nl">background-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-warning-block-bg</span><span class="p">);</span>

    <span class="nt">p</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-warning-block-text</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="nt">h1</span><span class="o">,</span> <span class="nt">h2</span><span class="o">,</span> <span class="nt">h3</span><span class="o">,</span> <span class="nt">h4</span><span class="o">,</span> <span class="nt">h5</span><span class="o">,</span> <span class="nt">h6</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-warning-block-title</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="k">&amp;</span><span class="nc">.block-danger</span> <span class="p">{</span>
    <span class="nl">border-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-danger-block</span><span class="p">);</span>
    <span class="nl">background-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-danger-block-bg</span><span class="p">);</span>

    <span class="nt">p</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-danger-block-text</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="nt">h1</span><span class="o">,</span> <span class="nt">h2</span><span class="o">,</span> <span class="nt">h3</span><span class="o">,</span> <span class="nt">h4</span><span class="o">,</span> <span class="nt">h5</span><span class="o">,</span> <span class="nt">h6</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-danger-block-title</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>A regular blockquote can be used as following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gt">&gt; This is a regular blockquote</span>
<span class="gt">&gt; and it can be used as usual</span>
</code></pre></div></div> <blockquote> <p>This is a regular blockquote and it can be used as usual</p> </blockquote> <p>These custom styles can be used by adding the specific class to the blockquote, as follows:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gt">&gt; ##### TIP</span>
<span class="gt">&gt;</span>
<span class="gt">&gt; A tip can be used when you want to give advice</span>
<span class="gt">&gt; related to a certain content.</span>
{: .block-tip }
</code></pre></div></div> <blockquote class="block-tip"> <h5 id="tip">TIP</h5> <p>A tip can be used when you want to give advice related to a certain content.</p> </blockquote> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gt">&gt; ##### WARNING</span>
<span class="gt">&gt;</span>
<span class="gt">&gt; This is a warning, and thus should</span>
<span class="gt">&gt; be used when you want to warn the user</span>
{: .block-warning }
</code></pre></div></div> <blockquote class="block-warning"> <h5 id="warning">WARNING</h5> <p>This is a warning, and thus should be used when you want to warn the user</p> </blockquote> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gt">&gt; ##### DANGER</span>
<span class="gt">&gt;</span>
<span class="gt">&gt; This is a danger zone, and thus should</span>
<span class="gt">&gt; be used carefully</span>
{: .block-danger }
</code></pre></div></div> <blockquote class="block-danger"> <h5 id="danger">DANGER</h5> <p>This is a danger zone, and thus should be used carefully</p> </blockquote>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="blockquotes"/><summary type="html"><![CDATA[an example of a blog post with custom blockquotes]]></summary></entry><entry><title type="html">a post with table of contents on a sidebar</title><link href="https://badal11.github.io/project/2023/sidebar-table-of-contents/" rel="alternate" type="text/html" title="a post with table of contents on a sidebar"/><published>2023-04-25T14:14:00+00:00</published><updated>2023-04-25T14:14:00+00:00</updated><id>https://badal11.github.io/project/2023/sidebar-table-of-contents</id><content type="html" xml:base="https://badal11.github.io/project/2023/sidebar-table-of-contents/"><![CDATA[<p>This post shows how to add a table of contents as a sidebar.</p> <h2 id="adding-a-table-of-contents">Adding a Table of Contents</h2> <p>To add a table of contents to a post as a sidebar, simply add</p> <div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">toc</span><span class="pi">:</span>
  <span class="na">sidebar</span><span class="pi">:</span> <span class="s">left</span>
</code></pre></div></div> <p>to the front matter of the post. The table of contents will be automatically generated from the headings in the post. If you wish to display the sidebar to the right, simply change <code class="language-plaintext highlighter-rouge">left</code> to <code class="language-plaintext highlighter-rouge">right</code>.</p> <h3 id="example-of-sub-heading-1">Example of Sub-Heading 1</h3> <p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p> <h3 id="example-of-another-sub-heading-1">Example of another Sub-Heading 1</h3> <p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p> <h2 data-toc-text="Customizing" id="customizing-your-table-of-contents">Customizing Your Table of Contents</h2> <p>If you want to learn more about how to customize the table of contents of your sidebar, you can check the <a href="https://afeld.github.io/bootstrap-toc/">bootstrap-toc</a> documentation. Notice that you can even customize the text of the heading that will be displayed on the sidebar.</p> <h3 id="example-of-sub-heading-2">Example of Sub-Heading 2</h3> <p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p> <h3 id="example-of-another-sub-heading-2">Example of another Sub-Heading 2</h3> <p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="toc"/><category term="sidebar"/><summary type="html"><![CDATA[an example of a blog post with table of contents on a sidebar]]></summary></entry><entry><title type="html">a post with audios</title><link href="https://badal11.github.io/project/2023/audios/" rel="alternate" type="text/html" title="a post with audios"/><published>2023-04-25T10:25:00+00:00</published><updated>2023-04-25T10:25:00+00:00</updated><id>https://badal11.github.io/project/2023/audios</id><content type="html" xml:base="https://badal11.github.io/project/2023/audios/"><![CDATA[<p>This is an example post with audios. It supports local audio files.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/epicaly-short-113909.mp3" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="https://cdn.pixabay.com/download/audio/2022/06/25/audio_69a61cd6d6.mp3" controls=""/> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between video rows, after each row, or doesn't have to be there at all. </div>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="audios"/><summary type="html"><![CDATA[this is what included audios could look like]]></summary></entry><entry><title type="html">a post with videos</title><link href="https://badal11.github.io/project/2023/videos/" rel="alternate" type="text/html" title="a post with videos"/><published>2023-04-24T21:01:00+00:00</published><updated>2023-04-24T21:01:00+00:00</updated><id>https://badal11.github.io/project/2023/videos</id><content type="html" xml:base="https://badal11.github.io/project/2023/videos/"><![CDATA[<p>This is an example post with videos. It supports local video files.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/pexels-engin-akyurt-6069112-960x540-30fps.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/pexels-engin-akyurt-6069112-960x540-30fps.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between video rows, after each row, or doesn't have to be there at all. </div> <p>It does also support embedding videos from different sources. Here are some examples:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <iframe src="https://www.youtube.com/embed/jNQXAC9IVRw" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <iframe src="https://player.vimeo.com/video/524933864?h=1ac4fd9fb4&amp;title=0&amp;byline=0&amp;portrait=0" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> </div>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="videos"/><summary type="html"><![CDATA[this is what included videos could look like]]></summary></entry><entry><title type="html">displaying beautiful tables with Bootstrap Tables</title><link href="https://badal11.github.io/project/2023/tables/" rel="alternate" type="text/html" title="displaying beautiful tables with Bootstrap Tables"/><published>2023-03-20T18:37:00+00:00</published><updated>2023-03-20T18:37:00+00:00</updated><id>https://badal11.github.io/project/2023/tables</id><content type="html" xml:base="https://badal11.github.io/project/2023/tables/"><![CDATA[<p>Using markdown to display tables is easy. Just use the following syntax:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>| Left aligned | Center aligned | Right aligned |
| :----------- | :------------: | ------------: |
| Left 1       | center 1       | right 1       |
| Left 2       | center 2       | right 2       |
| Left 3       | center 3       | right 3       |
</code></pre></div></div> <p>That will generate:</p> <table> <thead> <tr> <th style="text-align: left">Left aligned</th> <th style="text-align: center">Center aligned</th> <th style="text-align: right">Right aligned</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Left 1</td> <td style="text-align: center">center 1</td> <td style="text-align: right">right 1</td> </tr> <tr> <td style="text-align: left">Left 2</td> <td style="text-align: center">center 2</td> <td style="text-align: right">right 2</td> </tr> <tr> <td style="text-align: left">Left 3</td> <td style="text-align: center">center 3</td> <td style="text-align: right">right 3</td> </tr> </tbody> </table> <p></p> <p>It is also possible to use HTML to display tables. For example, the following HTML code will display a table with <a href="https://bootstrap-table.com/">Bootstrap Table</a>, loaded from a JSON file:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;table</span>
  <span class="na">id=</span><span class="s">"table"</span>
  <span class="na">data-toggle=</span><span class="s">"table"</span>
  <span class="na">data-url=</span><span class="s">"{{ '/assets/json/table_data.json' | relative_url }}"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;thead&gt;</span>
    <span class="nt">&lt;tr&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"id"</span><span class="nt">&gt;</span>ID<span class="nt">&lt;/th&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"name"</span><span class="nt">&gt;</span>Item Name<span class="nt">&lt;/th&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"price"</span><span class="nt">&gt;</span>Item Price<span class="nt">&lt;/th&gt;</span>
    <span class="nt">&lt;/tr&gt;</span>
  <span class="nt">&lt;/thead&gt;</span>
<span class="nt">&lt;/table&gt;</span>
</code></pre></div></div> <table data-toggle="table" data-url="/assets/json/table_data.json"> <thead> <tr> <th data-field="id">ID</th> <th data-field="name">Item Name</th> <th data-field="price">Item Price</th> </tr> </thead> </table> <p></p> <p>By using <a href="https://bootstrap-table.com/">Bootstrap Table</a> it is possible to create pretty complex tables, with pagination, search, and more. For example, the following HTML code will display a table, loaded from a JSON file, with pagination, search, checkboxes, and header/content alignment. For more information, check the <a href="https://examples.bootstrap-table.com/index.html">documentation</a>.</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;table</span>
  <span class="na">data-click-to-select=</span><span class="s">"true"</span>
  <span class="na">data-height=</span><span class="s">"460"</span>
  <span class="na">data-pagination=</span><span class="s">"true"</span>
  <span class="na">data-search=</span><span class="s">"true"</span>
  <span class="na">data-toggle=</span><span class="s">"table"</span>
  <span class="na">data-url=</span><span class="s">"{{ '/assets/json/table_data.json' | relative_url }}"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;thead&gt;</span>
    <span class="nt">&lt;tr&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-checkbox=</span><span class="s">"true"</span><span class="nt">&gt;&lt;/th&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"id"</span> <span class="na">data-halign=</span><span class="s">"left"</span> <span class="na">data-align=</span><span class="s">"center"</span> <span class="na">data-sortable=</span><span class="s">"true"</span><span class="nt">&gt;</span>ID<span class="nt">&lt;/th&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"name"</span> <span class="na">data-halign=</span><span class="s">"center"</span> <span class="na">data-align=</span><span class="s">"right"</span> <span class="na">data-sortable=</span><span class="s">"true"</span><span class="nt">&gt;</span>Item Name<span class="nt">&lt;/th&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"price"</span> <span class="na">data-halign=</span><span class="s">"right"</span> <span class="na">data-align=</span><span class="s">"left"</span> <span class="na">data-sortable=</span><span class="s">"true"</span><span class="nt">&gt;</span>Item Price<span class="nt">&lt;/th&gt;</span>
    <span class="nt">&lt;/tr&gt;</span>
  <span class="nt">&lt;/thead&gt;</span>
<span class="nt">&lt;/table&gt;</span>
</code></pre></div></div> <table data-click-to-select="true" data-height="460" data-pagination="true" data-search="true" data-toggle="table" data-url="/assets/json/table_data.json"> <thead> <tr> <th data-checkbox="true"></th> <th data-field="id" data-halign="left" data-align="center" data-sortable="true">ID</th> <th data-field="name" data-halign="center" data-align="right" data-sortable="true">Item Name</th> <th data-field="price" data-halign="right" data-align="left" data-sortable="true">Item Price</th> </tr> </thead> </table>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="tables"/><summary type="html"><![CDATA[an example of how to use Bootstrap Tables]]></summary></entry></feed>