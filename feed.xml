<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://badal11.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://badal11.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-02-28T15:27:04+00:00</updated><id>https://badal11.github.io/feed.xml</id><title type="html">blank</title><subtitle>This is a test. A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Pixel to Plate - Recipe from Food Image</title><link href="https://badal11.github.io/project/2023/Pixel-to-Plate-Recipe-from-Food-Image/" rel="alternate" type="text/html" title="Pixel to Plate - Recipe from Food Image"/><published>2023-12-01T13:56:00+00:00</published><updated>2023-12-01T13:56:00+00:00</updated><id>https://badal11.github.io/project/2023/Pixel%20to%20Plate:%20Recipe%20from%20Food%20Image</id><content type="html" xml:base="https://badal11.github.io/project/2023/Pixel-to-Plate-Recipe-from-Food-Image/"><![CDATA[<p>Ever found yourself staring at a tantalizing image of a scrumptious dish with no clue how to recreate it? This project, Pixel to Plate Recipe-from-Food-Image leverages the prowess of deep learning to provide you with the perfect solution. This application utilizes deep learning techniques to analyze food images and generate detailed cooking recipes. With just a snapshot of favorite dish, this system extracts crucial information, including:</p> <p><strong>Recipe Title</strong>: An enticing and descriptive title for the dish.</p> <p><strong>Ingredients</strong>: A comprehensive list of all the required ingredients.</p> <p><strong>Instructions</strong>: Step-by-step cooking instructions to ensure flawless dish replication.</p> <h3 id="key-features">Key Features:</h3> <p>This model employs state-of-the-art computer vision algorithms to identify ingredients and cooking processes within food images. Advanced natural language processing is used to create coherent and easy-to-follow cooking instructions.</p>]]></content><author><name></name></author><category term="Deep_learning"/><category term="project"/><summary type="html"><![CDATA[Ever found yourself staring at a tantalizing image of a scrumptious dish with no clue how to recreate it? This project, Pixel to Plate Recipe-from-Food-Image leverages the prowess of deep learning to provide you with the perfect solution. This application utilizes deep learning techniques to analyze food images and generate detailed cooking recipes. With just a snapshot of favorite dish, this system extracts crucial information, including:]]></summary></entry><entry><title type="html">Intelligent Slide Generation from Lecture Videos</title><link href="https://badal11.github.io/project/2023/Intelligent-Slide-Generation/" rel="alternate" type="text/html" title="Intelligent Slide Generation from Lecture Videos"/><published>2023-09-22T13:56:00+00:00</published><updated>2023-09-22T13:56:00+00:00</updated><id>https://badal11.github.io/project/2023/Intelligent-Slide-Generation</id><content type="html" xml:base="https://badal11.github.io/project/2023/Intelligent-Slide-Generation/"><![CDATA[<h1 id="video-to-slides-converter-transform-video-lectures-into-slide-presentations">Video to Slides Converter: Transform Video Lectures into Slide Presentations</h1> <p>This is a simple video-to-slide converter application that aims to obtain slide images (or pdf) given slide or lecture videos.</p> <p>This is highly useful when one wishes to have a video lecture(with or without animations) in the form of slides – either a ppt or pdf. However, more often than not, slides are not provided when such video lectures are hosted on platforms like YouTube. This project aims to build a robust application that can convert video lectures into corresponding slides using techniques such as basic frame differencing and statistical background subtraction models such as <strong>KNN</strong> or <strong>GMG</strong>.</p> <p>This project converts video frames into slide PDFs. Here’s a step-by-step explanation of how the code works:</p> <ol> <li><strong>Argument Parsing:</strong> <ul> <li>The script uses the <code class="language-plaintext highlighter-rouge">argparse</code> module to handle command-line arguments. Various parameters, such as the input video path (<code class="language-plaintext highlighter-rouge">video_path</code>), output directory (<code class="language-plaintext highlighter-rouge">out_dir</code>), background subtraction type (<code class="language-plaintext highlighter-rouge">type</code>), hash function (<code class="language-plaintext highlighter-rouge">hash-func</code>), hash size (<code class="language-plaintext highlighter-rouge">hash-size</code>), similarity threshold (<code class="language-plaintext highlighter-rouge">threshold</code>), queue length (<code class="language-plaintext highlighter-rouge">queue-len</code>), and flags for post-processing (<code class="language-plaintext highlighter-rouge">no_post_process</code>) and PDF conversion (<code class="language-plaintext highlighter-rouge">convert_to_pdf</code>), can be specified.</li> </ul> </li> <li><strong>Argument Validation:</strong> <ul> <li>The script validates certain arguments, such as ensuring that the queue length is positive and handling the case where it’s not. It also checks if the provided video path is a valid URL or an existing file.</li> </ul> </li> <li><strong>Video Download (if URL provided):</strong> <ul> <li>If the provided video path is a URL, the script attempts to download the video using the <code class="language-plaintext highlighter-rouge">download_video</code> function. If successful, it sets a flag (<code class="language-plaintext highlighter-rouge">temp_file</code>) to indicate that a temporary file was created.</li> </ul> </li> <li><strong>Output Directory Creation:</strong> <ul> <li>The script creates the output directory for storing the resulting slides. The directory is created based on the input video path and the chosen background subtraction type.</li> </ul> </li> <li><strong>Background Subtraction and Slide Capture:</strong> <ul> <li>Depending on the specified background subtraction type, the script calls either <code class="language-plaintext highlighter-rouge">capture_slides_frame_diff</code> (for frame differencing) or <code class="language-plaintext highlighter-rouge">capture_slides_bg_modeling</code> (for Gaussian Mixture Model - GMG or K-Nearest Neighbors - KNN background subtraction).</li> </ul> </li> <li><strong>Post-Processing (Duplicate Removal):</strong> <ul> <li>If post-processing is not disabled (<code class="language-plaintext highlighter-rouge">no_post_process</code> is not set), the script performs post-processing using difference hashing to identify and remove duplicate slides.</li> </ul> </li> <li><strong>PDF Conversion (Optional):</strong> <ul> <li>If the <code class="language-plaintext highlighter-rouge">convert_to_pdf</code> flag is set, the script converts the captured slides in the output directory to a PDF file using the <code class="language-plaintext highlighter-rouge">convert_slides_to_pdf</code> function.</li> </ul> </li> <li><strong>Temporary File Cleanup (if created):</strong> <ul> <li>If a temporary file was created during video download (<code class="language-plaintext highlighter-rouge">temp_file</code> is set), it is removed after processing.</li> </ul> </li> </ol> <p>The script utilizes modular functions from other Python files (<code class="language-plaintext highlighter-rouge">config.py</code>, <code class="language-plaintext highlighter-rouge">download_video.py</code>, <code class="language-plaintext highlighter-rouge">bg_modeling.py</code>, <code class="language-plaintext highlighter-rouge">frame_differencing.py</code>, <code class="language-plaintext highlighter-rouge">post_process.py</code>, and <code class="language-plaintext highlighter-rouge">utils.py</code>) for specific tasks like downloading videos, capturing slides, performing background modeling, and post-processing.</p>]]></content><author><name></name></author><category term="Machine_learning"/><category term="project"/><summary type="html"><![CDATA[This is a simple video-to-slide converter application that aims to obtain slide images (or pdf) given slide or lecture videos.]]></summary></entry><entry><title type="html">Playback Speed Adjustment skip silence or less crucial portions of videos</title><link href="https://badal11.github.io/project/2023/Playback-Speed-Adjustment/" rel="alternate" type="text/html" title="Playback Speed Adjustment skip silence or less crucial portions of videos"/><published>2023-08-12T13:56:00+00:00</published><updated>2023-08-12T13:56:00+00:00</updated><id>https://badal11.github.io/project/2023/Playback-Speed-Adjustment</id><content type="html" xml:base="https://badal11.github.io/project/2023/Playback-Speed-Adjustment/"><![CDATA[<h2 id="overview">Overview</h2> <p>The project is designed to automate the process of summarizing videos by analyzing their audio content and intelligently adjusting playback speed based on the perceived loudness. This can be particularly useful for skipping through less relevant sections, such as silent periods or segments where only background noise is present. The project is divided into several components, including audio processing, machine learning model training, video summarization, and indexing.</p> <h2 id="components">Components</h2> <h3 id="1-video_summarizerpy">1. <strong>video_summarizer.py</strong></h3> <p>This script serves as the main entry point for video summarization. It utilizes modules for audio processing, machine learning, and indexing to process a given video file. The process involves extracting audio features, predicting loudness using a trained model, and indexing the video for future reference.</p> <h3 id="2-audio_processingpy">2. <strong>audio_processing.py</strong></h3> <p>Responsible for handling audio-related tasks, this module extracts features from audio chunks of the video. It uses the PyDub library to convert the video’s audio into manageable segments and extracts basic features. More sophisticated feature extraction methods can be implemented based on project requirements.</p> <h3 id="3-modelpy">3. <strong>model.py</strong></h3> <p>Handles the training and utilization of the machine learning model. Currently, a Random Forest Classifier is used for predicting loudness. The model is trained on simulated data representing loud and silent audio segments. It can be expanded and replaced with more advanced models for improved accuracy.</p> <h3 id="4-indexingpy">4. <strong>indexing.py</strong></h3> <p>Deals with indexing videos for efficient future access. It checks whether a video has been previously indexed, and if not, it generates an index file containing the video’s timestamp and corresponding loudness predictions. This index is then sent to a server for future reference.</p> <h2 id="usage">Usage</h2> <ol> <li><strong>Training the Model:</strong> <ul> <li>Run <code class="language-plaintext highlighter-rouge">video_summarizer.py</code>.</li> <li>The script will check if the machine learning model has been trained. If not, it will train the model using simulated data.</li> </ul> </li> <li><strong>Summarizing a Video:</strong> <ul> <li>Specify the path to the video in <code class="language-plaintext highlighter-rouge">video_summarizer.py</code>.</li> <li>Run the script to extract audio features, predict loudness, and adjust playback speed.</li> </ul> </li> <li><strong>Server Interaction:</strong> <ul> <li>The script communicates with a server (<code class="language-plaintext highlighter-rouge">https://videosummarizer.soptik.tech/</code>) for video indexing.</li> <li>Simulated requests are used for illustration purposes. Actual server interaction depends on the project’s implementation.</li> </ul> </li> </ol> <h2 id="project-structure">Project Structure</h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>|-- video_summarizer.py
|-- audio_processing.py
|-- model.py
|-- indexing.py
|-- model.pkl
|-- data/
|   |-- simulated_data.npy
|   |-- ...
|-- documentation/
|   |-- README.md
|   |-- ...
</code></pre></div></div> <h2 id="dependencies">Dependencies</h2> <ul> <li>PyDub</li> <li>NumPy</li> <li>scikit-learn</li> <li>requests</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>The Video Summarizer project provides a foundation for automating the video summarization process. It combines audio processing, machine learning, and indexing to intelligently adjust playback speed based on audio content. While the current implementation uses simulated data, the project can be extended with real-world data and more advanced features for accurate and efficient video summarization.</p>]]></content><author><name></name></author><category term="Machine_learning"/><category term="project"/><summary type="html"><![CDATA[The project is designed to automate the process of summarizing videos by analyzing their audio content and intelligently adjusting playback speed based on the perceived loudness. This can be particularly useful for skipping through less relevant sections, such as silent periods or segments where only background noise is present. The project is divided into several components, including audio processing, machine learning model training, video summarization, and indexing.]]></summary></entry><entry><title type="html">Audio Based Nepali Political Figure</title><link href="https://badal11.github.io/project/2023/Audio-Based-Nepali-Political-Figure/" rel="alternate" type="text/html" title="Audio Based Nepali Political Figure"/><published>2023-05-15T13:56:00+00:00</published><updated>2023-05-15T13:56:00+00:00</updated><id>https://badal11.github.io/project/2023/Audio-Based%20Nepali%20Political%20Figure</id><content type="html" xml:base="https://badal11.github.io/project/2023/Audio-Based-Nepali-Political-Figure/"><![CDATA[<h1 id="introduction"><strong>Introduction</strong></h1> <p>Identifying people based on their audio is a fascinating topic. Every human voice is distinctive; as people age, their voices vary, and they sound different in different contexts, altering the problem landscape correspondingly. Speaker Identification is concerned with identifying a person based on the spoken audio of that person. Traditionally, speech processing models were widely recognized, but convolution neural networks have lately demonstrated that they, too, may generate astounding outcomes.</p> <p><strong>Data Description</strong></p> <p>I was always interested to train a model in my language. Therefore, I collected Nepali audio from youtube of exactly 34 politicians male and female speaking at different venues, providing proper care not to include noise in the audio and the average audio length is around 5 minutes. The dataset is available <a href="https://drive.google.com/file/d/44vgeREGE_4343R78XA/view?usp=sharing">here</a></p> <h1 id="converting-to-deep-learning-problem"><strong>Converting to Deep Learning Problem</strong></h1> <p><img src="https://miro.medium.com/v2/resize:fit:1050/1*jiuTG4UgSKhFt9wN5XhFeA.png" alt=""/></p> <p>Showing the formulation of deep learning problem</p> <p>The necessary preprocessing is done depending upon the audio we have it may include removing some part of the audio, removing silence, etc. As CNN can only work on images (1d CNN can word on text but not considering here), the spectrogram images is created from the preprocessed audio and provided as input to the model which will perform training to produce the result. Basically there are four steps:</p> <ol> <li> <p>Reading the audio</p> </li> <li> <p>Perform necessary pre-processsing</p> </li> <li> <p>Selecting a Model</p> </li> <li> <p>Preparing data for the model</p> </li> <li> <p>Model Architecture</p> </li> </ol> <h2 id="1-reading-the-audio">1. Reading the audio</h2> <p>The first step in this many step process is reading an audio, the audio can be in many format .wav, .mp3, etc. It can be done my using signal processing library Librosa.</p> <h1 id="reading-the-audio">Reading the audio</h1> <p>import librosa<br/> path= ‘path to audio’sample_rate= 22050<br/> arr_audio,_=librosa.load(path,sr= sample_rate) #arr_audio is an array of amplitudes if the audio is 2sec in length<br/> #it will be (2,2*22050)= (2,44100), where 2 in the array is due to #dual channel of audio</p> <p>As audio is nothing more than a signal, where on the x-axis there is time and on the y-axis, there is amplitude, as it is a continuous signal we want to convert in a discrete form so that we can extract the features from it. By sampling the signal at a specific rate we can do this, what does it mean is that we are getting the value of amplitude from some instances of time. Here the sampling rate is 22050, meaning on one second of audio we will extract the amplitude 22050.</p> <h2 id="2-preprocessing"><strong>2. Preprocessing</strong></h2> <p><strong>2.1 Converting to mono channel</strong></p> <p>As the audio data is from youtube, it is a dual-channel meaning for the particular instance of time, there is two value of amplitude one for the left earpiece and one for the right. But we need a single value of amplitude at any instance. So just by taking the mean of the dual-channel amplitude array, we can convert it into a mono channel.</p> <p>arr_audio= arr_audio.sum(0)/2</p> <p><strong>2.2 Removing noise</strong></p> <p>In every audio, the first 1 minute is an advertisement or some other kind of unwanted audio segment. So we will be removing the first and the last 1 minute from every audio.</p> <p>arr_audio= arr_audio[sample_rate<em>60:arr_audio.shape[0]-sample_rate</em>60]</p> <p><strong>2.3 Removing silence</strong></p> <p>While speaking they may we momentary pause in between, we want our model to learn the characteristics of individual persons audio, it will learn nothing from the silence. So the silence needs to be removed.</p> <p><img src="https://miro.medium.com/v2/resize:fit:1050/1*pGd2ErQigpjxDeMyrjbqpA.png" alt=""/></p> <p>Steps to remove silence</p> <p>from pydub import AudioSegment<br/> from pydub.silence import split_on_silencedef float_to_int(array, type=np.int16):<br/> if array.dtype == type:<br/> return array if array.dtype not in [np.int16, np.int32, np.int64]:<br/> if np.max(np.abs(array)) == 0:<br/> array[:] = 0<br/> array = type(array * np.iinfo(type).max)<br/> else:<br/> array = type(array / np.max(np.abs(array)) * np.iinfo(type).max)<br/> return arraydef int_to_float(array, type=np.float32):<br/> if array.dtype == type:<br/> return array if array.dtype not in [np.float16, np.float32, np.float64]:<br/> if np.max(np.abs(array)) == 0:<br/> array = array.astype(np.float32)<br/> array[:] = 0<br/> else:<br/> array = array.astype(np.float32) / np.max(np.abs(array))<br/> return array#Step1:Create an convert to int and create audio_segment#Convert to float<br/> feature= float_to_int(arr_audio)#Create audio segment<br/> audio = AudioSegment(feature.tobytes(),frame_rate = self.sample_rate,sample_width = feature.dtype.itemsize\<br/> ,channels = 1) #Step2:removing the silence from the audio segment<br/> audio_chunks= split_on_silence(audio,min_silence_len=min_silence,silence_thresh= -30,keep_silence= 100)<br/> #Step3:converting it back to the array<br/> #Convert audio segment to array<br/> arr_audio= sum(audio_chunks)<br/> arr_audio= np.array(arr_audio.get_array_of_samples())#Convert back to int<br/> arr_audio= self.int_to_float(arr_audio)</p> <p>After doing all the preprocessing the data looks like this:</p> <p><img src="https://miro.medium.com/v2/resize:fit:1050/1*8MlE7HkDzwRmRK7vmV_z2A.png" alt=""/></p> <p>A single data point contains the name of the speaker, its gender, Duration meaning the length of the audio in seconds, and finally Features that contain the amplitude of the audio sampled at a specific sampling rate.</p> <h1 id="3-selecting-a-model"><strong>3. Selecting a model</strong></h1> <p>There are 34 speakers if we consider each speaker as a single class there will be 34 classes. As the speaker increases, the class will also increase which is not an ideal scenario. To identify a single class supposes k1, the model needs to learn features that clearly distinguish it from other (K-k1) classes which will require lots of data ie increasing the length of audio of a single speaker. I don’t think a user wants to spend time recording its audio, he/she is willing to record for 5–30 sec and get done with it. So, using this strategy is not a choice.</p> <p>A Siamese Network is a network that contains two identical subnetworks ‘<em>identical’</em> here means, they have the same configuration with the same parameters and weights. Parameter updating is mirrored across both sub-networks. It is used to find the similarity of the inputs by comparing their feature vectors, so these networks are used in many applications.</p> <p>If we want to add a new speaker here we can update the neural network and retrain it on the whole dataset which was not possible in traditional architecture. SNNs, on the other hand, learn a similarity function. Thus, we can train it to see if the two images are the same (which we will do here). This enables us to classify new speakers without training the network again.</p> <p><img src="https://miro.medium.com/v2/resize:fit:1050/1*1ObEvWMVJA0AAdW__g-4kA.png" alt=""/></p> <p>Siamese Network.(<a href="https://medium.com/swlh/one-shot-learning-with-siamese-network-1c7404c35fda">https://medium.com/swlh/one-shot-learning-with-siamese-network-1c7404c35fda</a>)</p> <p>Siamese networks accept two inputs x1 and x2. For our case, it is either two spectrogram images from the same user or from different users. Images from the same user are labeled as 1 and images from different users are labeled as 0. These images are sent through an identical network and feature vectors f(x1) and f(x2) are generated. A function is used to calculate the difference between these two feature vectors. The difference score between the feature vector for the same user will be small as they will be located in the same neighborhood in that dimensional space whereas for the different users the feature vector difference will be large.</p> <p>Lets take example the two feature vector are (0.5,0.7) and (1,0.7)</p> <p>Different User:</p> <p><img src="https://miro.medium.com/v2/resize:fit:1023/1*PTAYjnjYVYPg42u3O0LYZw.png" alt=""/></p> <p>As we can see the model tries to increase the distance between the feature vector if they are from different user.</p> <p>From same user:</p> <p><img src="https://miro.medium.com/v2/resize:fit:999/1*gsKzbC3RMMmUjy8hCbGeHg.png" alt=""/></p> <p>Similarly if the feature vectors is from same user after every iteration model reduces the distance between the two bringing them closer in the vector space.</p> <h1 id="4-preparing-data-for-siamese-network"><strong>4. Preparing data for siamese network</strong></h1> <p>Let’s suppose there is an audio from speaker Hari of 0.3miliSeconds and by using the sampling rate= 22050, we can compute amplitude at 8 instances ie (0.3*1000 sec)/22050~10. SO, the feature vector for this audio segment looks like this:</p> <p><img src="https://miro.medium.com/v2/resize:fit:1050/1*ae7TmYPKXcvKYqc4AOReMQ.png" alt=""/></p> <h2 id="1-train-test-splitting"><strong>1. Train test splitting</strong></h2> <p><img src="https://miro.medium.com/v2/resize:fit:1050/1*-BjBafPxBRRb8veW6HAwdA.png" alt=""/></p> <p>The first 80% of the audio segment was utilized to generate a training set, while the remaining 20% was used to create the test set.</p> <h2 id="2-getting-similar-and-disimilar-pairs-of-spectogram-from-speakers-audio"><strong>2 Getting similar and disimilar pairs of spectogram from speakers audio</strong></h2> <p><strong>2.1 Similar pair:</strong></p> <p>A single pair of the image of spectrogram created from the specific user audio will create one data point and the label of this data point will be 1 indicating that this pair of spectrogram images belong to individual audio.</p> <p><img src="https://miro.medium.com/v2/resize:fit:1050/1*8miEz-MVNMlpILGGYTPmtw.png" alt=""/></p> <p>The most important parameter while creating a spectrogram is the audio window length which defines the length of the audio segment which will be used to create a single spectrogram image. Similarly, the starting position for these two windows will be selected randomly. For my implementation, I have selected 100 pairs of datapoint for an single user, therefore the total datapoint which will have label 1 are 34*100= 3400.</p> <p><strong>2.2 Dissimilar pair:</strong></p> <p>A single data point contains one spectrogram from one user and another spectrogram from another user, with the label 0 indicating that the spectrograms belongs to a separate user. As a result, we will pair each user with every other user and construct all possible combinations of pairs from distinct users.</p> <p>We chose 25 pairs of spectrogram photographs, one from each of two different people. As a result, we will generate 25 pairs of spectrograms with each other user for each user (33 in our situation), for a total of 33<em>25= 825 pairs of images. Similarly, the total number of dissimilar spectrogram pairings for 34 persons is 34</em>825=28050, and these pairs are labeled as 0, indicating that the spectrogram images in the pair are from distinct people.</p> <p>The dataset looks like this after performing all these tasks:</p> <p><img src="https://miro.medium.com/v2/resize:fit:1050/1*7Nk4kOCYZKZOpKmAcMLkZA.png" alt=""/></p> <h1 id="4-model-architecture">4 Model Architecture</h1> <p>Because the image number is just 31000, transfer learning is the only option. As a result, DenseNet121, which has already been trained on “imagenet,” is employed here. We must utilize the spectrogram height and breadth that we created while initializing this network.</p> <p>subnetwork= tf.keras.applications.densenet.DenseNet121(include_top=False, weights=’imagenet’, ,input_tensor=None,input_shape=(height,width,3), pooling=None)</p> <p>Similarly, we need a distance function which calculates the distance between two feature vector produced by the DenseNet. For this purpose we will be using euclidean distance:</p> <p>def euclidean_distance(self,inp): x,y= inp<br/> sum_square = tf.math.reduce_sum(tf.math.square(x - y),axis=1,keepdims=True)<br/> return tf.math.sqrt(tf.math.maximum(sum_square,1e-07))</p> <p>Finally, let’s put all the pieces together and create a complete model.</p> <p>def create_siamese_network(height,width):<br/> #Inputs for two spectrogram images<br/> image1= Input(shape=(height,width,3))<br/> image2= Input(shape=(height,width,3)) #sending the images through the DenseNet<br/> output1= Flatten()(subnetwork(image1))<br/> output2= Flatten()(subnetwork(image2)) #Calculating the distance between the feature vector<br/> output = layers.Lambda(euclidean_distance)([output1, output2])</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    model= Model(inputs=[image1,image2],outputs=output)  
    return model
</code></pre></div></div> <p>DenseNet will generate a feature matrix with multiple channels; to get a vector from it, we flatten the output matrix and calculate the distance between the channels.</p> <h1 id="5-training-the-model">5. Training the model</h1> <h2 id="51-contractive-loss">5.1 Contractive Loss</h2> <p>We need a loss function that minimizes the distance between spectrograms from the same individual and maximizes the distance between spectrograms of a different individual. Contractive loss works exactly like that and we will be using it here to train the model.</p> <p>margin= 3.0<br/> def contractive_loss(label,distance):</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>difference= margin-distance  
temp= tf.where(tf.less(difference,[0.0]),[0.0],difference)     
loss= tf.math.reduce_mean(tf.cast(label,dtype=tf.float32)*tf.math.square(distance)+(1-tf.cast(label,dtype=tf.float32))*tf.math.square(temp))  
  
return loss
</code></pre></div></div> <p>The margin value is the most crucial parameter in the contractive loss. This number tells the model what the minimal distance between feature vectors from different persons should be, and we set the margin value to 3 in our implementation. Similarly, the model attempts to reduce the distance between spectrograms of the same image to 0.</p> <h2 id="52-evaluation">5.2 Evaluation</h2> <p>similar distance= The distance between the feature vectors of two spectrogram images generated from the same person.</p> <p>dissimilar distance= The distance between the feature vectors of two spectrogram images generated from the different person.</p> <p>At first, the similar and dissimilar distances were almost identical, but as the model trained, they began to diverge, and after 30 epochs, the distance between training and testing looks like this:</p> <p><img src="https://miro.medium.com/v2/resize:fit:1050/1*1KQ-JBHGGa0GzVbWl-e8sw.png" alt=""/></p> <p>Epoch vs Distance</p> <p>For the training set, the average distance between spectrograms of the same individual was 1.4, whereas the distance between spectrograms of different people was 4.7. Similarly, the average similar distance was 1.8 and the average dissimilar distance was 4.4 for the test set.</p> <p>Because the model simply outputs the value of the distance between the pairings, we can conclude that if the distance between the pairs is less than 1.8, we will label it as 1, and else it will be labeled as 0.</p> <p><img src="https://miro.medium.com/v2/resize:fit:897/1*HRiVAUdY2Isq67TFSIiCsg.png" alt=""/></p> <p>The precision and recall values for the training set were determined to be 0.87 and 0.84, respectively, whereas the precision and recall values for the test set were close to 0.8.</p> <h1 id="6-training-parameter">6. Training Parameter</h1> <p>The model was trained for 30 epochs with a learning rate of 1e-4. The Adam optimizer was employed, with a polynomial learning rate decay and the first 100 steps of the models operating as warn steps.</p>]]></content><author><name></name></author><category term="Deep_Learning"/><category term="project"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Predictive Analysis for Nabil Bank Stock Prices</title><link href="https://badal11.github.io/project/2023/stock_price_nabil/" rel="alternate" type="text/html" title="Predictive Analysis for Nabil Bank Stock Prices"/><published>2023-02-12T13:56:00+00:00</published><updated>2023-02-12T13:56:00+00:00</updated><id>https://badal11.github.io/project/2023/stock_price_nabil</id><content type="html" xml:base="https://badal11.github.io/project/2023/stock_price_nabil/"><![CDATA[ <p>In statistical terms, time series forecasting is the process of analyzing the time series data using statistics and modeling to make predictions and informed strategic decisions. It falls under Quantitative Forecasting. Examples of Time Series Forecasting are weather forecast over next week, forecasting the closing price of a stock each day etc. In this article we will see different types of models which can be used for this analysis and pick what is best for what situations.</p> <p><br/></p> <h2 id="time-series-data">Time series data</h2> <p>Time series data are simply measurements or events that are tracked, monitored, downsampled, and aggregated over time. This could be server metrics, application performance monitoring, network data, sensor data, events, clicks, trades in a market, and many other types of analytics data. We will be taking stock price data to perform our analysis.</p> <p>Nabil bank is a bank located in Nepal, it’s been trading in NEPSE(Nepal Stock Exchange ) for the past 20 years. We can easily get this data by going to the NEPSE website. This data contains four features such as the price of the stock when the market opens on a particular date, the maximum value of the stock, the minimum value, and the value of the stock at which the market close on that day.</p> <p><img src="/assets/img/from_other/stock_dataframe.png#center" alt="Image of the data" title="Dataset"/></p> <p><br/></p> <h2 id="preprocessing">Preprocessing</h2> <h3 id="1-normalization">1. Normalization</h3> <p>We need to normalize between 0-1, to remove the problem which arises if the features having in different scales. But when normalizing validate and test data don’t use the validate.max() or text.max() and validate.min() or text.min() for their respective normalization use train.max and train.min for both of them. Because we can’t look at the validate or test dataset they are unknown to us. The important thing to note here is that the normalization has been done on the input feature only not on the label, the model will predict the actual value of stock.</p> <p><br/></p> <h3 id="2-sliding-window">2. Sliding window</h3> <p>To perform Supervised learning the dataset should have inputs and its corresponding label. Data windowing is a popular technique for converting historical data like time series to data suitable for supervised learning. It works as it sounds, we select a window for inputs and feed the model the data which has been selected into that window and the model will try to predict the label for that window. The main features of the input windows are:</p> <p>• The width (number of time steps) of the input and label windows.</p> <p>• The time offset between them.</p> <p>• Which features are used as inputs, labels, or both.</p> <p>Depending on the task and type of model we may want to generate a variety of data windows. Here are some examples:</p> <ol> <li>A model that makes a prediction one hour into the future given six days of history, would need a window like this:</li> </ol> <p><img src="/assets/img/from_other/example1.png#center" alt="Example_1" title="Example 1"/></p> <ol> <li>Similarly, to make a single prediction 24 days into the future, given 24 days of history, we might define a window like this:</li> </ol> <p><img src="/assets/img/from_other/example2.png#center" alt="Example_2" title="Example 1"/></p> <p><a href="https://www.tensorflow.org/tutorials/">source</a></p> <p>Therefore, depending upon the task and model we can generate varieties of inputs which helps to reduce the redundancy of code as by defining an data window using a class.</p> <p><br/></p> <h2 id="models">Models</h2> <p>In time series forecasting depending upon the number of steps we are going to do the prediction for the models can be classified into two types:</p> <h3 id="1-single-step-model">1. Single step Model</h3> <p>In single step model, model will look one step into the future. For example given all the past one month of stock data model will predict what will be the stock value tomorrow. For this task we will be using models like:</p> <p><br/></p> <h4 id="11-dense-model">1.1 Dense model:</h4> <p>A single dense layer is a single layer of fully connected neural network. Here, we will be sending our Inputs of specific input width into multiple dense layer and finally the output of these dense layer will be send though a single neuron dense layer to produce a single step output. It is an regression problem where we take Open, Close, Low and High as input to predict the closing value of the stock.</p> <p>def dense_func(input_shape): input= tf.keras.Input(shape= tf.constant(input_shape)) x= tf.keras.layers.Flatten()(input)</p> <p>#Basically there are four dense layer each followed by an dropout layer x= tf.keras.layers.Dense(units=556, activation=’relu’)(x) x= tf.keras.layers.Dropout(0.2)(x)</p> <p>x= tf.keras.layers.Dense(units=228, activation=’relu’)(x) x= tf.keras.layers.Dropout(0.2)(x)</p> <p>x= tf.keras.layers.Dense(units=128, activation=’relu’)(x) x= tf.keras.layers.Dropout(0.2)(x)</p> <p>x= tf.keras.layers.Dense(units=64, activation=’relu’)(x) x= tf.keras.layers.Dropout(0.2)(x)</p> <p>output= tf.keras.layers.Dense(units=1)(x) model= tf.keras.Model(inputs= input,outputs= output)<br/> return model</p> <p><img src="/assets/img/from_other/desce_code.png#center," alt="Architecture" title="Architecture"/></p> <h5 id="a-hyperparameter-tuning">a. Hyperparameter tuning</h5> <p>One of the most important hyperparameter for stock price prediction is the number of days that the model sees to make future prediction ie input_width. This hyperparameter value is calculated by training the model on different number of input width and the model which produces lowest loss it is selected.</p> <p><img src="/assets/img/from_other/hyperparameter_for_dense_model.png#center" alt="Input width vs Mean square Error(MSE)" title="Input width vs Mean square Error(MSE)"/></p> <p>We trained the model on input width [3,5,8,15,18,22,25,28,31] and among them the minimum value of MSE was obtained with the 3. So, the input width for the dense model is selected as 3.</p> <p><br/></p> <h5 id="b-evaluation">b. Evaluation</h5> <p>At input width 3, the label and prediction for Close value of stock look like this: <img src="/assets/img/from_other/Dense_model_ground_truth_vs_prediction.png#center" alt="label vs prediction" title="label vs prediction"/></p> <p><br/></p> <h4 id="12-lstm-model">1.2 LSTM model</h4> <p>A Recurrent Neural Network (RNN) is a type of neural network well-suited to time series data. RNNs process a time series step-by-step, maintaining an internal state from time step to time step.Let’s see understand how RNN will process time series data:</p> <p><img src="/assets/img/from_other/RNN.png#center" alt="LSTM modelS" title="LSTM model"/></p> <p>Here the RNN/LSTM is trained on every single input step, as a result, it makes the model more robust to changing landscape which is common in the stock dataset. It will take stock prices[Open, Close, High, Low] as input for the first day and predict the Close value for the second day. Similarly, a second time stamp will take the feature vector generated from the first time stamp and second days inputs to predict the 3rd step value and so on until it predicts one step into the future.</p> <p>def lstm_model(input_shape):</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>inp= Input(shape=input_shape) #BATCH,TIMESTAMP,FEATURES
x= tf.keras.layers.LSTM(128,return_sequences=False,name= 'LSTM')(inp)#batch,timestamp,32
x= Dense(units=256, activation='relu',name= 'Dense1')(x)
x=Dense(units=64, activation='relu',name= 'Dense2')(x)

x=Dense(units=32, activation='relu',name= 'Dense3')(x)
out= Dense(units=1)(x)
model= Model(inp,out)
return model
</code></pre></div></div> <p><img src="/assets/img/from_other/lstm_code.png#center" alt="Architecture" title="Architecture"/></p> <h5 id="a-evaluation">a. Evaluation</h5> <p>The minimum value of loss was obtained at input width 11 and its MSE value is similar dense model. Let’s look it label and prediction plot: <img src="/assets/img/from_other/rnn_ground_truth_vs_prediction.png#center" alt="label vs prediction" title="label vs prediction"/></p> <p><br/></p> <h3 id="2-multi-step-model">2 Multi-step model</h3> <p>In a multi-step prediction, the model needs to learn to predict a range of future values. Thus, unlike a single-step model, where only a single future point is predicted, a multi-step model predicts a sequence of the future values. There are two rough approaches to this:</p> <p>2.1. Single-shot Model Single-shot Model makes prediction of the entire time series at once. It is a time machine that can jump to any day into the future.</p> <p>2.2. Autoregressive predictions where the model only makes single-step predictions and its output is fed back as its input. We can see it as a time machine that can’t directly jump to any future date, instead, it had to go through each of the previous dates until it reaches the required future date.</p> <p><img src="/assets/img/from_other/autoregressive.png#center" alt="Autoregressive Model" title="Autoregressive Model"/></p> <p>For example, a person is living in 2012 who wants to go to 2022, if he used its single-shot time machine he can directly go to the year 2022 but as the machine doesn’t have any information about the jumped years its prediction events may be different from the actual events. But on the other hand, if he used its autoregressive time machine, the time machine will take him to the year 2013 and then 2014 until he reaches the year 2022, therefore the machine learns information about the intermediate year also which helps to improve the prediction significantly.</p> <p>class denseLayers(tf.keras.layers.Layer): def <strong>init</strong>(self): super().<strong>init</strong>() self.dense1= layers.Dense(256) self.dense2= layers.Dense(128) self.dense3= layers.Dense(32) self.dense4= layers.Dense(1)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def call(self,inputs):
    x= self.dense1(inputs)
    x= layers.Dropout(0.1)(x)
    
    x= self.dense2(x)
    x= layers.Dropout(0.1)(x)
    
    x= self.dense3(x)
    x= layers.Dropout(0.1)(x)
    
    x= self.dense4(x)
    return x
</code></pre></div></div> <p>def AutoRegressive_func():</p> <p>class AutoRegressive(tf.keras.Model): def <strong>init</strong>(self, units,output_steps): super().<strong>init</strong>() self.unit= units self.out_step= output_steps self.lstm_cell= layers.LSTMCell(self.unit)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      self.lstm_layer= layers.LSTM(128,return_state=True)

      self.dense_layer= denseLayers()

      
  def call(self,inputs,training= True):
      '''
      input= [batch,timestamp,features]
      '''
      predictions= []
      output,state_h,state_c= self.lstm_layer(inputs) 
      #output=[batch,units], similarly output= state_h

      state= [state_h,state_c] # The state
  
      prediction= self.dense_layer(output) 
      predictions.append(prediction)
      
      #Now iterating through the every step
      for i in range(self.out_step-1):
          output,state= self.lstm_cell(output,state,training)
          prediction= self.dense_layer(output) #Prediction= [batch,1] As we are outputting "Close" value at every time stamp
          predictions.append(prediction)
          

      # predictions.shape =&gt; (time, batch, features)
      predictions = tf.stack(predictions)
      
      # predictions.shape =&gt; (batch, time, features)
      predictions = tf.transpose(predictions, [1, 0, 2])
      return predictions   return AutoRegressive
</code></pre></div></div> <p>In the code, we can see we send the input to an LSTM layer which produces an output and state of the last LSTM cell. These output and state vectors are sent to an LSTM cell to forecast the price for a single day. Similarly, the next LSTM cell executes the previous cell output as input, and the state of the previous cell gets initialized as its initial state. It goes on until we predicted the whole range of output.</p> <p><br/></p> <h4 id="a-hyperparamter-tuning">a. Hyperparamter Tuning</h4> <p>In previous single step model, the minimum value of MSE was obtain when the input width is small but interesting in autoregressive model as the input width increase the MSE reduces. Therefore, the model performs the best when it’s looking large number of previous date data.</p> <p><img src="/assets/img/from_other/hyperparameter_for_autoregressiv_model.png#center" alt="Input width vs Mean square Error(MSE)"/></p> <p>We can see the model performs the best when the input width is 31.</p> <p><br/></p> <h4 id="b-evaluation-1">b. Evaluation</h4> <p><img src="/assets/img/from_other/autoregressive_prediction.png#center" alt="Plotting Close value for consecutive days"/></p> <p>The model is taking a consecutive input of the past 31 days and it is predicting the next 3 days. The difference between the actual price and the predicted price is not much. Therefore depending upon the task we can select an appropriate model and do the task.</p>]]></content><author><name></name></author><category term="Machine-learning"/><category term="project"/><summary type="html"><![CDATA[In statistical terms, time series forecasting is the process of analyzing the time series data using statistics and modeling to make predictions and informed strategic decisions. It falls under Quantitative Forecasting. Examples of Time Series Forecasting are weather forecast over next week, forecasting the closing price of a stock each day etc. In this article we will see different types of models which can be used for this analysis and pick what is best for what situations.]]></summary></entry><entry><title type="html">Single Image Super Resolution</title><link href="https://badal11.github.io/project/2023/single_image_super_resolution/" rel="alternate" type="text/html" title="Single Image Super Resolution"/><published>2023-01-12T13:56:00+00:00</published><updated>2023-01-12T13:56:00+00:00</updated><id>https://badal11.github.io/project/2023/single_image_super_resolution</id><content type="html" xml:base="https://badal11.github.io/project/2023/single_image_super_resolution/"><![CDATA[<p><a href="https://arxiv.org/abs/1406.2661">GAN</a> (Generating Adversarial Network) is about creating, like drawing but completely from scratch. It has got two models: the Generator and the Discriminator are put together into a game of adversary. Through which they learn the intricate details of the target data distribution. These two models are playing a MIN-MAX game where one tries to minimize the loss and the other tries to maximize. While doing so a global optimum is reached, where the Discriminator is no longer able to distinguish between real and generated (fake) data distribution.</p> <p>SISR(Single Image Super-Resolution) is an application of GAN. Image super-resolution is the process of enlarging small photos while maintaining a high level of quality, or of restoring high-resolution images from low-resolution photographs with rich information. Here the model’s work is to map the function from low-resolution image data to its high-resolution image. Instead of giving a random noise to the Generator, a low-resolution image is fed into it. After passing through various Convolutional Layers and Upsampling Layers, the Generator gives a high-resolution image output. Generally, there are multiple solutions to this problem, so it’s quite difficult to master the output up to original images in terms of richness and quality.</p> <p><br/></p> <h2 id="data-preprocessing-and-augmentation">Data Preprocessing and Augmentation:</h2> <p>We have used the <a href="https://data.vision.ee.ethz.ch/cvl/DIV2K/">DIV2K</a> [Agustsson and Timofte (2017)] dataset provided by the TensorFlow library. There are altogether 800 pairs of low resolution and high-resolution images in the the training set whereas 100 pairs in the testing set. This data contains mainly people, cities, fauna, sceneries, etc. We used flipping and rotating through 90, 180, and 270 degrees randomly over the dataset. Since we had limited memory on the training computer, we had to split large images into patches of smaller size. 19 patches of size 96 ✕ 96 pixels resolution were obtained from an image randomly. Our generator is designed to upsample images by 4 times so, the output image patch will be of dimension: 384 ✕ 384 pixels.</p> <p><br/></p> <h2 id="generator">Generator</h2> <p>The generator is the block in the architecture which is responsible for generating the high resolution(HR) images from low resolution(LR) images. In 2015, <a href="https://arxiv.org/abs/1609.04802">SRGAN</a> was published which introduced the concept of using GAN for SISR tasks which produced the state the art solution. The generator of <a href="https://arxiv.org/abs/1609.04802">SRGAN</a> consists of several residual blocks that facilitate the flow of the gradient during backpropagation.</p> <p><img src="/assets/img/from_other/generator.png#center" alt="Generator Architecture"/></p> <p>To further enhance the quality of generator images <a href="https://arxiv.org/abs/1809.00219">ESRGAN</a> was released which performed some modifications in the generator of the <a href="https://arxiv.org/abs/1609.04802">SRGAN</a> which includes:</p> <ul> <li>Removing the batch normalized(BN) layers.</li> <li>Replacing the original residual block with the proposed Residual-in-Residual Dense Block (RRDB), which combines multi-level residual network and dense connections as in the figure below:</li> </ul> <p><img src="/assets/img/from_other/rrdb.png#center" alt="RRDB Diagram"/></p> <p>Fig: Residual in Residual Dense Block(RRDB)</p> <p>Removing BN layers has proven to increase performance and reduce the computational complexity in different PSNR-oriented tasks including SR and deblurring. BN layers normalize the features using mean and variance in a batch during training and use the estimated mean and variance of the whole training dataset during testing. When the statistics of training and testing datasets differ a lot, BN layers tend to introduce unpleasant artifacts and limit the generalization ability. The researchers empirically observe that BN layers are more likely to bring artifacts when the network is deeper and trained under a GAN framework. These artifacts occasionally appear among iterations and different settings, violating the need for stable performance overtraining. Therefore, removing BN layers for stable training and consistent performance. Furthermore, removing BN layers helps to improve generalization ability and to reduce computational complexity and memory usage.</p> <p>RRDB employs a deeper and more complex structure than the original residual block in SRGAN. Specifically, as shown in the figure above, the proposed RRDB has a residual-in-residual structure, where residual learning is used at different levels. Here, the RRDB uses dense block in the main path, where the network capacity becomes higher benefiting from the dense connections. In addition to the improved architecture, it also exploits several techniques to facilitate training a very deep network such as residual scaling(beta) i.e., scaling down the residuals by multiplying a constant between 0 and 1 before adding them to the main path to prevent instability.</p> <p><br/></p> <h2 id="discriminator">Discriminator</h2> <p>The task of the discriminator is to discriminate between real HR images and generated SR images. Discriminator architecture here used is similar to DC-GAN architecture with LeakyReLU activation function. The network contains eight convolutional layers with 3×3 filter kernels, increasing by a factor of 2 from 64 to 512 kernels as in the VGG network. Strided convolutions are used to reduce the image resolution each time the number of features is doubled. But to overcome the instability while training of original GAN, we use a variant of GANs named improved training of Wasserstein GANs (WGAN-GP). So the last sigmoid layer of the conventional DC-GAN discriminator is omitted. This helps in not restricting the feature maps in 0 to 1 value.</p> <p><img src="/assets/img/from_other/discriminator.png#center" alt="discriminator image"/></p> <h2 id="losses">Losses:</h2> <h3 id="generator-loss">Generator Loss</h3> <p>The generator loss is the sum of MSE, perceptual loss +adversarial loss</p> <p><em>l<sub>G</sub> = MSE+Perceptual Loss +Adversarial loss</em></p> <p><em>l<sub>G</sub>= l<sub>MSE</sub>+l<sub>p</sub>+ l<sub>GA</sub></em></p> <p><br/></p> <h3 id="mean-square-errormse">Mean Square Error(MSE)</h3> <p>As the most common optimization objective for SISR, the pixelwise MSE loss is calculated as:</p> <table> <tbody> <tr> <td>_l<sub>MSE</sub> =</td> <td> </td> <td>G<sub>Θ</sub>(I<sub>LR</sub>) - I<sub>HR</sub></td> <td> </td> <td><sub>2</sub><sup>2</sup>_,</td> </tr> </tbody> </table> <p>where the parameter of the generator is denoted by ; the generated image, namely I<sub>SR</sub>,is denoted by G<sub>Θ</sub>(I<sub>LR</sub>); and the ground truth is denoted by I<sub>HR</sub> . Although models with MSE loss favor a high PSNR value, the generated results tend to be perceptually unsatisfying with overly smooth textures. Despite the aforementioned shortcomings, this loss term is still kept because MSE has clear physical meaning and helps to maintain color stability.</p> <p><br/></p> <h3 id="perceptual-loss">Perceptual Loss</h3> <p>To compensate for the shortcomings of MSE loss and allow the loss function to better measure semantic and perceptual differences between images, we define and optimize a perceptual loss based on high-level features extracted from a pretrained network. The rationality of this loss term lies in that the pretrained network for classification originally has learned to encode the semantic and perceptual information that may be measured in the loss function. To enhance the performance of the perceptual loss, a 19-layer VGG network is used. The perceptual loss is actually the Euclidean distance between feature representations, which is defined as</p> <table> <tbody> <tr> <td>_l<sub>p</sub> =</td> <td> </td> <td>𝜙(G<sub>Θ</sub>(I<sub>LR</sub>)) - 𝜙(I<sub>HR</sub>)</td> <td> </td> <td><sub>2</sub><sup>2</sup>_,</td> </tr> </tbody> </table> <p>where 𝜙 refers to the 19-layer VGG network. With this loss term, I<sub>SR</sub> and I<sub>HR</sub> are encouraged to have similar feature representations rather than to exactly match with each other in a pixel wise manner.</p> <p><br/></p> <h3 id="adversarial-losses">Adversarial Losses:</h3> <p>In <a href="https://arxiv.org/abs/1609.04802">SRGAN</a>, the adopted generative model is generative adversarial network (GAN) and it suffers from training instability. WGAN leverages the Wasserstein distance to produce a value function, which has better theoretical properties than the original GAN. However, WGAN requires that the discriminator must lie within the space of 1-Lipschitz through weight clipping, resulting in either vanishing or exploding gradients without careful tuning of the clipping threshold. <br/> To overcome the flaw of clipping , a new approach is applied called Gradient Pelanty method. It is used to enforce the Lipschitz constraint. This way Wasserstein distance between two distributions to help decide when to stop the training but penalizes the gradient of the discriminator with respect to its input instead of weight clipping. With gradient penalty, the discriminator is encouraged to learn smoother decision boundaries.</p> <p><br/></p> <h3 id="generator-loss-1">Generator Loss</h3> <p><em>l<sub>GA</sub>=-𝔼[D(G<sub>Θ</sub>(I<sub>LR</sub>)]</em></p> <p><br/></p> <h3 id="discriminator-loss">Discriminator Loss</h3> <table> <tbody> <tr> <td>_l<sub>DA</sub>=𝔼[D(G<sub>Θ</sub>(I<sub>LR</sub>)]-𝔼[D(I<sub>HR</sub>)] + λ𝔼(</td> <td> </td> <td>▽<sub>hat{I}</sub>D(hat{I})-1</td> <td> </td> <td><sub>2</sub>-1)<sup>2</sup>_</td> </tr> </tbody> </table> <p><img src="/assets/img/from_other/work_flow.png#center" alt="workflow diagram"/></p> <p>Normally, the output of the classifier i.e. discriminator in this case is kept between 0-1 using a sigmoid function in the last layer, where if discriminator prediction 0 for an image then the image is SR likewise if the prediction is 1 then it is an HR image. Here the discriminator is trained using WGAN-GP approach <a href="https://sulavtimilsina.github.io/posts/wgan-gp/">(described here)</a>, hence the output is not bounded between 0-1 instead the discriminator will try to maximize the distance between the prediction of SR image and HR image and generator will try to minimize it. Let’s look at the loss of the generator ie. I<sub>GA</sub> and the loss of discriminator I<sub>DA</sub> .</p> <p><br/></p> <h3 id="understanding-discriminator-adversarial-loss">UNDERSTANDING DISCRIMINATOR ADVERSARIAL LOSS</h3> <p>(not considering the gradient penalty term for making it easier to understand)</p> <p><em>l<sub>DA</sub>=𝔼[D(G<sub>Θ</sub>(I<sub>LR</sub>)]-𝔼[D(I<sub>HR</sub>)]</em></p> <p>(Note: <em>l<sub>DA</sub>= 𝔼[D(I<sub>HR</sub>)]-𝔼[D(G<sub>Θ</sub>(I<sub>LR</sub>)]</em> if the loss of the discriminator is in this form than the discriminator will try to maximize this equation and the generator will try to minimize the I<sub>GA</sub>).</p> <p>Considering D(G<sub>Θ</sub>(I<sub>LR</sub>))= 5 and D(I<sub>HR</sub>) = 5 initially when the discriminator doesn’t have the ability to differentiate between them.</p> <p>Therefore the loss at the very beginning: <em>l<sub>DA</sub>=5-5= 0,</em></p> <p>The discriminator wants to minimize the loss l<sub>DA</sub>, hence increasing the distance between D(G<sub>Θ</sub>(I<sub>LR</sub>))and D(I<sub>HR</sub>) . Suppose after the update of the gradient of the discriminator for the few step, the value of prediction becomes D(G<sub>Θ</sub>(I<sub>LR</sub>))=-2 and D(I<sub>HR</sub>) = 2 ,therefore discriminator is learning to know the difference between the LR image and the HR image, hence making the loss(l<sub>DA</sub>)= -4, Here the loss is minimized and the distance between the two predictions is maximized.</p> <p><img src="/assets/img/from_other/understanding_disc_adv_loss.png#center" alt="discriminator adverserial loss"/></p> <p><br/></p> <h3 id="understanding-generator-adversarial-loss">UNDERSTANDING GENERATOR ADVERSARIAL LOSS</h3> <p>Discriminator is trained for a few steps and then the update of the generator happens. Therefore the discriminator is kept a few steps ahead of the generator in terms of its learning. Let’s consider the discriminator has been trained for the few steps and it predicted outputs are:</p> <p><em>D(G<sub>Θ</sub>(I<sub>LR</sub>)) = -2</em> <em>D(I<sub>HR</sub>) = 2</em></p> <p>The loss of the generator is:</p> <p><em>l<sub>GA</sub> = -𝔼[D(G<sub>Θ</sub>(I<sub>LR</sub>)]</em></p> <p>Therefore, <em>l<sub>GA</sub>= -(-2) = 2</em></p> <p>Generator wants to minimize l<sub>GA</sub> , which can only we achieved by increasing the value of D(G<sub>Θ</sub>(I<sub>LR</sub>)) hence ultimately reducing the distance between D(G<sub>Θ</sub>(I<sub>LR</sub>)) and D(I<sub>HR</sub>) ,hence making the SR image and HR image identical as:</p> <p><em>l<sub>GA</sub>= -(large positive value) ≈ global minima</em></p> <p><img src="/assets/img/from_other/understanding_gen_adv_loss.png#center" alt="generator adverserial loss"/></p> <p><br/></p> <h3 id="result-and-conclusion">Result and Conclusion:</h3> <p>We chose Kaggle’s kernel with Tesla P100 GPU to train the model. Since it has 20 million training parameters, training it for 500 epochs is a tedious job. Until now we have trained it only up to 100 epochs. Following is the sample output of the 100th epoch. The rightmost image is Low-Resolution Patch, the Middle one is the High-Resolution Patch and the Left most one is the Generated High-Resolution Image.</p> <p><img src="/assets/img/from_other/output.png#center" alt="outputs"/></p>]]></content><author><name></name></author><category term="Deep_Learning"/><category term="project"/><summary type="html"><![CDATA[GAN (Generating Adversarial Network) is about creating, like drawing but completely from scratch. It has got two models: the Generator and the Discriminator are put together into a game of adversary. Through which they learn the intricate details of the target data distribution. These two models are playing a MIN-MAX game where one tries to minimize the loss and the other tries to maximize. While doing so a global optimum is reached, where the Discriminator is no longer able to distinguish between real and generated (fake) data distribution.]]></summary></entry><entry><title type="html">Nepali Student’s Insights</title><link href="https://badal11.github.io/project/2022/Nepali-Student's-Insights/" rel="alternate" type="text/html" title="Nepali Student’s Insights"/><published>2022-08-20T15:59:00+00:00</published><updated>2022-08-20T15:59:00+00:00</updated><id>https://badal11.github.io/project/2022/Nepali%20Student&apos;s%20Insights</id><content type="html" xml:base="https://badal11.github.io/project/2022/Nepali-Student&apos;s-Insights/"><![CDATA[<p>In my research journey to understand the significant challenges faced by primary school students in Nepal, I’ve outlined a step-by-step guide for conducting user research. Similarly, these are also the steps I myself followed.</p> <hr/> <hr/> <h1 id="user-research">User Research</h1> <p>To conduct user research with the objective of understanding the biggest pain points for primary school students in Nepal, you can follow these steps:</p> <ol> <li> <p>Define your target population: Clearly identify the students you want to target, such as grade 2 students, students from a specific school, or students from a particular area.</p> </li> <li> <p>Determine the research method: Decide on the method you will use to gather information, such as surveys, interviews, focus groups, or observation.</p> </li> <li> <p>Develop the survey or interview questions: Based on your objective, create questions that will help you gather the information you need. Make sure the questions are clear, easy to understand, and relevant to the students’ experiences.</p> </li> <li> <p>Recruit participants: Find a way to reach the students you want to interview or survey. You can do this through their school, through parents, or through community organizations.</p> </li> <li> <p>Conduct the research: Administer the survey or conduct the interviews or focus groups, making sure to follow all ethical guidelines.</p> </li> <li> <p>Analyze the data: Once you have collected the information, analyze it to identify patterns and trends. Pay particular attention to any recurring themes or pain points that the students mention.</p> </li> <li> <p>Report the findings: Prepare a report that summarizes the results of your research, including the key findings, trends, and recommendations for addressing the biggest pain points for primary school students in Nepal.</p> </li> </ol> <p>Share the findings: Share the findings with stakeholders, such as teachers, parents, and educational organizations, so that they can use the information to improve the learning experiences of primary school students in Nepal.</p> <hr/> <hr/> <h1 id="now-i-go-into-detail-for-each-of-the-above-steps">Now I go into detail for each of the above steps:</h1> <h2 id="1-target-population-sample">1. Target Population Sample</h2> <p>When defining a target population sample for a research objective, it is important to consider the following points:</p> <ol> <li> <p>Age: What is the age range of the primary school students in Nepal?</p> </li> <li> <p>Gender: Are you targeting both male and female students or just one gender?</p> </li> <li> <p>Location: Are you targeting students from a specific region or all regions of Nepal?</p> </li> <li> <p>Socio-economic status: Are you targeting students from a specific socio-economic background? For kids, it can include factors such as the education level and income of their parents, the neighborhood (urban, suburban, rural, gated(sanchay kosh, deep housing), low-income)  they live in, and the resources available to them.</p> </li> <li> <p>School type: Are you targeting students from government or private schools or both?</p> </li> <li> <p>Level of education: Are you targeting students from specific grades or levels of education?</p> </li> <li> <p>Academic performance: Are you targeting students based on their academic skills and abilities? Here are some of the categories based on academic performance: High-achieving, Average-performing, Struggling, At risk students.</p> </li> <li> <p>Language proficiency: Are you targeting students who are proficient in both English and Nepali, or just one language?</p> </li> <li> <p>Learning style: Are you targeting students with specific learning styles, such as visual or auditory learners?</p> </li> <li> <p>Technology usage: Are you targeting students who have access to technology and are comfortable using it for learning purposes?</p> </li> <li> <p>Parental involvement: Are you targeting students who have involved parents or guardians who can support their learning?</p> </li> </ol> <p>By considering these points, you can define a specific target population sample for your research objective and ensure that you are reaching the right audience for your study.</p> <p>When considering socio-economic status, it is important to consider the following factors:</p> <ol> <li> <p>Income: This includes the household income, as well as the individual income of parents or guardians.</p> </li> <li> <p>Education level: This refers to the highest level of education attained by the parents or guardians.</p> </li> <li> <p>Occupation: This refers to the type of job held by the parents or guardians, as well as their level of job security and stability.</p> </li> <li> <p>Housing type and condition: This refers to the type of dwelling in which the family resides, as well as its overall condition and amenities.</p> </li> <li> <p>Family structure: This refers to the composition of the family, including the number of parents, children, and other relatives living together.</p> </li> <li> <p>Neighborhood characteristics: This includes the physical and social characteristics of the neighborhood, such as its level of crime, poverty, and access to resources and services.</p> </li> <li> <p>Social and cultural values: This refers to the values, beliefs, and attitudes held by the family and community, which can influence the educational and life opportunities of children.</p> </li> </ol> <hr/> <hr/> <h2 id="2-research-methods">2. Research Methods</h2> <p>To determine the research method for your objective of understanding the biggest pain points for primary school students in Nepal, you will need to consider the following factors:</p> <ol> <li> <p>Research design: Determine whether you want to conduct qualitative or quantitative research, or a combination of both.</p> </li> <li> <p>Sampling method: Decide on the method for selecting your sample of primary school students in Nepal. This can be either a probability or non-probability sampling method.</p> </li> <li> <p>Data collection tools: Choose the tools that you will use to collect data, such as surveys, interviews, focus groups, observation, or existing data sources.</p> </li> <li> <p>Data analysis methods: Determine the methods you will use to analyze your data, such as statistical analysis, content analysis, or thematic analysis.</p> </li> </ol> <p>Based on these factors, you can determine the best research method for your objective. For example, if you want to gather in-depth information about the pain points of primary school students in Nepal, you may choose to conduct qualitative research using methods such as in-depth interviews or focus groups. On the other hand, if you want to gather numerical data about the pain points of a large number of primary school students, you may choose to conduct a survey using a quantitative research method.</p> <p>The sampling method you choose for your research will depend on your research objectives, target population, and available resources. There are several types of sampling methods that can be used, including:</p> <ol> <li> <p>Simple Random Sampling: This is a method where each member of the target population has an equal chance of being selected for the sample. For example, if you want to select 100 students from a population of 1000 students, you can use a random number generator to choose 100 unique numbers that correspond to the students’ ID numbers.</p> </li> <li> <p>Stratified Sampling: This is a method where the target population is divided into subgroups (strata) based on certain characteristics (such as age, gender, socio-economic status, etc.), and then a sample is selected from each stratum. For example, if you want to study the pain points of primary school students in Nepal, you could divide the target population into subgroups based on their grade level and select a sample from each grade level.</p> </li> <li> <p>Cluster Sampling: This is a method where the target population is divided into clusters (such as schools, neighborhoods, or villages), and then a sample of clusters is selected. For example, if you want to study the pain points of primary school students in Nepal, you could divide the target population into clusters based on the geographic region they live in, and then select a sample of schools from each region.</p> </li> <li> <p>Convenience Sampling: This is a method where the sample is selected based on convenience or accessibility, rather than random selection. For example, if you want to study the pain points of primary school students in Nepal, you could conduct a survey at a school near your home or office.</p> </li> </ol> <p>These are some of the commonly used sampling methods, but the method you choose will depend on your research objectives, resources, and the target population. The available resources can greatly impact the choice of sampling method for a research study. For example, if the resources are limited, such as time, budget, and manpower, then a convenient or a non-probability sampling method may be preferred. On the other hand, if the resources are abundant, then a more rigorous and representative sampling method, such as a random or a stratified sampling method, can be employed.</p> <p>The research design method for your objective of understanding the biggest pain points for primary school students in Nepal could be a qualitative research design. Qualitative research allows for a deep and rich understanding of a topic and can be useful in exploring and understanding complex experiences, perspectives, and attitudes.</p> <p>Some examples of qualitative research designs for this objective could include:</p> <p>Focus Group Discussions (FGD) - This involves gathering a small group of primary school students together and facilitating a discussion on their experiences and challenges in school. This could provide valuable insights into their pain points.</p> <p>In-Depth Interviews (IDI) - This involves conducting one-on-one interviews with primary school students to explore their experiences, challenges, and perspectives on the issues they face in school. This can provide a more in-depth understanding of their pain points.</p> <p>Ethnography - This involves conducting observations and participating in the daily activities of primary school students to understand their experiences and perspectives. This can provide a rich and detailed understanding of their pain points.</p> <p>These are just a few examples of qualitative research designs that could be used to understand the biggest pain points for primary school students in Nepal. The exact research design would depend on various factors such as available resources, research objectives, and target population, among others.</p> <p>For the objective of understanding the biggest pain points for primary school students in Nepal, a qualitative research design, such as interviews or focus groups (need experience), might be more appropriate. This approach would allow researchers to gain in-depth insight into the experiences and perspectives of the students, and to identify the specific challenges and difficulties that they face.</p> <hr/> <hr/> <h2 id="3-interview">3. Interview</h2> <p>Conducting an interview as a research method involves the following steps:</p> <ol> <li> <p>Develop a list of questions: This should be based on your research objectives and the information you need to gather from your participants.</p> </li> <li> <p>Choose participants: Select participants who are representative of your target population.</p> </li> <li> <p>Schedule the interviews: You can either conduct face-to-face interviews, phone interviews or online interviews, depending on your accessibility to the participants and their availability.</p> </li> <li> <p>Prepare the environment: Make sure the environment is conducive to a good interview. It should be quiet and private, with a comfortable seat and adequate lighting.</p> </li> <li> <p>Start the interview: Start with a brief introduction and explain the purpose of the interview. Then proceed to ask the questions on your list. Listen actively and avoid interrupting the participants.</p> </li> <li> <p>End the interview: Close the interview by thanking the participants and asking if they have any further questions or comments.</p> </li> <li> <p>Transcribe the interview: Take notes or record the interview for later transcription.</p> </li> <li> <p>Analyze the data: Analyze the data from the interviews and look for patterns, themes or insights that can help you understand the biggest pain points for primary school students in Nepal.</p> </li> </ol> <p>It’s important to keep in mind that conducting interviews as a research method requires good communication and interpersonal skills, as well as respect for participants and their opinions.</p> <p>Here is a sample list of questions, for understanding the biggest pain points for primary school students in Nepal:</p> <ol> <li> <p>Can you tell me about your school and what grade are you in?</p> </li> <li> <p>What do you like most about school?</p> </li> <li> <p>What subjects do you like the most and why?</p> </li> <li> <p>What do you find challenging about school?</p> </li> <li> <p>Have you ever felt frustrated or upset at school? Can you tell me about it?</p> </li> <li> <p>How do you feel about your teachers and classmates?</p> </li> <li> <p>Do you have enough materials and resources to learn at school?</p> </li> <li> <p>Do you feel like you receive enough attention and help from your teachers?</p> </li> <li> <p>What changes would you like to see in your school to make your learning experience better?</p> </li> </ol> <p>Here are some interview questions that you could ask parents in a one-on-one conversation:</p> <ol> <li> <p>Can you tell me about your child’s school experience so far?</p> </li> <li> <p>How does your child feel about going to school?</p> </li> <li> <p>How does your child feel about their teachers and classmates?</p> </li> <li> <p>Does your child have any challenges or difficulties at school that you are aware of?</p> </li> <li> <p>Does your child have access to the materials and resources they need to succeed in school?</p> </li> <li> <p>How does your child feel about the school environment and culture?</p> </li> <li> <p>Have you noticed any changes in your child’s behavior or attitudes towards school recently?</p> </li> <li> <p>Do you feel like your child is receiving enough attention and support from their teachers?</p> </li> <li> <p>Are there any changes that you would like to see in your child’s school to improve their experience?</p> </li> <li> <p>Is there anything else you would like to share about your child’s school experience?</p> </li> </ol> <p>It’s important to keep the questions open-ended and non-judgmental, and to allow the parents to share.</p> <p>Here are some interview questions you can use to gather information from teachers in a one-on-one conversation:</p> <ol> <li> <p>Can you tell me about your experience teaching in primary schools?</p> </li> <li> <p>How do you approach teaching and creating lesson plans for primary school students?</p> </li> <li> <p>In your experience, what are the common challenges faced by primary school students in Nepal?</p> </li> <li> <p>How do you address the diverse needs and abilities of your students in the classroom?</p> </li> <li> <p>How do you create a positive and inclusive classroom environment?</p> </li> <li> <p>How do you support students who are struggling with their studies?</p> </li> <li> <p>Can you share any successful strategies or initiatives you have implemented in your classroom?</p> </li> <li> <p>How do you involve parents and the community in their child’s education?</p> </li> <li> <p>What changes would you like to see in the education system to better support primary school students in Nepal?</p> </li> <li> <p>Can you share any examples of primary school students who have overcome challenges and achieved success in your classroom?</p> </li> </ol> <p>Note: These questions are just a starting point, and you may need to modify them based on your specific research objectives and the information you hope to gather from the teachers.heir experiences and perspectives in their own words.</p> <hr/> <hr/> <h2 id="4-recruit">4. Recruit</h2> <p>To approach a parents for conducting an interview with their children, you need to take the following steps:</p> <ol> <li> <p>Introduce yourself: Start by introducing yourself, your organization, and the purpose of the research. Explain that the research is focused on understanding the biggest pain points for primary school students in Nepal.</p> </li> <li> <p>Explain the interview process: Explain the interview process to the parents, including the type of questions that will be asked and the duration of the interview. Assure them that the questions are designed to be kid-friendly and will not cause any harm to the child.</p> </li> <li> <p>Get their consent: Ensure that the parents understand the purpose of the research and are comfortable with their child participating in the interview. Obtain written or verbal consent from the parents.</p> </li> <li> <p>Schedule the interview: Schedule the interview with the parents at a convenient time and place for both parties.</p> </li> <li> <p>Prepare the child: Before the interview, prepare the child by explaining the purpose of the interview and what to expect. Let the child know that they can ask for clarification or take a break if needed.</p> </li> <li> <p>Thank the parents: After the interview, thank the parents for their time and cooperation. Provide them with a summary of the findings, if they are interested.</p> </li> <li> <p>Ensure confidentiality: Ensure that the data collected during the interview is confidential and protected. Let the parents know that their child’s responses will not be shared with anyone outside of the research team.nt for the kids to express their thoughts and opinions.</p> </li> </ol> <p>To approach a school for conducting an interview with their students, you need to take the following steps:</p> <ol> <li> <p>Introduction: Start by introducing yourself, your research objectives and the purpose of the interview. Explain how the information collected will be used and the benefits of participating.</p> </li> <li> <p>Get Permission: Ask the school administration for permission to conduct the interview with their students. Provide them with a detailed explanation of the research process, the questions you will be asking, and how the information collected will be kept confidential and anonymous.</p> </li> <li> <p>Secure a Schedule: Schedule a convenient time for the interview that does not interfere with the school’s routine activities.</p> </li> <li> <p>Prepare the Students: Let the students know about the interview in advance so that they can be mentally prepared. Explain to them the purpose of the interview and what they can expect.</p> </li> <li> <p>Provide Consent Forms: Provide parents or guardians of the students with a consent form that they can sign to give their permission for their child to participate in the interview.</p> </li> <li> <p>Provide Reassurance: Reassure the school and the students that the interview process is safe and confidential. Explain that all the information collected will be kept anonymous, and no personal information will be shared.</p> </li> <li> <p>Respect the School’s Guidelines: Follow the school’s guidelines and regulations for conducting research activities in their premises.</p> </li> </ol> <p>By following these steps, you can approach a school for conducting an interview with their students. Remember to be respectful and professional at all times, and to build trust with the school and the students.</p> <hr/> <hr/> <h1 id="interview-scripts">Interview Scripts</h1> <h2 id="scripts-for-parents">Scripts for parents</h2> <p>Introduction:</p> <p>Hi, my name is [Your Name], and I am a researcher who is interested in understanding the experiences and perspectives of primary school students in Nepal. As part of my research, I would like to interview some students and gather their thoughts and opinions on their school experiences. I understand that your child [Child’s Name] attends [School Name], and I would be grateful if they could participate in the interview.</p> <p>Purpose of the Interview:</p> <p>I am conducting this research to gain a better understanding of the challenges that primary school students face and how they feel about their school experience. The data collected will be used to improve the quality of education in Nepal and make schools more engaging and supportive for students.</p> <p>Length of the Interview: The interview should take approximately 20-30 minutes and will be conducted either in person or over the phone, depending on your preference.</p> <p>Confidentiality: All information collected during the interview will be kept confidential, and only anonymous and aggregated data will be used in any reports or publications.</p> <p>Consent Form: I will provide you with a consent form that outlines the purpose of the study and the rights of the participants. Please take some time to review the form and let me know if you have any questions.</p> <p>Incentives: We would like to offer a small token of appreciation for your time and effort in participating in the interview. This could be in the form of a small gift, such as a notebook or pen.</p> <p>Closing: Thank you for taking the time to consider participating in this study. I understand that it may not be possible for everyone to participate, and I appreciate your time and consideration. If you are interested in participating, please let me know, and I will arrange for a convenient time for the interview.</p> <p>Please don’t hesitate to contact me if you have any questions or concerns.</p> <p>Thank you,</p> <p>[Your Name]</p> <hr/> <hr/> <h2 id="scripts-for-school">Scripts for school</h2> <p>Introduction: Hello, my name is [Your Name], and I am a researcher interested in understanding the experiences of primary school students in Nepal. I am reaching out to you today because I believe your school would be an excellent location for me to gather this information.</p> <p>Objective: My objective is to gather information about the biggest pain points that primary school students in Nepal are facing, so that we can better understand their needs and improve the quality of education for all students.</p> <p>Methodology: I plan to conduct a series of student interviews at your school, in which I will ask students a series of questions about their school experiences. I believe that by conducting these interviews, we can gain valuable insights into the challenges that students are facing, and what can be done to improve the quality of education for all students.</p> <p>Request for Cooperation: I would like to request your cooperation in arranging a time for me to conduct these student interviews at your school. I understand that you are busy, but I believe that this research is important, and I believe that your students would have valuable insights to share. I would be grateful if you could arrange a time for me to come to your school and conduct these interviews.</p> <p>Benefit for the School: I believe that by participating in this research, your school will be helping to contribute to the larger goal of improving education for all students in Nepal. Additionally, by gaining insights into the experiences of your students, you may be able to identify areas where you can improve the quality of education at your school.</p> <p>Thank you for your time and consideration. I look forward to the opportunity to work with your school on this important research project.</p> <hr/> <hr/> <h2 id="short-and-sweet-script-for-a-school">Short and sweet script for a school</h2> <p>Introduction:</p> <ul> <li>Hi, my name is [Your Name], and I am a researcher.</li> <li>I am interested in understanding the experiences and perspectives of primary school students in Nepal.</li> </ul> <p>Purpose of the Study:</p> <ul> <li>I would like to conduct a research study to understand the biggest pain points for primary school students in Nepal.</li> </ul> <p>Request for Participation:</p> <ul> <li>I was wondering if it would be possible to conduct a series of one-on-one interviews with a few of your students as part of my research study.</li> <li>The interview will last for about 30 minutes and will be conducted in a confidential and secure manner.</li> </ul> <p>Benefits of Participation:</p> <ul> <li>By participating in this study, you will be helping to provide valuable insights and recommendations to improve the education system in Nepal.</li> </ul> <p>Conclusion:</p> <ul> <li>I would be grateful if you could consider my request and let me know if this is something that you would be willing to support.</li> <li>I can provide you with more information and answer any questions that you may have.</li> </ul> <p>Thank you for your time and consideration.</p> <hr/> <hr/> <h1 id="template-for-consent">Template for consent</h1> <h2 id="sample-template-for-consent">Sample template for consent</h2> <p>Here is a sample template for a consent paper that you can use for your purposes:</p> <p>Title: Consent for Participation in [Name of Research or Study]</p> <p>Introduction:</p> <p>The purpose of this consent form is to provide you with information about a research study [Name of Research or Study]. Your participation in this study is entirely voluntary and you are free to withdraw at any time without penalty.</p> <p>Study Purpose: The purpose of this study is to [state the purpose of the study].</p> <p>Procedures: During this study, you will be asked to [state what the participant will be asked to do in the study]. The total time commitment for this study is [state the estimated time commitment].</p> <p>Benefits and Risks: By participating in this study, you may [state the potential benefits of participation]. There are no direct benefits to participating in this study.</p> <p>Confidentiality: The information that you provide in this study will be kept confidential to the extent permitted by law. [State any limitations to confidentiality].</p> <p>Compensation: You will not receive any compensation for your participation in this study.</p> <p>Voluntary Participation: Your participation in this study is entirely voluntary. You are free to withdraw from the study at any time without penalty.</p> <p>Contact Information: If you have any questions about this study, you may contact [Name of Researcher and Contact Information].</p> <p>Signature:<strong>__</strong><strong>__</strong><strong>__</strong>___</p> <p>I have read and understand the information provided above. I voluntarily agree to participate in this study.</p> <p>[Your Name]</p> <p>[Date]</p> <p>Note: This is just a sample template and may need to be modified based on the specific requirements of your study.</p> <hr/> <hr/> <h2 id="example-of-a-consent-paper">Example of a consent paper</h2> <p>Here is an example of a consent paper that could be used when conducting interviews with students in a school setting:</p> <p>Title: Student Interview Consent Form</p> <p>Purpose of the Study:</p> <p>The purpose of this study is to gather information about the experiences and perspectives of primary school students, in order to better understand the challenges and difficulties they face in their education. The information collected will be used in our project aimed at improving education for students.</p> <p>Participation: Participation in the study is voluntary and participants can withdraw at any time. Participants will be asked to complete a short interview that will take approximately 20-30 minutes. The responses given during the interview will be confidential and anonymous.</p> <p>Resources Needed: The researchers will require the assistance of school staff and teachers in order to conduct the interviews. Access to school resources and data such as marksheets may also be required.</p> <p>Benefits and Risks: There are no known risks associated with participating in the study. Participants will have the opportunity to positively impact education through their participation.</p> <p>Confidentiality: The researchers will maintain the confidentiality of participants’ personal information and responses, and personal information will not be used in reports or publications. Only the researchers will have access to the responses collected during the interviews.</p> <p>Compensation: There is no compensation for participating in the study.</p> <p>Contact Information: If you have any questions or would like a copy of the results, please do not hesitate to contact the researchers at [insert contact information for researchers].</p> <p>Ethical Guidelines: The study will follow ethical guidelines set by [insert relevant ethical organizations or guidelines]. Confidentiality and privacy of participants will be maintained, and informed consent will be obtained from all participants or their guardians.</p> <p>Consent: By signing below, I confirm that I have read and understand the information provided and agree to participate voluntarily in the study. I understand that I have the right to withdraw at any time.</p> <p>Signature: <strong>__</strong><strong>__</strong><strong>__</strong><strong>__</strong> I have read and understand the information provided above. I voluntarily agree to participate in this study.</p> <p>Date: <strong>__</strong><strong>__</strong><strong>__</strong><strong>__</strong>_</p> <hr/> <hr/> <h1 id="revised-interview-questions">Revised interview questions</h1> <p>Can you tell me about a day you spend at school?</p> <p>What makes it hard for you to understand what you learn in class?</p> <p>How do you feel when you have many things to do for school?</p> <p>Do you feel worried or upset about school sometimes? Can you share what happened?</p> <p>Who helps you when you need help in school?</p> <p>Do you feel like the things you learn in school are easy or hard for you?</p> <p>Do you have enough time to finish your school work?</p> <p>Is there a time in school when you feel like you don’t want to be there? Can you share about it?</p> <p>Is there anything you wish you could learn more about in school?</p> <p>How do you feel about your time at school in general?</p> <p>Can you tell me about your school and what grade are you in?</p> <p>What do you like most about school?</p> <p>What subjects do you like the most and why?</p> <p>What do you find challenging about school?</p> <p>Have you ever felt frustrated or upset at school? Can you tell me about it?</p> <p>How do you feel about your teachers and classmates?</p> <p>Do you have enough materials and resources to learn at school?</p> <p>Do you feel like you receive enough attention and help from your teachers?</p> <p>What changes would you like to see in your school to make your learning experience better?</p> <p>What are the challenges you face when you do your homework?</p> <p>Do you have a designated space to study at home?</p> <p>Are you able to focus on your studies while at home?</p> <p>Do you have access to the resources you need to complete your assignments (e.g. textbooks, internet, etc.)?</p> <p>Do you receive enough support from your parents or guardians when it comes to your studies?</p> <p>Do you have a set routine for doing your homework?</p> <p>Have you ever struggled with completing your homework because of distractions at home?</p> <p>How do you manage to balance other responsibilities (such as chores, hobbies, etc.) with your studies?</p> <p>Have you ever felt frustrated or stressed about your schoolwork while at home? Can you tell me about it?</p> <p>Are there any specific subjects or tasks that you find particularly difficult to complete at home?</p> <p>Do you feel like you have a good balance between your schoolwork, playtime, and other activities at home?</p> <hr/> <hr/> <h2 id="interview-questions-at-school">Interview Questions at School:</h2> <ol> <li> <p>Can you tell me about a day you spend at school? Good experience or bad experience</p> </li> <li> <p>How did you feel in those days?</p> </li> <li> <p>Can you tell me about your learning experience in class?</p> </li> <li> <p>Have you ever s</p> </li> <li> <p>What makes it hard for you to understand what you learn in class?</p> </li> <li> <p>How do you feel when you have many things to do for school?</p> </li> <li> <p>Do you feel worried or upset about school sometimes? Can you share what happened?</p> </li> <li> <p>Who helps you when you need help in school?</p> </li> <li> <p>Do you feel like the things you learn in school are easy or hard for you?</p> </li> <li> <p>Do you have enough time to finish your schoolwork?</p> </li> <li> <p>Is there a time in school when you feel like you don’t want to be there? Can you share about it?</p> </li> <li> <p>Is there anything you wish you could learn more about in school?</p> </li> <li> <p>How do you feel about your time at school in general?</p> </li> <li> <p>Can you tell me about your school and what grade are you in?</p> </li> <li> <p>What do you like most about school?</p> </li> <li> <p>What subjects do you like the most and why?</p> </li> <li> <p>What do you find challenging about school?</p> </li> <li> <p>Have you ever felt frustrated or upset at school? Can you tell me about it?</p> </li> <li> <p>How do you feel about your teachers and classmates?</p> </li> <li> <p>Do you have enough materials and resources to learn at school?</p> </li> <li> <p>Do you feel like you receive enough attention and help from your teachers?</p> </li> <li> <p>What changes would you like to see in your school to make your learning experience better?</p> </li> </ol> <hr/> <hr/> <h2 id="at-home">At Home:</h2> <ol> <li> <p>What are the challenges you face when you do your homework?</p> </li> <li> <p>Do you have a designated space to study at home?</p> </li> <li> <p>Are you able to focus on your studies while at home?</p> </li> <li> <p>Do you have access to the resources you need to complete your assignments (e.g. textbooks, internet, etc.)?</p> </li> <li> <p>Do you receive enough support from your parents or guardians when it comes to your studies?</p> </li> <li> <p>Do you have a set routine for doing your homework?</p> </li> <li> <p>Have you ever struggled with completing your homework because of distractions at home?</p> </li> <li> <p>How do you manage to balance other responsibilities (such as chores, hobbies, etc.) with your studies?</p> </li> <li> <p>Have you ever felt frustrated or stressed about your schoolwork while at home? Can you tell me about it?</p> </li> <li> <p>Are there any specific subjects or tasks that you find particularly difficult to complete at home?</p> </li> <li> <p>Do you feel like you have a good balance between your schoolwork, playtime, and other activities at home?</p> </li> <li> <p>Do you find it easy or difficult to get started on your homework?</p> </li> <li> <p>Do you have a specific time of day that you set aside for homework?</p> </li> <li> <p>Do you have any particular strategies for staying focused while doing your homework?</p> </li> <li> <p>Is there anything that makes it difficult for you to concentrate when doing your homework?</p> </li> <li> <p>Do you have any distractions at home that make it hard to do your homework?</p> </li> <li> <p>Do you feel like you have enough support from your family to do your best in school?</p> </li> <li> <p>Do you ever feel overwhelmed by the amount of homework you have to do?</p> </li> <li> <p>Do you have any particular tools or resources that you use to help you with your homework?</p> </li> <li> <p>Do you ever feel like you need extra help with your homework but don’t know how to get it?</p> </li> <li> <p>Do you feel like you have a good understanding of how to do your homework and get good grades?</p> </li> </ol> <hr/> <hr/> <h1 id="question-subdivisions">Question subdivisions:</h1> <ol> <li> <p>School environment: questions about the physical space, facilities, and resources available in school</p> </li> <li> <p>Curriculum and academics: questions about the subjects taught, how they are taught, and how well students understand the material</p> </li> <li> <p>Relationships with peers and teachers: questions about social dynamics in the classroom, and how students feel about their classmates and teachers</p> </li> <li> <p>Engagement and motivation: questions about how students feel about school and learning, and what motivates them to do their best</p> </li> <li> <p>Home life: questions about the home environment, resources available for studying, and family support for education</p> </li> <li> <p>Health and wellbeing: questions about physical and emotional health, and any challenges that may impact a student’s ability to succeed in school.</p> </li> </ol> <hr/> <hr/> <h2 id="school-environment">School environment:</h2> <ol> <li> <p>Can you tell me about your school and what grade are you in?</p> </li> <li> <p>Do you have enough materials and resources(computer labs, science labs) to learn at school?</p> </li> <li> <p>What changes would you like to see in your school to make your learning experience better?</p> </li> </ol> <hr/> <hr/> <h2 id="curriculum-and-academics">Curriculum and academics:</h2> <ol> <li> <p>What subjects do you like the most and why?</p> </li> <li> <p>What do you find challenging about school?</p> </li> <li> <p>Is there anything you wish you could learn more about in school?</p> </li> <li> <p>Do you feel like the things you learn in school are easy or hard for you?</p> </li> <li> <p>Do you have any particular tools or resources that you use to help you with your homework?</p> </li> <li> <p>Do you feel like you have a good understanding of how to do your homework and get good grades?</p> </li> </ol> <hr/> <hr/> <h2 id="relationships-with-peers-and-teachers">Relationships with peers and teachers:</h2> <ol> <li> <p>How do you feel about your teachers and classmates?</p> </li> <li> <p>Do you feel like you receive enough attention and help from your teachers?</p> </li> <li> <p>Have you ever felt frustrated or upset at school? Can you tell me about it?</p> </li> <li> <p>How do you feel about your classmates?</p> </li> </ol> <hr/> <hr/> <h2 id="engagement-and-motivation">Engagement and motivation:</h2> <ol> <li> <p>What do you like most about school?</p> </li> <li> <p>How do you feel when you have many things to do for school?</p> </li> <li> <p>Do you feel worried or upset about school sometimes? Can you share what happened?</p> </li> <li> <p>How do you feel about your time at school in general?</p> </li> </ol> <hr/> <hr/> <h2 id="home-life">Home life:</h2> <ol> <li> <p>What are the challenges you face when you do your homework?</p> </li> <li> <p>Do you have a designated space to study at home?</p> </li> <li> <p>Are you able to focus on your studies while at home?</p> </li> <li> <p>Do you have access to the resources you need to complete your assignments (e.g. textbooks, internet, etc.)?</p> </li> <li> <p>Do you receive enough support from your parents or guardians when it comes to your studies?</p> </li> <li> <p>Do you have a set routine for doing your homework?</p> </li> <li> <p>Have you ever struggled with completing your homework because of distractions at home?</p> </li> <li> <p>How do you manage to balance other responsibilities (such as chores, hobbies, etc.) with your studies?</p> </li> <li> <p>Have you ever felt frustrated or stressed about your schoolwork while at home? Can you tell me about it?</p> </li> <li> <p>Do you feel like you have enough support from your family to do your best in school?</p> </li> </ol> <hr/> <hr/> <h2 id="health-and-wellbeing">Health and wellbeing:</h2> <ol> <li> <p>Do you feel like you have a good balance between your schoolwork, playtime, and other activities at home?</p> </li> <li> <p>Do you ever feel overwhelmed by the amount of homework you have to do?</p> </li> <li> <p>Do you have any specific subjects or tasks that you find particularly difficult to complete at home?</p> </li> <li> <p>Do you find it easy or difficult to get started on your homework?</p> </li> <li> <p>Do you have any distractions at home that make it hard to do your homework?</p> </li> </ol> <hr/> <hr/> <h2 id="homework">Homework:</h2> <ol> <li> <p>Challenges faced when doing homework</p> </li> <li> <p>Access to resources for completing assignments</p> </li> <li> <p>Routine for doing homework</p> </li> </ol> <hr/> <hr/> <h2 id="home-environment">Home environment:</h2> <ol> <li> <p>Designated space for studying</p> </li> <li> <p>Ability to focus on studies</p> </li> <li> <p>Support from parents or guardians</p> </li> <li> <p>Balancing other responsibilities with studies</p> </li> <li> <p>Frustration or stress about schoolwork while at home</p> </li> <li> <p>Enough support from family to do best in school</p> </li> <li> <p>What other problems do you face while you study at home?</p> </li> </ol> <hr/> <hr/> <h2 id="customerprimary-school-students">Customer(Primary School Students)</h2> <p>Curriculum and Academics:</p> <ol> <li> <p>Understanding the subjects taught in primary school such as math, science, language arts, social studies, and others.</p> </li> <li> <p>Developing the ability to comprehend and apply the concepts taught in each subject.</p> </li> <li> <p>Developing critical thinking skills to analyze and evaluate information.</p> </li> <li> <p>Learning effective study skills to retain and recall information.</p> </li> <li> <p>Developing research skills to gather information and knowledge.</p> </li> </ol> <hr/> <hr/> <h2 id="relationships-with-peers-and-teachers-1">Relationships with Peers and Teachers:</h2> <ol> <li> <p>Developing positive relationships with classmates and teachers.</p> </li> <li> <p>Understanding the importance of teamwork and cooperation in academic and social settings.</p> </li> <li> <p>Learning to communicate effectively with others.</p> </li> <li> <p>Learning to resolve conflicts and negotiate differences in a respectful manner.</p> </li> <li> <p>Understanding the role of authority figures, such as teachers and parents, in providing guidance and support.</p> </li> </ol> <hr/> <hr/> <h2 id="engagement-and-motivation-1">Engagement and Motivation:</h2> <ol> <li> <p>Staying motivated and engaged in their studies, despite challenges and obstacles.</p> </li> <li> <p>Developing a love of learning and a growth mindset.</p> </li> <li> <p>Setting personal goals for academic and personal development.</p> </li> <li> <p>Understanding the benefits of hard work and perseverance.</p> </li> <li> <p>Recognizing the impact of their efforts on their academic and personal success.</p> </li> </ol> <hr/> <hr/> <h2 id="homework-1">Homework:</h2> <ol> <li> <p>Managing time and resources to complete homework assignments.</p> </li> <li> <p>Setting priorities and managing distractions to stay focused on homework.</p> </li> <li> <p>Understanding the purpose and benefits of homework.</p> </li> <li> <p>Developing organizational skills to keep track of assignments and due dates.</p> </li> <li> <p>Seeking assistance and guidance when needed to complete homework effectively.</p> </li> </ol> <hr/> <hr/> <h2 id="home-environment-1">Home Environment:</h2> <ol> <li> <p>Balancing school work with other responsibilities at home.</p> </li> <li> <p>Receiving support and guidance from parents or guardians.</p> </li> <li> <p>Understanding the importance of a positive and supportive home environment for academic success.</p> </li> <li> <p>Seeking assistance and guidance from parents or guardians when needed.</p> </li> <li> <p>Developing time management skills to balance responsibilities and prioritize tasks.</p> </li> </ol> <hr/> <hr/> <h2 id="health-and-wellbeing-1">Health and Wellbeing:</h2> <ol> <li> <p>Maintaining physical health through regular exercise and a healthy diet.</p> </li> <li> <p>Maintaining mental health and avoiding stress and burnout.</p> </li> <li> <p>Understanding the importance of self-care and stress management.</p> </li> <li> <p>Seeking support and guidance when needed to maintain physical and mental health.</p> </li> <li> <p>Recognizing the impact of health and wellbeing on academic and personal success.</p> </li> </ol> <hr/> <hr/> <h2 id="extracurricular-activities">Extracurricular Activities:</h2> <ol> <li> <p>Participating in after-school activities that foster personal growth and development.</p> </li> <li> <p>Understanding the benefits of extracurricular activities for academic and personal growth.</p> </li> <li> <p>Developing new skills and interests through extracurricular activities.</p> </li> <li> <p>Building relationships and connections with peers through shared interests and activities.</p> </li> <li> <p>Balancing extracurricular activities with academic responsibilities and personal time.</p> </li> </ol> <hr/> <hr/> <h2 id="time-management">Time Management:</h2> <ol> <li> <p>Managing time effectively and efficiently.</p> </li> <li> <p>Setting priorities for academic, social, and personal tasks.</p> </li> <li> <p>Understanding the importance of balancing responsibilities and prioritizing tasks.</p> </li> <li> <p>Staying organized and on top of deadlines and appointments.</p> </li> <li> <p>Recognizing the impact of time management on academic and personal success.</p> </li> </ol> <h2 id="exam-preparation">Exam Preparation:</h2> <ol> <li> <p>Studying and preparing for exams in a manner that ensures success and reduces stress.</p> </li> <li> <p>Understanding the importance of exam preparation for academic success.</p> </li> <li> <p>Developing effective study strategies to prepare for exams.</p> </li> <li> <p>Seeking assistance and guidance when needed to prepare for exams effectively.</p> </li> <li> <p>Balancing exam preparation with other responsibilities and commitments.</p> </li> </ol> <p>Curriculum and academics:</p> <ol> <li> <p>Difficulty understanding complex subjects such as math, science, or language arts</p> </li> <li> <p>Lack of engagement or interest in academics</p> </li> <li> <p>Poor performance or grades despite putting in effort</p> </li> </ol> <h2 id="relationships-with-peers-and-teachers-2">Relationships with peers and teachers:</h2> <ol> <li> <p>Difficulty making friends or forming positive relationships with classmates</p> </li> <li> <p>Conflict or bullying from peers</p> </li> <li> <p>Negative or ineffective relationship with teachers</p> </li> </ol> <h2 id="engagement-and-motivation-2">Engagement and motivation:</h2> <ol> <li> <p>Lack of motivation or interest in studies</p> </li> <li> <p>Difficulty staying focused and engaged in class</p> </li> <li> <p>Feeling overwhelmed or stressed with school work</p> </li> </ol> <h2 id="homework-2">Homework:</h2> <ol> <li> <p>Difficulty completing homework assignments due to time or resource constraints</p> </li> <li> <p>Struggle to balance school work with other responsibilities and obligations</p> </li> <li> <p>Difficulty preparing for exams and retaining information</p> </li> </ol> <h2 id="home-environment-2">Home environment:</h2> <ol> <li> <p>Lack of support or understanding from parents or guardians</p> </li> <li> <p>Stress or pressure from family responsibilities or expectations</p> </li> <li> <p>Difficulty balancing school work with other responsibilities at home</p> </li> </ol> <h2 id="health-and-wellbeing-2">Health and wellbeing:</h2> <ol> <li> <p>Poor physical or mental health that affects academic performance</p> </li> <li> <p>Difficulty managing stress or avoiding burnout</p> </li> <li> <p>Difficulty balancing school work with other responsibilities and obligations</p> </li> </ol> <h2 id="extracurricular-activities-1">Extracurricular activities:</h2> <ol> <li> <p>Difficulty finding or participating in after-school activities that are enjoyable or meaningful</p> </li> <li> <p>Lack of time or resources to participate in extracurricular activities</p> </li> <li> <p>Difficulty balancing school work with other responsibilities and obligations</p> </li> </ol> <h2 id="time-management-1">Time management:</h2> <ol> <li> <p>Difficulty managing time effectively and efficiently</p> </li> <li> <p>Struggle to prioritize academic, social, and personal tasks</p> </li> <li> <p>Difficulty balancing school work with other responsibilities and obligations</p> </li> </ol> <h2 id="exam-preparation-1">Exam preparation:</h2> <ol> <li> <p>Difficulty retaining information or preparing for exams</p> </li> <li> <p>Feeling overwhelmed or stressed with exam preparation</p> </li> <li> <p>Difficulty balancing school work with other responsibilities and obligations</p> </li> </ol> <h2 id="target-population">Target Population</h2> <ol> <li> <p>Age: 6-12 years old</p> </li> <li> <p>Gender: Both male and female</p> </li> <li> <p>Location: Students from all regions of Nepal</p> </li> <li> <p>Socio-economic status: Students from a range of socio-economic backgrounds, including low-income families, middle-class families, and upper-income families. </p> </li> <li> <p>School type: Both government and private schools</p> </li> <li> <p>Level of education: Primary education (grades 1-5)</p> </li> <li> <p>Academic performance: All levels of academic performance, including high-achieving(90-100%), average-performing(80-90%), struggling(65-80%), and at-risk students(below 65%)</p> </li> <li> <p>Language proficiency: Both Nepali and English language proficiency</p> </li> <li> <p>Learning style: Students with a range of learning styles, including visual, auditory, and kinesthetic learners</p> </li> <li> <p>Technology usage: Students who have access to technology and are comfortable using it for learning purposes, as well as those who do not have access to technology</p> </li> <li> <p>Parental involvement: Students with varying levels of parental involvement in their education, including those with highly involved parents and those with less involved parents.</p> </li> </ol> <hr/> <hr/> <hr/> <hr/> <h1 id="select-few-responses">Select Few Responses</h1> <ol> <li> <p>Age: 9 years old</p> </li> <li> <p>Gender: female</p> </li> <li> <p>Location: Pokhara, Shanti Niketan Boarding School</p> </li> <li> <p>Socio-economic status: middle-class family</p> </li> <li> <p>School type: private schools</p> </li> <li> <p>Level of education: grade 2</p> </li> <li> <p>Academic performance: struggling students (65 - 80%)</p> </li> <li> <p>Language proficiency: proficient in speaking and understanding nepali language</p> </li> <li> <p>Learning style: visual, auditory, and kinesthetic learner</p> </li> <li> <p>Technology usage: has access to technology and is comfortable using it for entertainment purposes like youtube and tiktok but not skilled in using it for education purposes</p> </li> <li> <p>Parental involvement: very less involved parents, enrolled in tuition class</p> </li> <li> <p>Research method: interview</p> </li> <li> <p>Objective: To understand biggest problem in learning experience at home and in school</p> </li> </ol> <hr/> <hr/> <p>Can you tell me about your experience at school? What are some things you like and don’t like about school?</p> <p>Can you tell me about a time when you struggled with a particular subject or topic? How did you try to overcome this difficulty?</p> <p>How do you like to learn best? Do you prefer to read, listen, or do something hands-on?</p> <p>What do you think is your biggest challenge learning at school? Why do you think it’s a challenge for you?</p> <p>How do you like to learn best? Do you prefer to read, listen, or do something hands-on?</p> <p>How do you feel about your progress in school? Are you proud of the work you’ve done? Are there any areas you wish you could improve?</p> <p>What do you think is your biggest challenge learning at home? Why do you think it’s a challenge for you?</p> <p>Learning preferences: “How do you like to learn best?” helps to determine the learning activities user like most.</p> <p>Learning effectiveness: “How do you learn best?” suitable later on determining features that adapt learning style for each user.</p> <p>Teaching style: “How much do you understand teacher lectures</p> <hr/> <hr/> <p>School:</p> <ol> <li> <p>Can you describe your typical day at school? What classes do you have, and what are they like?(tmro school ma din kasto hunxa. K k garxau, k k class hunxa, ani kasto hunxa)</p> </li> <li> <p>Are there any classes or subjects that you struggle with more than others? Can you tell me about what makes these subjects challenging for you?(tmlai sabai vanda garho lagne subject kun ho? Kina tyo subject garho lagxa?</p> </li> <li> <p>What do you like most about school? Are there any specific classes, activities, or teachers that you particularly enjoy? (Tmlai schoolko k kura sabai vanda manparxa? Ani Classma? Kina tani? Kei testo activities garaunu hunxa teacherle?)</p> </li> <li> <p>Are there any particular classroom activities or projects that you find difficult or confusing? (Classma sabai vanda k garna garho lagxa? Teacherle testo kei activities thyo jun tmlai garna garho vathyo? )</p> </li> <li> <p>Do you feel comfortable asking your teachers questions when you don’t understand something? Why or why not?(Tmle teacherlai najaneko kura sodhna katiko sajilo lauxa ya dar lagaxa? Kina dar lagxa ni?)</p> </li> <li> <p>How can your teachers help you understand your lessons better and make your learning experience more enjoyable? (tmlai teacherle kasari padhayo vane badhi bujhchheu? Kasari padhauda ramailo lagxa? Kasari padhauda jhyau lagxa?)</p> </li> <li> <p>Are there any books, apps, or other tools that you find helpful for learning at school? (Tmle kunai books,apps haru use garxau padhnako lagi? Edi app/book use garxau vane k ko lagi badhi use garchheu? )</p> </li> </ol> <hr/> <hr/> <p>Home:</p> <ol> <li> <p>What does a typical day of learning at home look like for you? (Timi school jana aghi ra school bata aye paxi k garxau?)</p> </li> <li> <p>Can you tell me about a time when you found it difficult to focus on studying at home? What was the reason for this?(Tmlai padhda dhyan dina garho hunxa ki nai? Last time kaile vayo? Tesko barema vanana? Esto katiko vairakhxa ni?)</p> </li> <li> <p>How do you stay motivated to learn at home? Do you have any tricks or strategies that help you stay focused? (kaile kahi hw garna wa padhna man lageko hundaina tei pani padhna parxa jastai exam ko bela testo bela k garxau, kasari garxau? Parents nahuda(dai nahuda, tuition nahuda)</p> </li> <li> <p>Are there any specific resources or tools that you find helpful for studying at home? (testo kunai app, books xa gharma padhnako lagi?)</p> </li> <li> <p>Can you walk me through how you typically approach studying for a new subject or topic? (Naya chapter/topic padhna paryo vane kasari suru garxau?)</p> </li> <li> <p>Are there any specific challenges or obstacles you face when trying to learn at home? How do you typically try to overcome these? (Gharma padhne bela testo khi samasya auxa? K garxau ni testo belama?)</p> </li> </ol> <hr/> <hr/> <p>Here are some tips for starting and concluding an interview:</p> <p>Starting the interview:</p> <ol> <li> <p>Introduce yourself and explain the purpose of the interview.</p> </li> <li> <p>Ask the interviewee if they have any questions or concerns before beginning.</p> </li> <li> <p>Start with some easy and non-threatening questions to help the interviewee feel comfortable.</p> </li> <li> <p>Let the interviewee know that they can take their time and that there are no right or wrong answers.</p> </li> <li> <p>Avoid jumping right into the more sensitive or personal questions, and instead work your way towards them gradually.</p> </li> </ol> <hr/> <hr/> <p>Concluding the interview:</p> <ol> <li> <p>Thank the interviewee for their time and participation.</p> </li> <li> <p>Ask if they have any final thoughts or comments they would like to share.</p> </li> <li> <p>Let the interviewee know what the next steps will be, such as when they can expect to hear back from you or what you plan to do with the information they’ve provided.</p> </li> <li> <p>Assure the interviewee that their feedback and insights are valuable and appreciated.</p> </li> <li> <p>End on a positive note, such as by thanking the interviewee again or wishing them well.</p> </li> <li> <p>Remember to stay professional, respectful, and attentive throughout the interview. It’s important to make the interviewee feel comfortable and valued, as this will encourage them to provide more honest and thoughtful responses.</p> </li> </ol>]]></content><author><name></name></author><summary type="html"><![CDATA[In my research journey to understand the significant challenges faced by primary school students in Nepal, I’ve outlined a step-by-step guide for conducting user research. Similarly, these are also the steps I myself followed.]]></summary></entry><entry><title type="html">Playback Speed Adjustment skip silence or less crucial portions of videos version 1</title><link href="https://badal11.github.io/project/2022/Playback-Speed-Adjustment-skip-silence1/" rel="alternate" type="text/html" title="Playback Speed Adjustment skip silence or less crucial portions of videos version 1"/><published>2022-05-10T13:56:00+00:00</published><updated>2022-05-10T13:56:00+00:00</updated><id>https://badal11.github.io/project/2022/Playback-Speed-Adjustment-skip-silence1</id><content type="html" xml:base="https://badal11.github.io/project/2022/Playback-Speed-Adjustment-skip-silence1/"><![CDATA[<h3 id="introduction">Introduction</h3> <p>This project is designed to speed up lectures by modifying the playback speed based on sound characteristics.</p> <p>This script is inspired by the work presented in the paper <a href="https://arxiv.org/abs/1801.00054">“Deep Reinforcement Learning for Unsupervised Video Summarization with Diversity-Representativeness Reward”</a>. The original Theano implementation can be found <a href="https://github.com/KaiyangZhou/vsumm-reinforce">here</a>.</p> <p>The “SpeedLecture” project works by leveraging a Python script, <code class="language-plaintext highlighter-rouge">lecture_shortener.py</code>, to intelligently shorten lecture videos based on sound characteristics. Here is an overview of how the project works:</p> <ol> <li> <p><strong>Input Video</strong>: The script takes an input video file (<code class="language-plaintext highlighter-rouge">INFILE</code>) representing a lecture or presentation.</p> </li> <li> <p><strong>Output Video</strong>: It produces an output video file (<code class="language-plaintext highlighter-rouge">OUTFILE</code>) that is a shortened version of the input lecture.</p> </li> <li> <p><strong>Sound Analysis</strong>: The script analyzes the audio characteristics of the lecture video, distinguishing between sound and silence periods.</p> </li> <li><strong>Dynamic Speed Adjustment</strong>: <ul> <li><strong>During Sound</strong>: The general video speed (<code class="language-plaintext highlighter-rouge">SPEED_SOUND</code>) is applied during periods of sound, ensuring that the actual content is presented at a faster pace.</li> <li><strong>During Silence</strong>: A different video speed (<code class="language-plaintext highlighter-rouge">SPEED_SILENCE</code>) is applied during periods of silence, allowing for faster navigation through less informative segments.</li> </ul> </li> <li><strong>Silence Detection</strong>: <ul> <li>The script identifies and labels sections as “silent” if the duration of silence exceeds a specified threshold (<code class="language-plaintext highlighter-rouge">MIN_SILENCE_LEN</code>).</li> <li>Frames are considered “silent” if their volume falls below a defined threshold (<code class="language-plaintext highlighter-rouge">SILENCE_THRESHOLD</code>).</li> </ul> </li> <li><strong>Adjustable Parameters</strong>: <ul> <li>Users can fine-tune the behavior of the script through various parameters such as the general video speed, silence video speed, minimum silence length, silence threshold, step duration, and the number of processing threads.</li> </ul> </li> <li> <p><strong>Multithreading Support</strong>: The script supports parallel processing with multiple threads (<code class="language-plaintext highlighter-rouge">THREADS</code>), potentially speeding up the execution time for large videos.</p> </li> <li><strong>Verbose Output</strong>: Users can opt for verbose output, providing additional information during the execution of the script.</li> </ol> <p>By dynamically adjusting the video speed based on sound characteristics, the “SpeedLecture” project aims to provide a more time-efficient way of consuming lecture content, skipping through less informative periods while maintaining comprehension during periods of speech or sound.</p>]]></content><author><name></name></author><category term="Machine_learning"/><category term="project"/><summary type="html"><![CDATA[This project is designed to speed up lectures by modifying the playback speed based on sound characteristics.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://badal11.github.io/project/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://badal11.github.io/project/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://badal11.github.io/project/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Automated Musical Tune Generation using char RNN</title><link href="https://badal11.github.io/project/2022/music_generation/" rel="alternate" type="text/html" title="Automated Musical Tune Generation using char RNN"/><published>2022-01-12T13:56:00+00:00</published><updated>2022-01-12T13:56:00+00:00</updated><id>https://badal11.github.io/project/2022/music_generation</id><content type="html" xml:base="https://badal11.github.io/project/2022/music_generation/"><![CDATA[<h2 id="demo-video"><a href="https://youtu.be/9OP5QJuB0w4">Demo video</a></h2> <h2 id="code"><a href="https://github.com/badal11/Musical_tune_composition">Code</a></h2> <h2 id="objectives">Objectives</h2> <p>-To create an automated musical tune prediction model that takes arbitary notes as input and predicts a sequence of musical notes</p> <p>-To develop an abc notation media player that can transform the converted note sequence into beautiful musical notes</p> <h2 id="music-and-its-representation">Music and it’s representation</h2> <p>A musical tune is a combination of sounds in time through the elements of melody, harmony, rhythm, and timbre. Music, like language, is a method of communication in which a series of notes can convey a variety of emotions. One important point to remember is that music must be both expressive and precise. Classical music, for example, is noted for its careful structure and emotional impact. Beginners find it difficult to compose musical tunes because they must first learn the “language” of music, such as time signatures and how to keep unit beats steady throughout a song while introducing creative uniqueness. We investigate if deep learning can be used to model this dynamic musical structure and effects. Nowadays, the music generation is quite crucial. It has a wide range of applications. Musicians and artists build on the machine’s output to create their creative work. Software-generated music or art is sometimes marketed by the companies or individuals that created it. They can be licensed to businesses and retailers for use in advertising, retail music, and so forth.</p> <h2 id="abc-notation">ABC notation</h2> <p>It uses alphanumeric Character to represent music. ABC notation is a simple but powerful ASCII musical notation format. ABC notation consists of 2 parts: first metadata and second actual music representation. Metadata: Contains the Title(T), Note length(L), Key and Clef(K) and so on. But most important as it determines the beat length of music.</p> <p>Music: Contain information about actual music.</p> <p><img src="https://user-images.githubusercontent.com/67474080/169774597-fdb9e6c5-af3f-4a40-b303-b0c0d9e532cb.png" alt="abc"/></p> <h2 id="midi-format">Midi Format</h2> <p><img src="https://user-images.githubusercontent.com/67474080/169775222-bd3d10ee-3b9c-43ab-9b84-bea3a1d99cf3.png" alt="midi format"/></p> <p>IT Uses Cord name, key and beats to represent music</p> <p>Image</p> <h2 id="sheet-music">Sheet Music</h2> <p><img src="https://user-images.githubusercontent.com/67474080/169775262-227390d2-8f12-44f3-9b71-8c4c9d7c2951.png" alt="sheet music"/></p> <p>Sheet music is a type of musical notation that is handwritten or printed and includes musical symbols to describe the pitches, rhythms, and chords of a song or instrumental musical composition.</p> <h2 id="why-abc-notation">Why abc notation?</h2> <p>Music, as we know it, is continuous in formats such as mp3, midi, wav, and so on, and for an artificially intelligent agent to learn from it, it must first master intricate polyphonic structures in music. The entire syntax, grammar, and logic of music make it difficult to build an artificial network because it has so many parameters like amplitude, pitch, timing, and so on. How does ABC notation help to solve this problem? Music is made using different notes with specific timing for each note. A beautiful melody can be made using such nodes from different instruments superimposing on themselves. Single musical instruments like piano, Guitar, Violin, and bass can be represented using sheet music. Contrary to other formats sheet music is a discrete form of musical representation. But several assumptions like common time (4/4-time signature), key of G, and so on are taken into consideration. And thus, ABC notation maps each symbol from sheet music. ABC notation has strings(A-G). and numerals to represent music. Thus, music is represented in sequential form like the sequence of DNA. This narrow band of string in sequential form is used to train the ANN.</p> <h2 id="dataset-preparation">Dataset Preparation</h2> <p>As bulk music ABC format data is not available in a single source, we’ve collected data from multiple sources using web scraping. Numbers and special characters, as well as letter notations from A to G, are utilized to represent the specified notes in the ABC notation representation of the dataset. Each letter corresponds to a note in the classical scale, such as Do, Re, or MI. Other units are utilized as extra information in addition to the notes that make up the melody, such as the reference number, composer, origin, note length, tempo, rhythm key, ornament, and so on. The characteristics of our data collection are made up of each of these values. Below is a sample example of music written in ABC notation from the dataset.</p> <p>Tools used for Web Scraping:<br/> BeautifulSoup<br/> Requests</p> <p>The websites that were used for data collections are:<br/> https://abcnotation.com/browseTunes,<br/> http://roaringjelly.org<br/> http://www.folktunefinder.com</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">requests</span>
<span class="kn">from</span> <span class="n">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>

<span class="n">f</span> <span class="o">=</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">folktune.txt</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">w</span><span class="sh">'</span><span class="p">)</span>
<span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sh">''</span><span class="p">)</span>
<span class="n">f</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>

<span class="c1">#from page1 to page10
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">11</span><span class="p">):</span>
    <span class="n">url</span> <span class="o">=</span> <span class="sh">'</span><span class="s">http://www.folktunefinder.com/tunes?features=&amp;page=</span><span class="sh">'</span><span class="o">+</span><span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="n">page</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>

    <span class="n">soup</span> <span class="o">=</span> <span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">page</span><span class="p">.</span><span class="n">text</span><span class="p">,</span> <span class="sh">'</span><span class="s">lxml</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">div</span> <span class="o">=</span> <span class="n">soup</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="sh">"</span><span class="s">div</span><span class="sh">"</span><span class="p">,</span> <span class="n">class_</span><span class="o">=</span> <span class="sh">"</span><span class="s">col-md-9</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">tunes</span> <span class="o">=</span> <span class="n">div</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="sh">'</span><span class="s">ul</span><span class="sh">'</span><span class="p">).</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">"</span><span class="s">li</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">tune</span> <span class="ow">in</span> <span class="n">tunes</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">tune</span><span class="p">.</span><span class="n">a</span><span class="p">.</span><span class="n">string</span><span class="p">)</span>

        <span class="n">site</span> <span class="o">=</span> <span class="sh">'</span><span class="s">https://www.folktunefinder.com</span><span class="sh">'</span><span class="o">+</span><span class="n">tune</span><span class="p">.</span><span class="n">a</span><span class="p">[</span><span class="sh">"</span><span class="s">href</span><span class="sh">"</span><span class="p">]</span>
        <span class="n">sitePage</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">site</span><span class="p">)</span>
        <span class="n">siteSoup</span> <span class="o">=</span> <span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">sitePage</span><span class="p">.</span><span class="n">text</span><span class="p">,</span> <span class="sh">'</span><span class="s">lxml</span><span class="sh">'</span><span class="p">)</span>

        <span class="n">f</span> <span class="o">=</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">folktune.txt</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">siteSoup</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="sh">'</span><span class="s">pre</span><span class="sh">'</span><span class="p">).</span><span class="n">string</span><span class="p">)</span>
        <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sh">'</span><span class="se">\n\n\n\n</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">f</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>
<span class="kn">import</span> <span class="n">requests</span>

<span class="n">f</span> <span class="o">=</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">roaringdata.txt</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">w</span><span class="sh">'</span><span class="p">)</span>
<span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sh">''</span><span class="p">)</span>
<span class="n">f</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>

<span class="n">url</span> <span class="o">=</span> <span class="sh">'</span><span class="s">http://roaringjelly.org/~jc/cgi/abc/find.cgi?P=.*&amp;find=FIND&amp;m=title&amp;W=wide&amp;scale=0.65&amp;limit=160&amp;thresh=5&amp;fmt=single&amp;V=1&amp;Tsel=tune&amp;Nsel=0</span><span class="sh">'</span>
<span class="n">page</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="n">soup</span> <span class="o">=</span> <span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">page</span><span class="p">.</span><span class="n">text</span><span class="p">,</span><span class="sh">'</span><span class="s">lxml</span><span class="sh">'</span><span class="p">)</span>

<span class="n">form</span> <span class="o">=</span> <span class="n">soup</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="sh">"</span><span class="s">form</span><span class="sh">"</span><span class="p">,</span> <span class="n">class_</span> <span class="o">=</span> <span class="sh">"</span><span class="s">match</span><span class="sh">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">table</span> <span class="ow">in</span> <span class="n">form</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">table</span><span class="sh">'</span><span class="p">,</span> <span class="n">class_</span> <span class="o">=</span> <span class="sh">'</span><span class="s">match</span><span class="sh">'</span><span class="p">):</span>
    <span class="n">lengthOfTable</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">table</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">tr</span><span class="sh">'</span><span class="p">))</span>

    <span class="n">checkTitle</span> <span class="o">=</span><span class="sh">''</span>
    <span class="n">title</span> <span class="o">=</span> <span class="sh">''</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">lengthOfTable</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>  <span class="c1">#final range = lengthOfTable-1
</span>        <span class="n">data</span> <span class="o">=</span> <span class="n">table</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">tr</span><span class="sh">'</span><span class="p">)[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">title</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">td</span><span class="sh">'</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">string</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">title</span>  <span class="o">+</span> <span class="sh">'</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>


        <span class="n">urlStringConcat</span> <span class="o">=</span> <span class="sh">'</span><span class="s">http://roaringjelly.org/~jc/cgi/abc/get.cgi?F=ABC&amp;U=</span><span class="sh">'</span>
        <span class="n">abcUrl</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">td</span><span class="sh">'</span><span class="p">)[</span><span class="mi">4</span><span class="p">].</span><span class="nf">find</span><span class="p">(</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">)[</span><span class="sh">"</span><span class="s">href</span><span class="sh">"</span><span class="p">]</span>
        <span class="n">abcUrl</span> <span class="o">=</span> <span class="n">abcUrl</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">U=</span><span class="sh">'</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">finalUrl</span> <span class="o">=</span> <span class="n">urlStringConcat</span> <span class="o">+</span> <span class="n">abcUrl</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">finalUrl</span> <span class="o">+</span><span class="sh">'</span><span class="se">\n\n\n\n</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">title</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">td</span><span class="sh">'</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">string</span>
        <span class="n">abcPage</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">finalUrl</span><span class="p">)</span>
        <span class="n">abcSoup</span> <span class="o">=</span> <span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">abcPage</span><span class="p">.</span><span class="n">text</span><span class="p">,</span><span class="sh">'</span><span class="s">lxml</span><span class="sh">'</span><span class="p">)</span>

        <span class="n">abc</span> <span class="o">=</span> <span class="n">abcSoup</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="sh">"</span><span class="s">pre</span><span class="sh">"</span><span class="p">).</span><span class="n">string</span>
        <span class="n">f</span> <span class="o">=</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">roaringdata.txt</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="sh">"</span><span class="s">utf-8</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">abc</span><span class="p">)</span>
        <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sh">'</span><span class="se">\n\n\n</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">f</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
        <span class="c1"># print(abc)
</span>
<span class="c1"># parsing all tables
# from each table url is extracted
# this url is used to get abc format data
# file writting
# Same title data is not implemented
</span></code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">requests</span>
<span class="kn">from</span> <span class="n">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>


<span class="n">url</span> <span class="o">=</span> <span class="sh">'</span><span class="s">https://abcnotation.com/browseTunes</span><span class="sh">'</span>
<span class="n">page</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="n">soup</span> <span class="o">=</span> <span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">page</span><span class="p">.</span><span class="n">text</span><span class="p">,</span> <span class="sh">'</span><span class="s">lxml</span><span class="sh">'</span><span class="p">)</span>

<span class="n">table</span> <span class="o">=</span> <span class="n">soup</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="sh">"</span><span class="s">table</span><span class="sh">"</span><span class="p">,</span> <span class="n">class_</span> <span class="o">=</span><span class="sh">"</span><span class="s">width100pc table-bordered</span><span class="sh">"</span><span class="p">)</span>
<span class="n">tableRows</span> <span class="o">=</span> <span class="n">table</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">tr</span><span class="sh">'</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
<span class="c1">#should be iterated to all td
</span><span class="k">for</span> <span class="n">tableData</span> <span class="ow">in</span> <span class="n">tableRows</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">td</span><span class="sh">'</span><span class="p">):</span>

    <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">firstLink</span> <span class="ow">in</span> <span class="n">tableData</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">abcWebsite</span> <span class="o">=</span> <span class="sh">'</span><span class="s">https://abcnotation.com/</span><span class="sh">'</span>
            <span class="n">firstLink</span> <span class="o">=</span>  <span class="n">abcWebsite</span> <span class="o">+</span> <span class="n">firstLink</span><span class="p">[</span><span class="sh">'</span><span class="s">href</span><span class="sh">'</span><span class="p">]</span>

            <span class="n">secondPage</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">firstLink</span><span class="p">)</span>
            <span class="n">secondSoup</span> <span class="o">=</span> <span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">secondPage</span><span class="p">.</span><span class="n">text</span><span class="p">,</span> <span class="sh">'</span><span class="s">lxml</span><span class="sh">'</span><span class="p">)</span>

            <span class="n">checkString</span> <span class="o">=</span> <span class="sh">''</span>

            <span class="n">secondSoup</span><span class="p">.</span><span class="n">pre</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">)[</span><span class="mi">1</span><span class="p">][</span><span class="sh">"</span><span class="s">href</span><span class="sh">"</span><span class="p">]</span>

            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">secondSoup</span><span class="p">.</span><span class="n">pre</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">):</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">i</span><span class="p">[</span><span class="sh">"</span><span class="s">href</span><span class="sh">"</span><span class="p">]</span>
                    <span class="nf">if</span><span class="p">(</span><span class="n">i</span><span class="p">.</span><span class="n">string</span> <span class="o">!=</span> <span class="n">checkString</span><span class="p">):</span>
                        <span class="c1"># # print(i['href'])
</span>                        <span class="c1"># print(i.string+'\n')
</span>                        <span class="n">checkString</span> <span class="o">=</span> <span class="n">i</span><span class="p">.</span><span class="n">string</span>
                        <span class="n">thirdLink</span> <span class="o">=</span> <span class="n">abcWebsite</span> <span class="o">+</span> <span class="n">i</span><span class="p">[</span><span class="sh">'</span><span class="s">href</span><span class="sh">'</span><span class="p">]</span>
                        <span class="c1">#print(thirdLink)
</span>                        <span class="nf">print</span><span class="p">(</span><span class="n">counter</span><span class="p">)</span>
                        <span class="n">counter</span> <span class="o">+=</span><span class="mi">1</span>

                        <span class="n">thirdPage</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">thirdLink</span><span class="p">)</span>
                        <span class="n">thirdSoup</span> <span class="o">=</span> <span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">thirdPage</span><span class="p">.</span><span class="n">text</span><span class="p">,</span> <span class="sh">'</span><span class="s">lxml</span><span class="sh">'</span><span class="p">)</span>
                        <span class="c1"># # print(thirdSoup.find('pre'))
</span>
                        <span class="n">f</span> <span class="o">=</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">datafromABC.txt</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">)</span>
                        <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">thirdSoup</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="sh">'</span><span class="s">pre</span><span class="sh">'</span><span class="p">).</span><span class="n">string</span><span class="p">)</span>
                        <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sh">'</span><span class="se">\n\n\n\n</span><span class="sh">'</span><span class="p">)</span>
                        <span class="n">f</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>

                <span class="k">except</span><span class="p">:</span>
                    <span class="k">pass</span>
        <span class="k">except</span><span class="p">:</span>
                <span class="k">pass</span>
</code></pre></div></div> <h2 id="data-cleaning">Data cleaning</h2> <p>Data cleaning is an important task. Uncleaned data produces more inaccuracy in the result. It helps in detecting, removing and correcting the incomplete, irrelevant, corrupt or inaccurate part of data from a dataset. Example of a cleaned sample data</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>X:111
T:11. DROPS OF BRANDY (32 bar polka)
T:Spokes's Tune/Vic's No. 2
M:2/4
L:1/8
K:D
d/2e/2|\
fd ec|dc/2B/2 AG|FA dF|EA Ad/2e/2|\
fd ec|dc/2B/2 AG|FA Bc|[1 d D D:| [2 d D D2 |
FA/2A/2 AA|GB/2B/2 BB|FA/2A/2 AF|EA A&gt;G|\
FA df|ge dc|dB AG|FE D2 :|
</code></pre></div></div> <h2 id="data-preparation">Data Preparation</h2> <p>Each data point has different characters length. But are a total of 87 unique characters in for musical representation in every data point. Each unique character has been allocated a numerical index. We’ve developed a dictionary in which the key corresponds to a character and the value is the character’s index. We’ve also made the polar opposite, where the key belongs to the index and the value is the character for ease of recognition.</p> <p><img src="https://user-images.githubusercontent.com/67474080/169769684-acd3fcfa-f3e6-4ba0-9251-b0e24beaeb16.png" alt="unique_chars"/></p> <p>Data is fed into batches. We’ll feed a batch of character sequences into our model at once. First, we must create our batches. The following parameters have been set: Batch size = 16 Sequence Length= 64, Input.txt length = 129,665, Number of unique characters = 87 Every batch has got two components, X tensor of size: 16<em>64 and Y tensor of size: 16</em>64*87.</p> <h2 id="model-building">Model Building</h2> <p>The code for model building is available in the repo above. The model buidling process is inspired form the very famous blog of Andrej Karpathy <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a>. Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems. Networks in each recurrent cell learns to control the storage of information through the use of:</p> <p><strong>Forget gate</strong><br/> The forget gate determines which data need attention and which can be overlooked. The sigmoid function passes information from the current input X(t) and the hidden state h(t-1). Sigmoid generates values ranging from 0 to 1. It determines if a portion of the previous output is required (by giving the output closer to 1). The cell will utilize this value of f(t) for point-by-point multiplication later.</p> <p><strong>Input gate</strong><br/> To update the cell status, the input gate performs the following processes. The second sigmoid function receives the current state X(t) and the previously hidden state h(t-1). The values are changed from 0 to 1 (important) (not-important). The tanh function will then be used to pass the identical information from the hidden and current states. The tanh operator will construct a vector (C(t)) with all the possible values between -1 and 1 to regulate the network. The activation functions generate output values that are ready for point-by-point multiplication.</p> <p><strong>Output gate</strong><br/> The value of the next hidden state is determined by the output gate. This state stores data from earlier inputs. The current state and previous concealed state values are first sent to the third sigmoid function. The tanh function is then used to construct a new cell state from the old cell state. These two outputs are multiplied one by one. The network selects which information the concealed state should convey based on the final value. Prediction is based on this concealed state. The new cell state and hidden state are then passed forward to the next time step.</p> <p><strong>Why LSTM?</strong><br/> Good for handling long term dependencies in data<br/> Quickly adaptable to new data<br/> Does not suffer from exploding and vanishing gradients.<br/> Language models can be operated at the character level, n-gram level, sentence level or even paragraph level.</p> <p><img src="https://user-images.githubusercontent.com/67474080/169770697-19b3705a-5d30-4074-a691-022bfb633bc5.png" alt="Underlying structure of a single LSTM cell"/></p> <p><strong>Underlying structure of a single LSTM cell</strong></p> <p><img src="https://user-images.githubusercontent.com/67474080/169770745-13b5b7de-4c4a-4a22-8c86-3ae29fbb482c.png" alt="Illustration of multiple LSTM cells to form a network"/></p> <p><strong>Illustration of multiple LSTM cells to form a network</strong></p> <h2 id="workflow">Workflow</h2> <p>The basic principle in generation of musical notes<br/> • select some seed information as the first item (e.g., the first note of a melody);<br/> • feedforward it into the recurrent network in order to produce the next item (e.g., next note);<br/> • use this next item as the next input to produce the next item; and<br/> • repeat this process iteratively until a sequence (e.g., of notes, i.e., a melody) of the desired length is produced.</p> <p>Insert Two figs- underlying stru and mutilple combns</p> <h2 id="dropout-layers">Dropout layers</h2> <p>The Dropout layer, which helps minimize overfitting, sets input units to 0 at random with a rate frequency at each step during training time. Inputs that are not set to 0 are scaled up by 1/(1 - rate) such that the total sum remains unaltered. The Dropout layer only applies when the training parameter is set to True, which means no data are dropped during inference. When using model.fit, training is automatically set to True, and in other cases, you can manually set the kwarg to True when calling the layer.</p> <h2 id="the-softmax-activation-function">The Softmax Activation function</h2> <p>In neural network models that predict a multinomial probability distribution, the softmax function is utilized as the activation function in the output layer. Softmax is utilized as the activation function for multi-class classification issues in which class membership is required to classify characters from one of the 87 mappings.</p> <h2 id="optimizer-and-hyperparameter-tuning">Optimizer and hyperparameter tuning</h2> <p>Standard gradient descent and its variants still face a lot of problems in training deep neural networks in practical solutions which gave rise to an optimizer called Adam. Adam calculates adaptive learning-rates for every parameter which makes it suitable for sparse datasets. It uses momentum which is a concept that reduces oscillations of loss function in unnecessary directions thereby accelerating the convergence towards the relevant direction. In our case the hyperparameters had values 0.9 and 0.999 and value of 0.0001 as learning rate.</p> <h2 id="model-training">Model training</h2> <p>The model was trained on a local PC with an RTX 2070 GPU and the required software. Three basic functions comprise the model training. The first function is batch data reading. The second function is to train the batches in a sequential order, and the third is to build the model in which the batches of data are trained. Our batch size is 16, and the length of the sequence is 64. The total number of batches used is 129665, with each batch containing 8104 data points. The training was initially completed over 100 epochs, with each epoch containing 126 batches. Because numerical values are easy to compute. The character to index and vice versa mapping is done here to make the computation easier since numerical values are flexible to perform complex calculations. The batch size, sequence length, and vocabulary size inputs are passed to the model construct python method. It then uses category cross entropy and the Adam optimizer to assemble the model. There are 256 LSTM layers in total, with a dropout of 0.2 and a model of sequential flow of pattern and softmax activation function. The generated train model weights are kept in a separate model weights file, and checkpointing is conducted to allow the training procedure to resume if the run time is off for various technical reasons. On a high-end system, model training took 2 hours for 100 epochs with 126 batches each. The model was also evaluated on Google Colab’s free version, which took roughly 7 hours.</p> <h2 id="next-character-probability-determination">Next character probability determination</h2> <p><img src="https://user-images.githubusercontent.com/67474080/169771366-f5dda257-23ed-4676-821a-197c27d4734a.png" alt="Next character probability determination"/></p> <p><img src="https://user-images.githubusercontent.com/67474080/169771404-3fa474d0-a21d-4c3e-9016-53067b15839b.png" alt="Next char A"/><br/></p> <p><strong>Therefore, the next character is: A</strong></p> <p><img src="https://user-images.githubusercontent.com/67474080/169771435-20459c1d-61f6-48cf-958b-ae8522a39c3c.png" alt="Next char B"/>,<br/> <strong>Therefore, the next character is: “</strong></p> <p>A limitation of the iterative feedforward strategy on an RNN, is that generation is deterministic. Indeed, a neural network is deterministic . As a consequence, feedforwarding the same input will always produce the same output. As the generation of the next note, the next note, etc., is deterministic, the same seed note will lead to the same generated series of notes .<br/> Sampling Fortunately, will solve this issue. The assumption is that the output representation of the melody is one-hot encoded. In other words, the output representation is of abc type with 87 possible characters, the output activation layer is SoftMax and generation is modeled as a classification task.</p> <h2 id="model-metrics">Model metrics</h2> <p><img src="https://user-images.githubusercontent.com/67474080/169775526-6e3e30ba-2ab7-4df2-b38b-3f5e93e0beb8.gif" alt="Epoch vs Accuracy Loss plot"/> <img src="https://user-images.githubusercontent.com/67474080/169775546-c47e9d44-ff89-4bce-a365-08025913fde7.png" alt="Testing different optimizers"/> <img src="https://user-images.githubusercontent.com/67474080/169775590-8898b3dc-5453-4614-b1f9-0739e0567541.png" alt="Testing different number of LSTM layers"/> <img src="https://user-images.githubusercontent.com/67474080/169775604-9dc148b6-150b-4620-9f4d-a33e1ddf4168.png" alt="Testing different number of dropout values"/></p> <h2 id="input">Input</h2> <p>As we used RNN technique, the first character for the output can be given. It should be noted that the first character can be random but we can always give freedom to user to choose the first musical character. Moreover, input interface can contain input field to mention the epoch of model to be used for generating the music. As the epoch goes higher the error tends to be lower but takes more computational power. Finally, the length of the piece of music can be manually set. The input interface is similar to below:</p> <p><img src="https://user-images.githubusercontent.com/67474080/169776375-3e9a598a-7a1a-42ba-a153-97a6f7343702.png" alt="Input"/></p> <p>Ouput using inbuilt package <img src="https://user-images.githubusercontent.com/67474080/169776417-c8993e45-9528-4fe3-9743-39ee81df8c43.png" alt="Output using inbuilt package"/></p> <p>Ouput using external web application abcjs.net <img src="https://user-images.githubusercontent.com/67474080/169776441-d545213f-6422-4bd7-8c5f-06374b615d62.png" alt="Output using abcjs net"/></p> <h2 id="references">References</h2> <p>[1]Andrej karpathy. “The Unreasonable Effectiveness of Recurrent Neural Networks”, May 2021<br/> [2] J. McCormack, “Grammar based music composition, Complex systems”, Chapter 96, 1996<br/> [3] K. Dongwoo, W. Christian, “Computer Assisted Composition with Recurrent Neural Network”, 2018<br/> [4] S Madjiheurem, L Qu, C Walder, “Chord2vec: Learning musical chord embeddings - Proceedings of the constructive machine learning”, 2016<br/> [5] O. D. Van, A. Dieleman, Z. Sander, S. Heiga, K. Vinyals, A. Graves, N. Kalchabrenner, A. Senior, K. Kavukcuogl, “WaveNet:A Generative Model for Raw Audio, 1609”,2016<br/> [6] Payne, Christine. MuseNet: OpenAI, Available at: openai.com/blog/musenet. 25 Apr. 2019<br/> [7] C. Dalitz, “ABC User’s Guide Book”, Apr. 2022<br/> [8] L. Richardson, Beautiful soup Documentation Release 4.4.0, 24 Dec. 2019</p>]]></content><author><name></name></author><category term="Deep_Learning"/><category term="project"/><summary type="html"><![CDATA[Automated composition of abc notation using char RNN that can be played by using inbuilt music library or external library]]></summary></entry></feed>