<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Audio Based Nepali Political Figure | Badal Tripathi</title> <meta name="author" content="Badal Tripathi"> <meta name="description" content="This is a test. A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://badal11.github.io/project/2023/Audio-Based-Nepali-Political-Figure/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Badal </span>Tripathi</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/project/2022/Nepali-Student's-Insights/">Nepali Student's Insights</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Audio Based Nepali Political Figure</h1> <p class="post-meta">May 15, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/project"> <i class="fa-solid fa-hashtag fa-sm"></i> project</a>     ·   <a href="/blog/category/deep-learning"> <i class="fa-solid fa-tag fa-sm"></i> Deep_Learning</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction"><strong>Introduction</strong></h1> <p>Identifying people based on their audio is a fascinating topic. Every human voice is distinctive; as people age, their voices vary, and they sound different in different contexts, altering the problem landscape correspondingly. Speaker Identification is concerned with identifying a person based on the spoken audio of that person. Traditionally, speech processing models were widely recognized, but convolution neural networks have lately demonstrated that they, too, may generate astounding outcomes.</p> <p><strong>Data Description</strong></p> <p>I was always interested to train a model in my language. Therefore, I collected Nepali audio from youtube of exactly 34 politicians male and female speaking at different venues, providing proper care not to include noise in the audio and the average audio length is around 5 minutes. The dataset is available <a href="https://drive.google.com/file/d/44vgeREGE_4343R78XA/view?usp=sharing" rel="external nofollow noopener" target="_blank">here</a></p> <h1 id="converting-to-deep-learning-problem"><strong>Converting to Deep Learning Problem</strong></h1> <p><img src="https://miro.medium.com/v2/resize:fit:1050/1*jiuTG4UgSKhFt9wN5XhFeA.png" alt=""></p> <p>Showing the formulation of deep learning problem</p> <p>The necessary preprocessing is done depending upon the audio we have it may include removing some part of the audio, removing silence, etc. As CNN can only work on images (1d CNN can word on text but not considering here), the spectrogram images is created from the preprocessed audio and provided as input to the model which will perform training to produce the result. Basically there are four steps:</p> <ol> <li> <p>Reading the audio</p> </li> <li> <p>Perform necessary pre-processsing</p> </li> <li> <p>Selecting a Model</p> </li> <li> <p>Preparing data for the model</p> </li> <li> <p>Model Architecture</p> </li> </ol> <h2 id="1-reading-the-audio">1. Reading the audio</h2> <p>The first step in this many step process is reading an audio, the audio can be in many format .wav, .mp3, etc. It can be done my using signal processing library Librosa.</p> <h1 id="reading-the-audio">Reading the audio</h1> <p>import librosa<br> path= ‘path to audio’sample_rate= 22050<br> arr_audio,_=librosa.load(path,sr= sample_rate) #arr_audio is an array of amplitudes if the audio is 2sec in length<br> #it will be (2,2*22050)= (2,44100), where 2 in the array is due to #dual channel of audio</p> <p>As audio is nothing more than a signal, where on the x-axis there is time and on the y-axis, there is amplitude, as it is a continuous signal we want to convert in a discrete form so that we can extract the features from it. By sampling the signal at a specific rate we can do this, what does it mean is that we are getting the value of amplitude from some instances of time. Here the sampling rate is 22050, meaning on one second of audio we will extract the amplitude 22050.</p> <h2 id="2-preprocessing"><strong>2. Preprocessing</strong></h2> <p><strong>2.1 Converting to mono channel</strong></p> <p>As the audio data is from youtube, it is a dual-channel meaning for the particular instance of time, there is two value of amplitude one for the left earpiece and one for the right. But we need a single value of amplitude at any instance. So just by taking the mean of the dual-channel amplitude array, we can convert it into a mono channel.</p> <p>arr_audio= arr_audio.sum(0)/2</p> <p><strong>2.2 Removing noise</strong></p> <p>In every audio, the first 1 minute is an advertisement or some other kind of unwanted audio segment. So we will be removing the first and the last 1 minute from every audio.</p> <p>arr_audio= arr_audio[sample_rate<em>60:arr_audio.shape[0]-sample_rate</em>60]</p> <p><strong>2.3 Removing silence</strong></p> <p>While speaking they may we momentary pause in between, we want our model to learn the characteristics of individual persons audio, it will learn nothing from the silence. So the silence needs to be removed.</p> <p><img src="https://miro.medium.com/v2/resize:fit:1050/1*pGd2ErQigpjxDeMyrjbqpA.png" alt=""></p> <p>Steps to remove silence</p> <p>from pydub import AudioSegment<br> from pydub.silence import split_on_silencedef float_to_int(array, type=np.int16):<br> if array.dtype == type:<br> return array if array.dtype not in [np.int16, np.int32, np.int64]:<br> if np.max(np.abs(array)) == 0:<br> array[:] = 0<br> array = type(array * np.iinfo(type).max)<br> else:<br> array = type(array / np.max(np.abs(array)) * np.iinfo(type).max)<br> return arraydef int_to_float(array, type=np.float32):<br> if array.dtype == type:<br> return array if array.dtype not in [np.float16, np.float32, np.float64]:<br> if np.max(np.abs(array)) == 0:<br> array = array.astype(np.float32)<br> array[:] = 0<br> else:<br> array = array.astype(np.float32) / np.max(np.abs(array))<br> return array#Step1:Create an convert to int and create audio_segment#Convert to float<br> feature= float_to_int(arr_audio)#Create audio segment<br> audio = AudioSegment(feature.tobytes(),frame_rate = self.sample_rate,sample_width = feature.dtype.itemsize\<br> ,channels = 1) #Step2:removing the silence from the audio segment<br> audio_chunks= split_on_silence(audio,min_silence_len=min_silence,silence_thresh= -30,keep_silence= 100)<br> #Step3:converting it back to the array<br> #Convert audio segment to array<br> arr_audio= sum(audio_chunks)<br> arr_audio= np.array(arr_audio.get_array_of_samples())#Convert back to int<br> arr_audio= self.int_to_float(arr_audio)</p> <p>After doing all the preprocessing the data looks like this:</p> <p><img src="https://miro.medium.com/v2/resize:fit:1050/1*8MlE7HkDzwRmRK7vmV_z2A.png" alt=""></p> <p>A single data point contains the name of the speaker, its gender, Duration meaning the length of the audio in seconds, and finally Features that contain the amplitude of the audio sampled at a specific sampling rate.</p> <h1 id="3-selecting-a-model"><strong>3. Selecting a model</strong></h1> <p>There are 34 speakers if we consider each speaker as a single class there will be 34 classes. As the speaker increases, the class will also increase which is not an ideal scenario. To identify a single class supposes k1, the model needs to learn features that clearly distinguish it from other (K-k1) classes which will require lots of data ie increasing the length of audio of a single speaker. I don’t think a user wants to spend time recording its audio, he/she is willing to record for 5–30 sec and get done with it. So, using this strategy is not a choice.</p> <p>A Siamese Network is a network that contains two identical subnetworks ‘<em>identical’</em> here means, they have the same configuration with the same parameters and weights. Parameter updating is mirrored across both sub-networks. It is used to find the similarity of the inputs by comparing their feature vectors, so these networks are used in many applications.</p> <p>If we want to add a new speaker here we can update the neural network and retrain it on the whole dataset which was not possible in traditional architecture. SNNs, on the other hand, learn a similarity function. Thus, we can train it to see if the two images are the same (which we will do here). This enables us to classify new speakers without training the network again.</p> <p><img src="https://miro.medium.com/v2/resize:fit:1050/1*1ObEvWMVJA0AAdW__g-4kA.png" alt=""></p> <p>Siamese Network.(<a href="https://medium.com/swlh/one-shot-learning-with-siamese-network-1c7404c35fda" rel="external nofollow noopener" target="_blank">https://medium.com/swlh/one-shot-learning-with-siamese-network-1c7404c35fda</a>)</p> <p>Siamese networks accept two inputs x1 and x2. For our case, it is either two spectrogram images from the same user or from different users. Images from the same user are labeled as 1 and images from different users are labeled as 0. These images are sent through an identical network and feature vectors f(x1) and f(x2) are generated. A function is used to calculate the difference between these two feature vectors. The difference score between the feature vector for the same user will be small as they will be located in the same neighborhood in that dimensional space whereas for the different users the feature vector difference will be large.</p> <p>Lets take example the two feature vector are (0.5,0.7) and (1,0.7)</p> <p>Different User:</p> <p><img src="https://miro.medium.com/v2/resize:fit:1023/1*PTAYjnjYVYPg42u3O0LYZw.png" alt=""></p> <p>As we can see the model tries to increase the distance between the feature vector if they are from different user.</p> <p>From same user:</p> <p><img src="https://miro.medium.com/v2/resize:fit:999/1*gsKzbC3RMMmUjy8hCbGeHg.png" alt=""></p> <p>Similarly if the feature vectors is from same user after every iteration model reduces the distance between the two bringing them closer in the vector space.</p> <h1 id="4-preparing-data-for-siamese-network"><strong>4. Preparing data for siamese network</strong></h1> <p>Let’s suppose there is an audio from speaker Hari of 0.3miliSeconds and by using the sampling rate= 22050, we can compute amplitude at 8 instances ie (0.3*1000 sec)/22050~10. SO, the feature vector for this audio segment looks like this:</p> <p><img src="https://miro.medium.com/v2/resize:fit:1050/1*ae7TmYPKXcvKYqc4AOReMQ.png" alt=""></p> <h2 id="1-train-test-splitting"><strong>1. Train test splitting</strong></h2> <p><img src="https://miro.medium.com/v2/resize:fit:1050/1*-BjBafPxBRRb8veW6HAwdA.png" alt=""></p> <p>The first 80% of the audio segment was utilized to generate a training set, while the remaining 20% was used to create the test set.</p> <h2 id="2-getting-similar-and-disimilar-pairs-of-spectogram-from-speakers-audio"><strong>2 Getting similar and disimilar pairs of spectogram from speakers audio</strong></h2> <p><strong>2.1 Similar pair:</strong></p> <p>A single pair of the image of spectrogram created from the specific user audio will create one data point and the label of this data point will be 1 indicating that this pair of spectrogram images belong to individual audio.</p> <p><img src="https://miro.medium.com/v2/resize:fit:1050/1*8miEz-MVNMlpILGGYTPmtw.png" alt=""></p> <p>The most important parameter while creating a spectrogram is the audio window length which defines the length of the audio segment which will be used to create a single spectrogram image. Similarly, the starting position for these two windows will be selected randomly. For my implementation, I have selected 100 pairs of datapoint for an single user, therefore the total datapoint which will have label 1 are 34*100= 3400.</p> <p><strong>2.2 Dissimilar pair:</strong></p> <p>A single data point contains one spectrogram from one user and another spectrogram from another user, with the label 0 indicating that the spectrograms belongs to a separate user. As a result, we will pair each user with every other user and construct all possible combinations of pairs from distinct users.</p> <p>We chose 25 pairs of spectrogram photographs, one from each of two different people. As a result, we will generate 25 pairs of spectrograms with each other user for each user (33 in our situation), for a total of 33<em>25= 825 pairs of images. Similarly, the total number of dissimilar spectrogram pairings for 34 persons is 34</em>825=28050, and these pairs are labeled as 0, indicating that the spectrogram images in the pair are from distinct people.</p> <p>The dataset looks like this after performing all these tasks:</p> <p><img src="https://miro.medium.com/v2/resize:fit:1050/1*7Nk4kOCYZKZOpKmAcMLkZA.png" alt=""></p> <h1 id="4-model-architecture">4 Model Architecture</h1> <p>Because the image number is just 31000, transfer learning is the only option. As a result, DenseNet121, which has already been trained on “imagenet,” is employed here. We must utilize the spectrogram height and breadth that we created while initializing this network.</p> <p>subnetwork= tf.keras.applications.densenet.DenseNet121(include_top=False, weights=’imagenet’, ,input_tensor=None,input_shape=(height,width,3), pooling=None)</p> <p>Similarly, we need a distance function which calculates the distance between two feature vector produced by the DenseNet. For this purpose we will be using euclidean distance:</p> <p>def euclidean_distance(self,inp): x,y= inp<br> sum_square = tf.math.reduce_sum(tf.math.square(x - y),axis=1,keepdims=True)<br> return tf.math.sqrt(tf.math.maximum(sum_square,1e-07))</p> <p>Finally, let’s put all the pieces together and create a complete model.</p> <p>def create_siamese_network(height,width):<br> #Inputs for two spectrogram images<br> image1= Input(shape=(height,width,3))<br> image2= Input(shape=(height,width,3)) #sending the images through the DenseNet<br> output1= Flatten()(subnetwork(image1))<br> output2= Flatten()(subnetwork(image2)) #Calculating the distance between the feature vector<br> output = layers.Lambda(euclidean_distance)([output1, output2])</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    model= Model(inputs=[image1,image2],outputs=output)  
    return model
</code></pre></div></div> <p>DenseNet will generate a feature matrix with multiple channels; to get a vector from it, we flatten the output matrix and calculate the distance between the channels.</p> <h1 id="5-training-the-model">5. Training the model</h1> <h2 id="51-contractive-loss">5.1 Contractive Loss</h2> <p>We need a loss function that minimizes the distance between spectrograms from the same individual and maximizes the distance between spectrograms of a different individual. Contractive loss works exactly like that and we will be using it here to train the model.</p> <p>margin= 3.0<br> def contractive_loss(label,distance):</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>difference= margin-distance  
temp= tf.where(tf.less(difference,[0.0]),[0.0],difference)     
loss= tf.math.reduce_mean(tf.cast(label,dtype=tf.float32)*tf.math.square(distance)+(1-tf.cast(label,dtype=tf.float32))*tf.math.square(temp))  
  
return loss
</code></pre></div></div> <p>The margin value is the most crucial parameter in the contractive loss. This number tells the model what the minimal distance between feature vectors from different persons should be, and we set the margin value to 3 in our implementation. Similarly, the model attempts to reduce the distance between spectrograms of the same image to 0.</p> <h2 id="52-evaluation">5.2 Evaluation</h2> <p>similar distance= The distance between the feature vectors of two spectrogram images generated from the same person.</p> <p>dissimilar distance= The distance between the feature vectors of two spectrogram images generated from the different person.</p> <p>At first, the similar and dissimilar distances were almost identical, but as the model trained, they began to diverge, and after 30 epochs, the distance between training and testing looks like this:</p> <p><img src="https://miro.medium.com/v2/resize:fit:1050/1*1KQ-JBHGGa0GzVbWl-e8sw.png" alt=""></p> <p>Epoch vs Distance</p> <p>For the training set, the average distance between spectrograms of the same individual was 1.4, whereas the distance between spectrograms of different people was 4.7. Similarly, the average similar distance was 1.8 and the average dissimilar distance was 4.4 for the test set.</p> <p>Because the model simply outputs the value of the distance between the pairings, we can conclude that if the distance between the pairs is less than 1.8, we will label it as 1, and else it will be labeled as 0.</p> <p><img src="https://miro.medium.com/v2/resize:fit:897/1*HRiVAUdY2Isq67TFSIiCsg.png" alt=""></p> <p>The precision and recall values for the training set were determined to be 0.87 and 0.84, respectively, whereas the precision and recall values for the test set were close to 0.8.</p> <h1 id="6-training-parameter">6. Training Parameter</h1> <p>The model was trained for 30 epochs with a learning rate of 1e-4. The Adam optimizer was employed, with a polynomial learning rate decay and the first 100 steps of the models operating as warn steps.</p> </div> </article> </div> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>